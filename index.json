[{"content":"https的出现是为了修复http安全方面的隐患.\nhttp并未对传输的报文进行充分的校验,它能做的只有把数据放到body部分,加密后再发送出去,但是中途可以被人拦截,篡改,冒认,接收方无法分辨收到的是否原始报文,也无法分辨是否正在与本人进行通信.这个叫中间人攻击.\nhttps比http多的的这个s就是ssl/tls.ssl是tls的前身,tls是ssl的升级,我简单搜索了下,并未看见有哪篇文章详细介绍它们的区别,所以这应该不重要,我们只需记住tls是更加安全的ssl即可.\ntls的作用是对通信的报文进行加密和验证,既然无法直接从收到的信息确认对方身份和信息是否原始无误,那么那么就要引入可靠的第三方来帮我们鉴定,这个第三方就是CA证书中心.\n与此同时还有几个核心概念:\n 密钥对: 分为公钥和私钥,公钥公开并存放在CA中心,私钥则自己保管任何人不可知,密钥对一一对应,服务端用自己的私钥进行对报文摘要进行加密得到数字签名,客户端从CA中心得到公钥,然后对签名进行解密,将解密后的摘要与报文作比较即可判断报文有无篡改以及对方身份是否合法.然后客户端用服务端公钥对自己将要发送的报文进行加密,服务端收到后用自己的私钥进行解密得到消息,这份报文里包含客户端的私钥,下一次服务端发送消息时就可以带上客户端的私钥用来证明自己的身份. 数字签名: 对报文本体进行hash计算得到一份摘要,然后用自己的私钥对摘要进行加密得到数字签名,接收方收到签名后使用发送方的公钥进行解密,再与报文内容进行比较即可确认对方身份和报文有无被篡改.由于私钥只有本人持有,所以数字签名无法伪造. 数字证书: 由CA中心颁发,包含发布机构,有效日期,证书持有者,持有者公钥,数字签名等信息,用于验证证书持有者身份.  具体流程如下:\n 客户端请求与服务端建立连接,并告知对方自己支持的hash算法和协议版本. 服务端用私钥进行签名,把证书发给客户端 客户端收到证书后前往CA中心求证,用CA中心提供的公钥来解密签名并与报文内容进行对比,如果无误则说明对方身份合法可信,报文未收到破坏.值得一提的是CA中心同样有自己的证书,它的身份由上级CA中心鉴定.我们一定要找到一个一定可靠不需鉴定的CA中心,那么最顶级的CA中心在哪呢?答案是已经内置在操作系统和浏览器里面. 确认证书没有问题后,客户端把自己的私钥放进报文,并用服务端的公钥进行加密后发送回去,由于只有持有私钥的服务端才能解密,因此不用担心消息泄露 服务端收到消息后用自己的私钥解密得到客户端的私钥,然后通知客户端连接已经建立,由于双方都持有客户端的私钥,所以从这里转为对称加密,消息都用客户端的私钥加密解密.这一步也不用担心被攻击,中间人得到消息但缺乏客户端私钥,无法解密,他也无法伪造成客户端去给服务端发消息  ","permalink":"http://euthpic.github.io/tech/https%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6%E5%92%8C%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D/","summary":"https的出现是为了修复http安全方面的隐患. http并未对传输的报文进行充分的校验,它能做的只有把数据放到body部分,加密后再发送出","title":"Https,数字证书和数字签名"},{"content":"需求背景 我们的集群迁移到云上的k8s后,本地无法继续直连调试,需要新的远程调试方案.\n同事先调研了阿里出品的kt-connect,给出的结论是由于协议转换的问题,在使用redis,kafka等中间件时会受影响,于是这个方案被否定了.然后我抽空试了下telepresence(和ambassador同源),基本需求都能满足,所以就把这个推给其他人了.\n安装traffic-manager 这类工具都需要在集群和本地创建代理,集群上的代理客户端叫traffic-manager,本地的叫telepresence.\n需要确保telepresence和traffic-manager版本一致.我们使用的版本是2.4.6\nhelm repo add datawire https://app.getambassador.io helm repo update kubectl create namespace ambassador helm install traffic-manager --namespace ambassador datawire/telepresence 安装telepresence macOS # Install via brew: brew install datawire/blackbird/telepresence # OR install manually: # 1. Download the latest binary (~60 MB): sudo curl -fL https://app.getambassador.io/download/tel2/darwin/amd64/2.4.6/telepresence -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence Linux # 1. Download the latest binary (~50 MB): sudo curl -fL https://app.getambassador.io/download/tel2/linux/amd64/2.4.6/telepresence -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence Windows 接触的时候他们刚推出windows的开发者预览版本,在安装脚本和使用过程中可能都会有一些小问题.\n  下载官方安装包: https://app.getambassador.io/download/tel2/windows/amd64/2.4.6/telepresence.zip\n  安装命令请在Powershell内以管理员身份执行(需要把安装包放到盘根目录再解压,否则可能失败)\nExpand-Archive -Path telepresence.zip Remove-Item \u0026#39;telepresence.zip\u0026#39; cd telepresence   默认安装到C:\\telepresence,可以通过编辑install-telepresence.ps1来修改安装路径\nSet-ExecutionPolicy Bypass -Scope Process .\\install-telepresence.ps1   安装完成后系统环境变量中会多出两行\nC:\\telepresence C:\\Program Files\\SSHFS-Win\\bin   确认是否安装成功\nPS C:\\Users\\iplas\u0026gt; telepresence.exe status Root Daemon: Not running User Daemon: Not running   安装kubectl(新版本的docker desktop v20.10.7自动安装了kubectl) https://kubernetes.io/zh/docs/tasks/tools/install-kubectl-windows/\n  验证\n$ kubectl.exe version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.2\u0026#34;, GitCommit:\u0026#34;092fbfbf53427de67cac1e9fa54aaa09a28371d7\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-06-16T12:59:11Z\u0026#34;, GoVersion:\u0026#34;go1.16.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;windows/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;18+\u0026#34;, GitVersion:\u0026#34;v1.18.4-tke.6\u0026#34;, GitCommit:\u0026#34;194201819cf1e5cf45d38f72ce1aac9efca4c7ff\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-12-29T09:13:24Z\u0026#34;, GoVersion:\u0026#34;go1.15.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;}   如果没有server的信息是因为kubeconfig没有配置,配置文件可以在Rancher获取并保存本地\napiVersion: v1 kind: Config clusters: - name: \u0026#34;testk8s\u0026#34; cluster: server: \u0026#34;https://rancher.xxx.com.cn/k8s/clusters/local\u0026#34; users: - name: \u0026#34;testk8s\u0026#34; user: token: \u0026#34;kubeconfig-u-sr6p9:xxxx\u0026#34; contexts: - name: \u0026#34;testk8s\u0026#34; context: user: \u0026#34;testk8s\u0026#34; cluster: \u0026#34;testk8s\u0026#34; current-context: \u0026#34;testk8s\u0026#34;   添加系统变量,变量名=KUBECONFIG 变量值=${kubeconfig.yml保存位置}\n  验证集群连通性\n$ kubectl cluster-info Kubernetes control plane is running at https://rancher.xxx.com.cn/k8s/clusters/local CoreDNS is running at https://rancher.xxx.com.cn/k8s/clusters/local/api/v1/namespaces/kube-system/services/kube-dns:dns-tcp/proxy KubeDNSUpstream is running at https://rancher.xxx.com.cn/k8s/clusters/local/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   升级   退出telepresence进程\n$ telepresence.exe quit Telepresence Root Daemon quitting... done Telepresence User Daemon quitting... done 2021-11-10 14:33:36 iplas@q /e/cmd $ telepresence.exe status Root Daemon: Not running User Daemon: Not running   重新安装新的版本(不用卸载)\n  使用 全量拦截   连接traffic-manager\n$ telepresence.exe connect Launching Telepresence Root Daemon Launching Telepresence User Daemon Connected to context testk8s (https://rancher.xxx.com.cn/k8s/clusters/local)   拦截指定deploy的全部流量,注意这里的service-name指deploy的name,remote-port指svc的port.\ntelepresence intercept \u0026lt;service-name\u0026gt; --port \u0026lt;local-port\u0026gt;[:\u0026lt;remote-port\u0026gt;] --env-file \u0026lt;path-to-env-file\u0026gt;   以test1-oss的mobile-oss为例(如果要拦截的svc只有一个端口,可以不指定远程端口)\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080 Using Deployment mobile-oss-deploy intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   检查拦截情况\n$ telepresence.exe list -n test1-oss activiti-service-deployment : ready to intercept (traffic-agent not yet installed) admin-service-deployment : ready to intercept (traffic-agent not yet installed) attachment-service-deployment : ready to intercept (traffic-agent not yet installed) crm-service-deployment : ready to intercept (traffic-agent not yet installed) crmdb-services-deployment : ready to intercept (traffic-agent not yet installed) mobile-oss-deploy : intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   此时访问该svc会发现流量流向本地了\n$ curl https://xxx-test1.xxx.com.cn/ /index   解除拦截\n$ telepresence.exe leave mobile-oss-deploy-test1-oss   再次检查拦截情况\n$ telepresence.exe list -n test1-oss activiti-service-deployment : ready to intercept (traffic-agent not yet installed) admin-service-deployment : ready to intercept (traffic-agent not yet installed) attachment-service-deployment : ready to intercept (traffic-agent not yet installed) crm-service-deployment : ready to intercept (traffic-agent not yet installed) crmdb-services-deployment : ready to intercept (traffic-agent not yet installed) mobile-oss-deploy : ready to intercept (traffic-agent already installed)   可以看到mobile-oss-deploy的状态从intercepted变为ready to intercept,括号内容从traffic-agent not yet installed变为traffic-agent already installed,这是因为每个被代理的deploy都会安装一个名为traffic-agent的sidecar,占用内存约50m.这个sidecar不会因解除拦截而销毁,只能手动销毁\n##销毁指定deploy $ telepresence.exe -n test1-oss uninstall -d mobile-oss-deploy ##销毁全部 $ telepresence uninstall --everything   局部拦截   telepresence还提供了名为preview url的拦截模式,该模式不影响集群原有流量,仅把通过该url访问的流量导向本地.\n  先要登录ambassador(只有登录状态下才能用preview url)\n$ telepresence.exe login Launching browser authentication flow... Login successful.   确认本地服务已启动的情况下,再次创建拦截\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080:80   然后访问https://reverent-dhawan-659.preview.edgestack.me/验证拦截情况(访问需要登录状态,如果是浏览器之外的访问方式记得带上cookie)\n  使用deploy的环境变量启动本地服务   维护本地环境变量是个繁琐的事情,IntelliJ可以通过EnvFile插件来使用集群的环境变量\n  创建拦截时通过-e参数把集群变量写入到本地文件\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080 -e /e/envfile/mobile-oss.env Using Deployment mobile-oss-deploy intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   spring boot/tomcat启动: 安装好EnvFile后,运行配置里会多出一列\u0026quot;EnvFile\u0026quot;,选择上一步保存的配置文件\n  maven启动: 用上一步的配置文件覆盖本地项目配置文件\n  使用场景  本地接口自测: 不需要创建拦截,直接用api doc来测试 调试服务调用的接口(东西流量): 只能全量拦截 前后端联调,接口的入口可以在前端修改(南北流量):可以全量拦截,也可以用preview url  FAQs telepresence连接失败怎么排查  检查telepresence的版本和traffic-manager的版本是否一致,目前用的2.4.0 检查是否开了socks代理,把系统变量里的http_proxy和https_proxy删掉 重启大法,试下依次重启telepresence进程,要拦截的deploy,本地主机  创建拦截失败怎么排查  如果svc暴露了多个port,需要显示指定远程的port(默认是80) 检查svc的port是否和deploy的port对应 traffic-agent只能绑定到一个port,如果之前创建的拦截是绑定到A端口,现在要改成B端口,要先卸载原先的traffic-agent,再创建拦截 创建preview url前本地服务需要先启动,并确保服务端口已经开放.创建了preview url后,可以访问 https://app.getambassador.io/cloud/services 查看拦截情况. 如果报错中有conflict关键字,可能是别人先抢占了环境,得等别人leave之后才能连(好像自己无法主动断开其他人的连接,所以要养成每次用完主动leave的习惯)  一些补充  本地的启动配置替换成k8s,不过由于本地没有namespace.所以svc要补全namespace 对于服务注册而言,像xxl-job和nacos这类组件可以手动配置注册ip,使用是正常的,但是如果遇到像我们自研框架的zookeeper这样会把本地ip注册上去(云上无法识别内网ip)的情况,就需要改造成注册ip可配置了. 如果已经telepresence login,那么可以创建preview url(只会拦截通过该url访问的流量,不影响集群),否则拦截全部流量. preview url依赖ambassador cloud,需要登录状态(小程序可能得考虑封装个全局的cookie) 创建preview url前本地服务需要先开启,并确保服务端口已经开放(可能ambassador cloud要检测什么) preview url原理: 生成一个请求头带标记的request,然后telepresence将请求转发到ambassador cloud(因为这个url由ambassador创建并公开),然后ambassador再转发回集群,集群内的traffic agent查看头部并拦截请求,将它转发回本地机器 曾被拦截过的deploy里面都会创建一个traffic-agent(一个agent消耗50m内存,如果每个环境的每个deploy都创建了一个agent,需要考虑内存占用的问题) 如果deploy对应的svc暴露了多个端口,需要在冒号后面指定拦截哪个端口(svc的port)  ","permalink":"http://euthpic.github.io/tech/telepresence/","summary":"需求背景 我们的集群迁移到云上的k8s后,本地无法继续直连调试,需要新的远程调试方案. 同事先调研了阿里出品的kt-connect,给出的结论是","title":"Telepresence"},{"content":"在迁移到k8s之前,我们的测试环境是用docker部署到自己的物理机上的.那时常常会发生这样一种情况:过了一晚上或一个周末后,原本在测试环境正常运行的服务一直提示无响应.\n一般情况下,把服务和zk都重启了之后就能解决,也因为是测试环境,所以背后的原因无人细究.有一次我重启了好几次都不奏效,得reboot之后才行,浪费了大半天时间,一气之下这才决定好好研究一番.\n首先并非全部服务都异常,仍有部分在正常工作,docker ps看了下zk进程也还在.然后我猜会不会是这些服务压根还没注册上去,因为也时常会发生修改了环境变量却没同步到各个环境,从而导致这个环境上的服务启动失败的情况. connect上zk server,ls发现无响应的服务确实是注册上来了,不是这个原因.\n然后查看zk的日志,猛地发现日志在疯狂输出,不断有服务尝试连接,但是又很快断开连接.我观察了几分钟,这种现象似乎没有停止的迹象.接着看了下这台机的连接信息,发现有大量连接都是处于TIME_WAIT状态.而且连接的一端都指向zookeeper.那么现在zk的连接具体有多少呢?用netstat看是两三百之间浮动(包括TIME_WAIT),而其他正常的环境是稳定在100左右.\n似乎找到突破口了,停下来开始分析,刚刚这些疯狂重连的服务应该就是异常的服务了,它们收不到服务器的响应,认为连接已断,所以又开始重连,可是重连之后不知发生了又快速地断开,所以堆积了大量TIME_WAIT的连接.\n那么是什么导致这些连接快速断开的呢?又回去仔细看了刚刚的日志,发现有个warning被我遗漏掉了,\u0026ldquo;Too many connections from \u0026hellip;. - max is 60\u0026rdquo;\n","permalink":"http://euthpic.github.io/tech/zookeeper%E6%9C%8D%E5%8A%A1%E9%A2%91%E7%B9%81%E6%97%A0%E5%93%8D%E5%BA%94%E6%88%91%E4%BB%85%E4%BF%AE%E6%94%B9%E4%B8%80%E8%A1%8C%E9%85%8D%E7%BD%AE%E5%B0%B1%E8%A7%A3%E5%86%B3%E5%AE%83%E4%BA%86/","summary":"在迁移到k8s之前,我们的测试环境是用docker部署到自己的物理机上的.那时常常会发生这样一种情况:过了一晚上或一个周末后,原本在测试环境","title":"一次解决zookeeper服务频繁无响应的经历"},{"content":"import java.io.{PrintWriter} import scala.collection.mutable import scala.io.{Source, StdIn} import scala.reflect.io.File case class Person(id: Int, name: String, age: Int) /** * Scala 基本笔记 */ object ScalaLesson extends App { /** * 基础-算术 */ def lesson1() = { println(1.+(2)) println(2.-(2)) println(2.*(2)) println(2./(2)) println(10 max 2) } /** * 基础-重载 */ def lesson2() = { val str = \u0026#34;Hello\u0026#34;(2) //等同于java中的”Hello“.charAt(2)  println(str) //实现原理为StringOps包中的一个apply方法：def apply(n:Int): Char  val s = \u0026#34;Hello\u0026#34;.apply(2) println(s) } /** * 基础-字符串操作 */ def lesson3() = { val str = \u0026#34;Hello World\u0026#34; //获取字符串的第一个字符  println(str.head) //获取字符串的最后一个字符  println(str.last) //获取字符串前3个字符  println(str.take(3)) //获取字符串后3个字符  println(str.takeRight(3)) //删除字符串前3个字符  println(str.drop(3)) //删除字符串后3个字符  println(str.dropRight(3)) } /** * 结构与函数-条件判断 */ def lesson4(n: Int) = { //if/else语法结构与java结构一致，在scala中表达式是有值的，这个值就是跟在if/else之后的表达式的值  val s = if (n \u0026gt; 1) 1 else 0 //等同于以下方式,不过在scala中推荐使用val而尽量不使用var可变量  //注：scala中不支持三目运算，scala把java的三目运算结合在了if/else中  var a = 0 if (n \u0026gt; 1) a = 1 else a = 0 println(s) println(a) } /** * 结构与函数-公共超类 */ def lesson5(n: Int) = { //在scala中存在一个公共超类Any，java中只能返回指定类型，scala可以返回不指定的类型,支持混合型表达式  val s = if (n \u0026gt; 0) \u0026#34;Hello World\u0026#34; else 1 println(s) //在scala中如果if/else中缺失了else后部分，如：  val a = if (n \u0026gt; 0) \u0026#34;Hello World\u0026#34; //在scala中每个表达式都是有值的，如果缺失了else后部分，为了解决这个问题，在java的基础上引入了一个Unit类，写作()，这样就等同于if(n\u0026gt;0) \u0026#34;Hello World\u0026#34; else ()  //这个()相当于一个无用值，Unit与java中的void相当，区别在于void没有值，Unit有一个“无用”的值。  println(a) } /** * 结构与函数-终止语句 */ def lesson6(n: Int) = { //scala中不需要加分号结束,如果是多行写成1行则需要分号结束，但不建议这样写,如：  val s = if (n \u0026gt; 0) { var r = 1 * n r -= 1 } else 1 //  } /** * 结构与函数-块表达式 */ def lesson7(n: Int) = { //与java中的{}一致，但scala的块是一种表达式，所以有值，值为表达式的最后语句返回值  val s = { val a = 5 a * n } println(s) } /** * 结构与函数-输入与输出 */ def lesson8() = { //不换行打印  //print(\u0026#34;Scala\u0026#34;)  //换行打印  //println(\u0026#34;你好\u0026#34;)  //上面2个打印等同于下面语句  //println(\u0026#34;Scala\u0026#34; + \u0026#34;你好\u0026#34;)  //带C风格格式化字符串的函数  //printf(\u0026#34;Hello,%s! 你是一款很好的语言,你存在有 %y 年了吗？\u0026#34;,\u0026#34;Scala\u0026#34;,10)  //读取控制台一行输入(2.11.0以后的版本使用StdIn)  val name = StdIn.readLine() println(\u0026#34;你输入的姓名是:\u0026#34; + name) val age = StdIn.readInt() println(\u0026#34;你输入的年龄是:\u0026#34; + age) } /** * 结构与函数-while循环 */ def lesson9(n: Int) = { //与java中的while和do循环一致  var i = n while (i \u0026gt; 0) { println(i * 3) i -= 1 } } /** * 结构与函数-for循环 */ def lesson10(n: Int) = { //scala中的for循环与java的结构不一样,scala的结构为 for(i \u0026lt;- 表达式)  for (i \u0026lt;- 0 to n) println(i * 3) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //在RichInt类中存在 to 这个方法。0 to n 代表的是 0到n的区间（包含n），如果只是0到n-1的话这采用until方法，如下:  for (i \u0026lt;- 0 until n) println(i * 3) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) val str = \u0026#34;Hello\u0026#34; var sum = 0 //在循环中不仅可以对数字区间进行遍历，也可以对字符串进行遍历  for (ch \u0026lt;- str) sum += ch println(sum) } /** * 结构与函数-退出循环 */ def lesson11(n: Int) = { //scala中的并没有提供break或者continue语句来退出循环，如果需要break该怎么做，如下：  //1：需要引入import util.control.Breaks._包  import util.control.Breaks._ //break例子  breakable( for (i \u0026lt;- 0 to 5) { println(i) if (i == n) break } ) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //continue例子,需要注意判断要放在最前面  for (i \u0026lt;- 0 to 5) { breakable { if (i == n) break println(i) } } println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //中断嵌套循环  breakable( for (i \u0026lt;- 0 to 5) { println(i) breakable( for (a \u0026lt;- 1 to 3) { println(\u0026#34;i * a = \u0026#34; + i * a) if (i * a == 1) break } ) } ) } /** * 结构与函数-高级循环 */ def lesson12(n: Int) = { //scala支持多个生成器,之间用分号隔开,(等同于多个嵌套for循环)如下:  for (i \u0026lt;- 0 to 3; j \u0026lt;- 1 to 4) println(i * j) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //scala支持守卫,以if开头的Boolean表达式(注：if之前没有分号):  for (i \u0026lt;- 0 to 3; j \u0026lt;- 1 to 4 if i \u0026lt; j) println(i * j) //scala还支持引入循环中的变量：  for (i \u0026lt;- 1 to 3; j \u0026lt;- n to 3) println(i * j) //for推导式，以yield开始:  val list = for (i \u0026lt;- 1 to 10) yield i % 3 println(list) } /** * 结构与函数-默认参数与带名参数 */ def lesson13(name: String, n: Int = 18) = { println(s\u0026#34;你的姓名是：$name,年龄是:$n\u0026#34;) } /** * 懒值，只需要在需要懒加载的值或函数前面加 lazy关键字即可,如： * 只有在调用的时候才执行，不调用的时候不执行 */ lazy val m = System.currentTimeMillis() /** * 结构与函数-异常处理 */ def lesson14(n: Int): Unit = { //scala异常的工作原理与java的一样，区别在于scala没有受体异常--不需要声明函数或者方法可能会抛出某种异常  // throw new XXException(xxx)  //捕抓异常采用模式匹配  val a = try { 10 / n } catch { case ex: Exception =\u0026gt; println(ex.getMessage) } } /** * 数组操作 */ def lesson15() = { import scala.collection.mutable.ArrayBuffer //定长数组  val arr1 = new Array[Int](5) println(arr1.toString, arr1.length) //推导数组，类型根据推导出来，提供初始值的时候不需要new  val arr2 = Array(\u0026#34;Hello\u0026#34;, \u0026#34;world\u0026#34;) println(arr2.toString, arr2.length) //根据角标获取值时采用()而不是java的[]  println(arr2(0)) //变长数组：数组缓冲  //创建空的数组缓冲，准备存放整数  val arr3 = ArrayBuffer[Int]() //用 += 在尾端添加元素  arr3 += 1 //得到ArrayBuffer(1)  arr3 += (1, 3, 4, 8, 2) //得到ArrayBuffer(1,1,3,4,8,2)  arr3 ++= Array(9, 3, 5) // 可以用 ++= 操作符追加任何的集合  //得到ArrayBuffer(1,1,3,4,8,2,9,3,5)  arr3.trimEnd(2) //移除最后2个元素  arr3.trimStart(2) //移除最前2个元素  //可以在任意位置插入元素(低效)  arr3.insert(1, 3) //可以在任意地方移除多少个元素(低效)  arr3.remove(1, 2) //数组缓冲转成Array  arr3.toArray //数组转数组缓冲  arr1.toBuffer } /** * 数组操作-遍历 */ def lesson16() = { val arr1 = Array(1, 2, 6, 3, 8, 4) //需要下标时  for (i \u0026lt;- 0 until arr1.length) println(i + \u0026#34; : \u0026#34; + arr1(i)) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //不需要下标时  for (e \u0026lt;- arr1) println(e) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //数组转换,采用yield关键字  val r = for (e \u0026lt;- arr1) yield 2 * e println(r.toList) } /** * 数组操作-常用算法 */ def lesson17() = { val arr1 = Array(1, 2, 6, 3, 8, 4) //求和  println(arr1.sum) //获取最大值  println(arr1.max) //获取最小值  println(arr1.min) //获取平均值  println(if (arr1.nonEmpty) arr1.sum / arr1.length else 0) //从小到大排序(注:排序得到的是一个新数组缓冲，原数组不会改变)  val r = arr1.sorted //指定排序  val r1 = arr1.sortWith(_ \u0026lt; _) println(r.toList) println(r1.toList) //数组拼接成字符串  println(r1.toString) println(r1.mkString) println(r1.mkString(\u0026#34;,\u0026#34;)) } /** * 数组操作-多维数组 */ def lesson18(): Unit = { //与java一样，多维数组是通过数组的数组来实现的。Double的二维数组类型为Array[Array[Double]]  //要构造这样函数可以采用ofDim方法：  val matrix = Array.ofDim[Double](3, 4) //三行四列  //访问这样的数组，使用两对圆括号(row)(column):  matrix(1)(2) } /** * 数组操作-与java的相互操作 */ def lesson19(): Unit = { //引入相应的包  import scala.collection.JavaConversions.bufferAsJavaList import scala.collection.mutable.ArrayBuffer val command = ArrayBuffer(\u0026#34;ls\u0026#34;, \u0026#34;-al\u0026#34;, \u0026#34;/home/cay\u0026#34;) val pb = new ProcessBuilder(command) //Scala转java  import scala.collection.JavaConversions.asScalaBuffer import scala.collection.mutable.Buffer val cmd: Buffer[String] = pb.command() //java转Scala  } /** * 映射与元组-映射 */ def lesson20(): Unit = { //构建一个不可变的映射(建议)  val map = Map(\u0026#34;张三\u0026#34; -\u0026gt; 98, \u0026#34;李四\u0026#34; -\u0026gt; 83, \u0026#34;王五\u0026#34; -\u0026gt; 100) //构建一个可变的映射(不建议)  val map1 = mutable.Map(\u0026#34;张三\u0026#34; -\u0026gt; 98, \u0026#34;李四\u0026#34; -\u0026gt; 83, \u0026#34;王五\u0026#34; -\u0026gt; 100) //构建一个空的映射，需要指定类型参数  val map2 = new mutable.HashMap[String, Int]() // -\u0026gt; 操作符用来创建对偶  // (\u0026#34;张三\u0026#34; -\u0026gt; 98) 等同于 (\u0026#34;张三\u0026#34; , 98)，所以也可以用以下方式定义映射:  val map3 = Map((\u0026#34;张三\u0026#34;, 98), (\u0026#34;李四\u0026#34;, 83), (\u0026#34;王五\u0026#34;, 100)) //获取映射中的值，直接使用(),相当于java中的map.get(\u0026#34;张三\u0026#34;)  println(map(\u0026#34;张三\u0026#34;)) //如果key不存在，则会抛异常，检查映射中是否存在某个key，可以使用contains方法:  val value = if (map.contains(\u0026#34;张三\u0026#34;)) map(\u0026#34;张三\u0026#34;) else 0 //可以简写成getOrElse(key,result):  println(map.getOrElse(\u0026#34;张三\u0026#34;, 0)) //更新可变映射中的值  map1(\u0026#34;张三\u0026#34;) = 67 //增加可变映射中的值  map1(\u0026#34;赵六\u0026#34;) = 74 //可以使用 += 增加多个关系  map1 += (\u0026#34;王七\u0026#34; -\u0026gt; 35, \u0026#34;蔡八\u0026#34; -\u0026gt; 86) //可以使用 -= 移除对应的键值  map1 -= \u0026#34;王五\u0026#34; //可以使用 + 操作符生成新的映射  val map4 = map + (\u0026#34;王七\u0026#34; -\u0026gt; 35, \u0026#34;蔡八\u0026#34; -\u0026gt; 86) //映射遍历 for((k,v) \u0026lt;- map)  } /** * 映射与元组-元组 */ def lesson21() = { //映射是键值对的集合，对偶则是元组（tuple）的最简单形态--元组是不同类型的值的聚集，元组的值是通过将单个的值包含在()构成的,如：  val t = (2, \u0026#34;张三\u0026#34;, 4d) //类型分别为 Tuple[Int, String, Double]  //元组的访问可以使用_1、_2、_3这样  println(t._1) println(t._2) println(t._3) //也可以在返回值的时候就定义下来，通常使用这种,这样更为直观的表达出返回的结果分别代表什么  val (first, second, third) = t println(first) println(second) println(third) } /** * 映射与元组-拉链操作 */ def lesson22(): Unit = { //使用元组的原因之一是吧多个值绑定在一起，以方便它们能够被一起处理，这个通常可以使用zip方法来完成，如下：  val a1 = Array(\u0026#34;张三\u0026#34;, \u0026#34;李四\u0026#34;, \u0026#34;王五\u0026#34;) val a2 = Array(55, 26, 96) val p = a1.zip(a2) //得到的对偶数组为Array((\u0026#34;张三\u0026#34;,55),(\u0026#34;李四\u0026#34;,26),(\u0026#34;王五\u0026#34;,96))  //还可以把对偶数组转成映射  println(p.toMap) //得到结果 Map(张三 -\u0026gt; 55, 李四 -\u0026gt; 26, 王五 -\u0026gt; 96)  } /** * 对象-单例对象 * 在Scala中没有静态方法或者静态字段，可以通过object语法结构来定义 * */ object Accounts { //伴生对象  private var lastNumber = 0 def newUniqueNumber() = { lastNumber += 1; lastNumber } } /** * 对象-伴生对象 * 在Scala中没有静态方法或者静态字段，可以通过object语法结构来定义 * */ class Accounts { val id = Accounts.newUniqueNumber() } /** * 对象-扩展类或特质对象 */ abstract class UndoableAction(val desc: String) { def undo(): Unit def redo(): Unit } object DoNothingAction extends UndoableAction(\u0026#34;Do nothing\u0026#34;) { override def undo(): Unit = {} override def redo(): Unit = {} } //DoNothingAction可以被所有所需要这个缺省行为的地方共用  val actions = Map(\u0026#34;open\u0026#34; -\u0026gt; DoNothingAction, \u0026#34;save\u0026#34; -\u0026gt; DoNothingAction) /** * 对象-应用程序对象 * 每个Scala程序都必须从一个对象main方法开始，也可以扩展App特质 * object Hello{ * def main(args:Array[String]) { * println(\u0026#34;Hello World\u0026#34;) * } * } */ /** * 对象-枚举 * scala中并没有枚举类型，不过提供了一个Enumeration助手类，可以用于产出枚举 */ object TrafficLightColor extends Enumeration { val Red = Value(0, \u0026#34;Stop\u0026#34;) val Yellow = Value(10) val Green = Value(\u0026#34;Go\u0026#34;) } def lesson23(): Unit = { println(TrafficLightColor.Red) println(TrafficLightColor.Yellow) println(TrafficLightColor.Green) } /** * 文件操作-读取文件 */ def lesson24(): Unit = { //以指定的GBK字符集读取文件，第一个参数可以是字符串或者是java.io.File  val source = Source.fromFile(\u0026#34;贪腐词库.txt\u0026#34;, \u0026#34;GBK\u0026#34;) //获取所有行  val lines = source.getLines() //将所有行放到list中  val list = lines.toList //将文件用逗号串起来(注：旧版采用source.mkString,新版中获取不到值)  val str = list.mkString //println(\u0026#34;------------\u0026#34;+str)  //迭代打印集合  //list.foreach(it=\u0026gt;println(it))  //关闭流  source.close() } /** * 文件操作-从URL读取文件 */ def lesson25(): Unit = { val source = Source.fromURL(\u0026#34;https://www.baidu.com\u0026#34;, \u0026#34;UTF-8\u0026#34;) //获取所有行  val lines = source.getLines() //将所有行放到list中  val list = lines.toList //将文件用逗号串起来(注：旧版采用source.mkString,新版中获取不到值)  val str = list.mkString //println(\u0026#34;------------\u0026#34;+str)  //迭代打印集合  list.foreach(it =\u0026gt; println(it)) //关闭流  source.close() } /** * 文件操作-读取二进制文件 */ def lesson26(): Unit = { val file = File(\u0026#34;其他词库.txt\u0026#34;) val in = file.inputStream() val bytes = new Array[Byte](file.length.toInt) in.read(bytes) println(bytes.mkString(\u0026#34;,\u0026#34;)) in.close() } /** * 文件操作-写入文件 */ def lesson27() = { //scala中没有内建对文件的支持，所以使用java.io.PrintWriter  val out = new PrintWriter(\u0026#34;test.txt\u0026#34;) for (i \u0026lt;- 1 to 100) out.println(i) } /** * 正则表达式 */ def lesson28() = { //采用String中的r方法  val pattern = \u0026#34;[0-9]+\u0026#34;.r println(pattern findAllIn \u0026#34;02,2\u0026#34;) } /** * 操作符 */ def lesson29() = { //在scala中可以使用任意序列的操作字符作为定义,如:  val * = \u0026#34;ere\u0026#34; val \u0026amp; = \u0026#34;ere\u0026#34; val ! = \u0026#34;ere\u0026#34; //一旦遇到命名定义是scala关键字的还可以采用反引号来拯救(当然平时还是注意少用scala的关键字来命名)  val `type` = 354 //还可以使用 a 标识符 b 这样写  //1 to 10 //实际上是调用了 1.to(10) 的方法  //1-\u0026gt; 10 //等同调用 1.—\u0026gt;(10) 的方法  //1 toString //等同于 1.toString ，以上均为一元操作符，点号可以省略  //赋值操作符 a 操作符= b 等同于 a = a + b  //1 += 2 //等同于 1 = 1 + 2  //结合性,已冒号结束，操作符是右结合  //1::2::Nil //等同于 1::(2::Nil)  } import math._ /** * 高阶函数-值函数 */ def lesson30() = { val num = 3.14 //把 ceil函数赋值给fun  val fun = ceil _ val a = Array(2.14, 1.42, 2.0).map(fun) println(a.mkString(\u0026#34;,\u0026#34;)) } /** * 高阶函数-匿名函数 */ def lesson31() = { val s = (x: Double) =\u0026gt; 3 * x //等同于 def s(x:Double) = 3 * x  println(s(3)) } /** * 高阶函数-带函数参数的函数 */ def lesson32(f: (Double) =\u0026gt; Double) = { f(0.25) } //对于只出现一次的参数，可以使用_替代  //lesson32(ceil _)  /** * 高阶函数-一些有用的高阶函数 */ def lesson33() = { val list = List(\u0026#34;张三\u0026#34;, \u0026#34;李四\u0026#34;, \u0026#34;王五\u0026#34;) //map方法，遍历应用到集合中的所有元素，并返回全新的结果集合  println(list.map(it =\u0026gt; it -\u0026gt; \u0026#34;你好\u0026#34;)) //foreach方法,遍历集合的所有元素，但不返回结果(对于只出现一次的参数，可以使用_替代)  list.foreach(println(_)) //filter方法,根据条件过滤生成全新的集合  val f = list.filter(_ != \u0026#34;张三\u0026#34;) println(f) //sortWith方法,指定参数排序，返回全新的集合  println(list.sortWith(_ \u0026gt; _)) //sorted方法,从低到高排序，返回全新的集合  println(list.sortWith(_ \u0026gt; _).sorted) //groupBy方法,根据指定参数进行合并分组，返回map[object,List[object]]集合  val userList = List(Person(1, \u0026#34;张三\u0026#34;, 23), Person(2, \u0026#34;李四\u0026#34;, 45), Person(1, \u0026#34;王五\u0026#34;, 30)) val map: Map[Int, List[Person]] = userList.groupBy(_.id) //Map遍历,_1代表key,_2代表value  map.foreach(it =\u0026gt; { println(it._1) println(it._2) }) //exists方法,遍历判断条件是否成立  println(list.exists(_ == \u0026#34;张三\u0026#34;)) //distinct方法,去除重复的元素  println(list.distinct) } /** * 集合之间的操作 */ def lesson34() = { //注：在scala中不可变集合进行操作生成的都为新的集合，原有集合不发生改变  val list = List(1, 2, 3) val map = Map(1 -\u0026gt; \u0026#34;java\u0026#34;, 2 -\u0026gt; \u0026#34;scala\u0026#34;, 3 -\u0026gt; \u0026#34;php\u0026#34;) //List追加元素, 后面追加采用 :+ 前面追加采用 +:  println(list :+ 2) //List移除元素  println(list.toSet - 2) //List第一位置增加元素  println(5 +: list) //2个List合并，采用:::或者 ++ 或者 | 或者++:  println(list ++ List(6)) println(list ::: List(6)) println(list ++: List(6)) println(list.toSet | List(7).toSet) //list中移除另外一个List中所有包含的元素,采用 -- 或者 \u0026amp;~  println(list.toSet -- List(2)) println(list.toSet \u0026amp;~ List(2).toSet) //俩个list的交集  println(list.toSet \u0026amp; List(2).toSet) } /** * 模式匹配-更好的switch */ def lesson35(n: Int) = { //scala的switch更优雅的处理方式，在c和类C语言中，模式匹配必须在末尾显式的使用break语句进行退出switch，scala不会有这样的问题  //match与if类似，都是使用表达式。在很多时候match的使用更优雅与if/else if，这样不会出现多层嵌套的情况  val result = n match { case 1 =\u0026gt; \u0026#34;张三\u0026#34; case 2 =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(result) //模式用还可以使用守卫，如下：  val r = n match { case 1 =\u0026gt; \u0026#34;张三\u0026#34; case a if (a \u0026gt; 5) =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(r) //类型匹配，如下：  val ar: Any = if (n \u0026gt; 0) 1 else \u0026#34;Hello\u0026#34; val s = ar match { case i: Int =\u0026gt; \u0026#34;张三\u0026#34; case s: String =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(s) } /** * 模式匹配-Option类型 */ def lesson36() = { //生成Option带值的元素  val o: Option[String] = Some(\u0026#34;张三\u0026#34;) println(o) //生成不带值的Option元素  val o1: Option[Nothing] = None println(o1) //Option元素获取值，采用get方法  println(o.get) //但是直接采用get方法很容易出现 java.util.NoSuchElementException: None.get异常，scala提供了组合方法getOrElse  println(o1.getOrElse(\u0026#34;\u0026#34;)) } /** * 模式匹配-断言 */ def lesson37(n: Int) = { //在scala中与java一样使用断言，格式 assert(条件,提示语)  assert(n \u0026gt; 1, \u0026#34;参数不能小于2\u0026#34;) } def lesson38() = { (0 to 10).map(println(_)) } lesson38() } ","permalink":"http://euthpic.github.io/tech/scala%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A0/","summary":"import java.io.{PrintWriter} import scala.collection.mutable import scala.io.{Source, StdIn} import scala.reflect.io.File case class Person(id: Int, name: String, age: Int) /** * Scala 基本笔记 */ object ScalaLesson extends App { /** * 基础-算术 */ def lesson1() = { println(1.+(2)) println(2.-(2)) println(2.*(2)) println(2./(2)) println(10 max 2) } /** * 基础-重载 */ def lesson2() = { val str = \u0026#34;Hello\u0026#34;(2) //等同","title":"Scala基础练习"},{"content":" 容器本身没有价值,有价值的是容器编排技术 容器(docker)其实是一种沙盒技术,一是可以将应用之间隔离开来,二是方便地将应用\u0026quot;搬来搬去\u0026quot;(快速装载,快速卸载) 容器其实是一种特殊的进程,在容器外面观察它时(ps),它是普通的进程,在容器里面观察它时,外部的细节被屏蔽掉(namespace),使得它以为自己是独立存在的.容器本质上就是一个加了限定参数(namespace)的进程 docker是没有上过历史课的进程 容器较轻量,虚拟机较重.容器底层依赖的仍然是宿主机的硬件/驱动,而虚拟机自己模拟了这些硬件/驱动.容器实现的仅是视图隔离 cgroups 是Linux内核提供的一种可以限制单个进程或者多个进程所使用资源的机制，可以对 cpu，内存等资源实现精细化的控制，目前越来越火的轻量级容器 Docker 就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制 虽然docker通过cgroups实现了容器的视图隔离,但是在容器内使用/proc下的命令,例如top/free等,看到的仍然是宿主机的信息,这是因为容器没有做到对/proc,/sys等文件系统的资源的视图隔离.可以通过lxcfs来解决这个问题 cgroup只能对容器使用的资源上限做限定,但不能锁定下限,这很容易导致被其他容器抢占资源.k8s完善了这点 rootfs : 根文件系统,挂载在容器的根目录上,用来为容器进程提供隔离后执行环境的文件系统.也就是所谓的\u0026quot;容器镜像\u0026quot; .包括的目录和文件有/bin , /etc , /proc等等.进入容器后执行的/bin/bash,就是这个目录下的/bin目录下的文件,与宿主机的/bin/bash不同 docker容器使用了多个增量rootfs联合挂载一个完整rootfs的方案,也就是容器镜像中\u0026quot;层\u0026quot;的概念,包括可读写层,init层,只读层.只读层是共享层,我们使用docker commit提交本地修改后的docker镜像,实际上提交的是可读写层的内容,我们的修改都保存在读写层(分层复用). 上面的读写层通常也称为容器层，下面的只读层称为镜像层，所有的增删查改操作都只会作用在容器层，相同的文件上层会覆盖掉下层。知道这一点，就不难理解镜像文件的修改，比如修改一个文件的时候，首先会从上到下查找有没有这个文件，找到，就复制到容器层中，修改，修改的结果就会作用到下层的文件，这种方式也被称为copy-on-write。 一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理 快速部署k8s的工具是kubeadm,它直接运行在宿主机上,然后k8s的其他组件以容器的方式被kubeadm调用(封装成pod) pod的定义文件时yaml.master等组件的yaml文件在/etc/kubernetes.manifests路径下 快速部署k8s的工具:kops,ranche,minikube,katacoda提供的在线学习平台 创建pod的命令: kubectl apply -f [url] ,可以使用的文件包括.yaml/.yml/.json rook:一个基于 Ceph 的 Kubernetes 存储插件 基于 Kubernetes 开展工作时，你一定要优先考虑这两个问题：我的工作是不是可以容器化？我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成？ 我们在yaml文件中定义API对象(pod),每一个API对象都有一个叫做Metadata的字段,即元数据,它是API对象的\u0026quot;标识\u0026quot;,也是我们从k8s里找到这个对象的主要依据.这其中主要用到的字段是Labels,它是一组k-v格式的标签 kubectl describe用于查看一个API对象的细节.在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中.所以，这个部分正是我们将来进行 Debug 的重要依据。如果有异常发生，你一定要第一时间查看这些 Events，往往可以看到非常详细的错误信息。 k8s鼓励开发者使用kubectl apply YAML文件这样的声明式API去替代docker run这种基于命令行的操作,因为这样可以以文件的形式记录下对k8s的操作,这样运维人员和开发人员可以通过yaml来进行交流沟通. 开发k8s应用,需要自己制作docker镜像? 容器是单进程模型(并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力) pod中可以有多个容器 Pod 的实现需要使用一个中间容器Infra.在这个pod中,infra容器永远是第一个被创建的容器,而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。 如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了  ","permalink":"http://euthpic.github.io/tech/k8s%E7%AC%94%E8%AE%B0/","summary":"容器本身没有价值,有价值的是容器编排技术 容器(docker)其实是一种沙盒技术,一是可以将应用之间隔离开来,二是方便地将应用\u0026quot;搬来","title":"K8s笔记"},{"content":"看完\u0026lt;快学scala\u0026gt;的一些笔记.\n该书的课后练习参考答案: https://github.com/vybae/scala-hello\n 类型推断错误有时候ide检测不出来 ide联想到的api不能总是及时展示出来,有时候忘记语法api什么的,可能自己写的是对的,但是没写完之前ide判断是错误的 定义变量用val和var,前者不可变量.后者可变量.推荐尽量使用val 若val/var或者表达式未赋值,则默认值为Unit空 操作符其实是方法,比如a+b是a.+(b)的简写 可以使用几乎任何符号来为方法命名, 没有三目运算符,但是可通过if else来替代 可以返回不指定的类型,也就是返回的类型不是通过方法头来确定的,而是根据方法最终的结果决定. 上面这条规定其实是由于val和var的引进,使得变量/常量的类型不需要提前声明,因而可在赋值时才确定. 函数可以不声明返回类型(除了递归),但是函数的所有参数必须声明类型. 引入了一个Unit类,写作(),相当于java中的void,像下面这条语句,没有else语句,如果if条件不成立,需要走else,那么else的返回值默认是Unit.  val a = if(n \u0026gt; 0) \u0026#34;Hello World\u0026#34;   不需要加分号表示语句结束,除非一行上有多条语句.\n  代码块也是一种表达式,有值,值为表达式的最后语句的返回值.\n   for循环分为for to和for until语句,区别在于最后一次是否执行\n  没有break和continue关键字,需要引入util.control.Breaks._包\n  for循环默认返回的是Unit空值,不过可以配合yield使用返回一个集合.yield的作用是把当前的元素记下来,保存在集合中,循环结束后将返回该集合.\n  函数比较灵活,没有参数时不用写括号,不过也看不出来调用的是变量还是方法.\n  //sortWith里面的\u0026#34;_\u0026#34;是参数的简略表示 val r1 = arr1.sortWith(_ \u0026lt; _)   object中有main方法的话就只会执行main方法,否则顺序执行object中全部代码块\n  对list,set,map的操作很灵活,可通过操作符而不是api来操作集合,并且它们之间的转化也很方便.\n  对list,set,map的操作一般都是返回新的集合,不会改变原来的集合.\n  所谓守卫,就是以if开头的Boolean表达式\n  没有返回值(实际返回的是Unit)的函数称为过程\n  元组的访问从1开始而不是0.元组常用于返回值不止一个的情况.\n  yield关键字好像不能放在大括号里面\n  可变的map创建时要注意有new关键字\n  如果没有给某个参数传递值,那么Scala将会传递一个默认值(仅限基本的Int,String等).但如果这个参数是自定义类型(抽象对象),Scala没有它的默认值,此时我们需要借助implicit给它传默认值,也就是隐式参数.在同一个上下文环境中,同一类型的隐式参数只能有一个.\n  有意思的???\noverride def deleteFinalPriceOfferFeedbackById(id: Int): Unit = ??? /** `???` can be used for marking methods that remain to be implemented. * @throws NotImplementedError */ //也就是等待实现的方法,也就是java中的抽象方法?  def ??? : Nothing = throw new NotImplementedError 变长参数\ndef sum(args:Int*)={ var result=0 for (arg \u0026lt;- args ) { result+=arg } result } println(sum(7,2,3)) for循环正常只能顺序遍历(也就是i++),如果要倒叙遍历,需要用reverse函数:\n//实现i--打印 for (i \u0026lt;- (0 to 10).reverse) { println(i) } Chapter5 类   调用无参方法时,可以写上圆括号,也可以不写.推荐对于改值器方法(即改变对象状态的方法)使用括号,对取值器方法去掉括号.\n  无参方法声明时可以不带(),这样调用的时候一定不能带()\n  Scala对于类中的每个字段都会设置成私有,并提供公有的getter和setter方法\n  val的字段不提供getter方法,var的字段setter和getter都提供\n  private的字段,其setter和getter都是private\n  一个类如果没有显式定义主构造器,那么它默认拥有一个无参的主构造器\n  辅助构造器的名称为this\n  val p1= new Person //主构造器 val p2=new Person(\u0026#34;Fred\u0026#34;) //第一个辅助构造器 val p3=new Person(\u0026#34;Fred\u0026#34;,42) //第二个辅助构造器   每个类都有主构造器,它与类定义交织在一起:\n  class Person(val name:String,val age: Int) { // (...)中的内容就是主构造器的参数 }   在Scala中,每个对象都有它自己的内部类,也就是a.Member和b.Member是不同的两个类\n  Chapter6 对象   对象(object)也就是类的单个实例\n  伴生对象也就是和类同名的对象,例如:\n  class Account{ ... } object Account{// 伴生对象  ... }   类和它的伴生对象可以相互访问private的字段/函数/构造方法,它们必须存在与同一个源文件中\n  一般都会定义apply()方法,类和对象都可以\n  假设有个Person类和它的伴生对象,声明了个该类型的person对象:\n  //显式调用apply Person.apply(...) //调用的是伴生对象定义的方法 person.apply(...)\t//调用的是类定义的方法  //上面的可以省去apply,效果是等价的 Person(...) person(...)   Scala的程序从一个对象的main方法开始,或者拓展App特性(extends App),这样就会执行对象内的所有代码块(不包含方法)\n  Chapter7 包和引入   一个文件可定义多个包.同一个包可以定义在多个文件当中(也就是要确认一个包里面有什么东西的话,在java中直接找到对应的目录即可,但是scala中可能得扫描全部的文件才能确认)\n  子包中可以访问父包内容,不需要写完整的包名.\n  在Java中,包名是绝对的;但是在Scala中,包名是相对的,因此引用错误的同名包/类的可能性较大,解决方法是使用绝对包名\n  每个包可以有唯一对应的包对象,可供包内访问调用\n  可以通过private[类名]来限制函数/字段的可见性,例如\n  package com.horstmann.impatient.people class Person{ private[people] def description1 =\u0026#34;....\u0026#34; private[impatient] def description2=\u0026#34;...\u0026#34; }   import语句可以出现在哎任何地方(不限于文件顶部,也包括方法内部),作用域延伸到同一代码块的末尾\n  以下三个包总会被隐式引入:\n  import java.lang._ import scala._ import Predef._ //Predef里面有Map,新建一个map时需要小心你需要的Predef的Map还是immutable/mutable里面的Map   Chapter8 继承   重写方法必须使用override,重写抽象方法除外\n  在Java中,protected修饰的成员对于所在包和子类可见.但是在Scala中只对子类可见,如果需要包可见,可以用包修饰符(见chapter 7)\n  构造器内不应该依赖val的值,因为它使用的是val的初始值,来看个例子:\n  class Creature{ val range: Int = 10 val env : Array[Int] =new Array[Int] (range) } class Ant extends Creature{ override val range = 2 //此时有个隐式的env=new Array[Int] (0)  //在父类中env依赖于range,可是构造器优先于字段的初始化,因此range还未初始化为10,而是有个默认的值0,于是env拿到这个0去为自己初始化了  //解决方法, 不太优雅  class Ant extends{ override val range = 2 } with Creature }   Null类型的唯一实例是null值,可以将null赋值给任何引用,但不能赋值给值类型的变量,比如Int.这决定了我们在使用int,long等基本类型时不可能发生空指针异常\n  Nothing类型没有实例.比如,空列表的类型是List[Nothing],它是List[T]的子类\n  判断两个对象是否相等可以直接使用==操作符,因为它会调用equals()\n  Chapter9 文件和正则表达式  如果字符串中含有\\或者\u0026quot;\u0026ldquo;的话可以使用原始字符串\u0026quot;\u0026ldquo;\u0026ldquo;\u0026ldquo;\u0026ldquo;\u0026ldquo;来定义,这样比转义字符易读些  Chapter10 特质 类可以实现任意数量的特质\r特质可以要求实现它们的类具备特定的字段/方法/超类\r和Java接口不同,Scala特质可以提供方法和字段的实现\r当你将多个特质叠加在一起时,顺序很重要--其方法先被执行的特质排在更后面\r特质的关键词是 trait\n 特质中不需要将方法声明为abstract\u0026mdash;因为这些方法默认就是抽象的 特质也可以有构造器.特质构造器的构造顺序从左往右进行(而特质执行顺序则从右往左进行) 构造器的执行顺序: 超类构造器 -\u0026gt; 父特质构造器 -\u0026gt; 从左往右的特质构造器 -\u0026gt;类  Chapter13 集合   +将元素添加到无先后次序的集合中\n  -和\u0026ndash;移除元素\n  +:和:+向前或向后追加到序列\n  ++将两个集合串接到一起\n  list要么是Nil(即空列表),要么由head和tail组成.head是头元素,tail也是一个list,由除了头元素以外的其它元素组成\n  注意map的声明,是(k-\u0026gt;v)而不是(k,v)\n  注意区分map取值的两种方式,map.get(\u0026ldquo;key\u0026rdquo;)返回的是Some类型,map(\u0026ldquo;key\u0026rdquo;)返回的是单纯的value.\n  map(\u0026ldquo;key\u0026rdquo;)如果找不到对应的元素,就会报NoSuchElementException的异常,所以推荐用**map.get()或者map.getOrElse()**比较安全.\n  list转成map: list.groupby(_.key) 转换后的map使用key做键,值是一个list\n  zipWithIndex()方法是为集合的每个元素创建一个下标/索引:\n  val days = Array(\u0026#34;Sunday\u0026#34;, \u0026#34;Monday\u0026#34;, \u0026#34;Tuesday\u0026#34;, \u0026#34;Wednesday\u0026#34;,\u0026#34;Thursday\u0026#34;, \u0026#34;Friday\u0026#34;, \u0026#34;Saturday\u0026#34;) days.zipWithIndex.foreach(println(_)) //(Sunday,0),(Monday,1),(Tuesday,2),(Wednesday,3),(Thursday,4),(Friday,5),(Saturday,6)   zipWithIndex主要的作用在于对list使用map()遍历时,可以获取元素的下标(在java中,fori循环可以获取下标,但在scala中可能是觉得这种循环写法太不优雅,所以用这个方法来替代)\n  request.priceOfferItems.zipWithIndex.map(x =\u0026gt; FinancePriceRequestItem(x._1.price, logisticsFeeList(x._2).price)) //request.priceOfferItems是个list,这段的作用是实际跟fori是一样的.   flatMap = map + flatten 即先对集合中的每个元素进行map，再对map后的每个元素（map后的每个元素必须还是集合）中的每个元素进行flatten\n  Chapter14 模式匹配   模式匹配发生在运行期,此时泛型已经被擦除,所以不能用模式匹配来匹配特定类型的Map(如果匹配不上就会报错)\n  但是数组中的类型是支持匹配的\n  val n=1 //如果声明时ar的类型是具体确定的,那么根据类型匹配时就会报错  //val ar=1 \t//所以得用下面这种,声明为Any类型再赋值  val ar:Any = if(n\u0026gt; 0) 1 else \u0026#34;Hello\u0026#34; val s = ar match { case i:Int =\u0026gt; \u0026#34;张三\u0026#34; case s:String =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34;   在模式匹配里,如果case都没有匹配成功就会报错,所以最后都要用 case _来兜底\n  模式匹配列表和元组时,变量可以绑定到它们的不同部分,比如(0,\u0026hellip;)以0开头的结构,或者(x,y)只包含x和y的结构等.这是通过提取器机制实现的.\n  BitInt和BigDecimal可以同时进行除法操作和取模操作,操作符是/%,但只有这种情况(可能是这两种运算的关联性强),其它的什么同时加减啊都是不支持的\n  在模式匹配中,匹配数组/列表/元组时, _* 操作符可以匹配剩余的全部元素\n  样例类是一种适合用于模式匹配的特殊类\n  样例class必须带括号,样例object必须不带括号\n  样例类好处:\n 创建实例时不需要new 免费得到toString,equals,hashCode和copy方法(浅克隆)    密封类通过关键字sealed声明,密封类的所有子类都必须在与该密封类相同的文件中定义.\n  ","permalink":"http://euthpic.github.io/tech/scala%E7%AE%80%E6%98%93%E7%AC%94%E8%AE%B0/","summary":"看完\u0026lt;快学scala\u0026gt;的一些笔记. 该书的课后练习参考答案: https://github.com/vybae/scala-hello 类型推断错误有时候ide检测不出来 ide联想到的api不能总是及时展","title":"Scala简易笔记"},{"content":"官方给出的部署教程比较简短,只有两行命令:\nnpm install -g yapi-cli --registry https://registry.npm.taobao.org yapi server 如果是新的Linux机器,跟着操作应该没什么问题,但是我在测试机和wsl上实操发现了一堆依赖问题,又是要更新node又是要装mongodb,所以不建议用yapi-cli来部署,能用docker尽量用docker.\n官方推荐的一个非官方docker部署方案: https://www.jianshu.com/p/a97d2efb23c5\n我部署完后发现这里还是有些小坑,所以顺带补充一些说明.\n运行 MongoDB # 创建存储卷 docker volume create mongo-data # 启动 MongoDB docker run -d \\  --name mongo-yapi \\  -v mongo-data:/data/db \\  -e MONGO_INITDB_ROOT_USERNAME=anoyi \\  -e MONGO_INITDB_ROOT_PASSWORD=anoyi.com \\  mongo # 进入mongodb docker exec -it mongo-yapi /bin/bash mongo #校验管理员账号,成功会返回1 use admin db.auth(\u0026#34;anoyi\u0026#34;,\u0026#34;anoyi.com\u0026#34;) 部署都尽可能用root账号,避免出现权限问题,如果命令中指定了目录,要确认对应的目录是否存在,否则也可能出问题.比如,这里要先创建好/data/db这个目录,否则校验mongodb账号的时候会失败.\nanoyi和anoyi.com就是mongodb的管理员账密,自己看着改\n获取YAPI镜像 docker pull registry.cn-hangzhou.aliyuncs.com/anoyi/yapi 自定义配置文件 vi /yapi/config.json # 输入以下内容 { \u0026#34;port\u0026#34;: \u0026#34;3000\u0026#34;, \u0026#34;adminAccount\u0026#34;: \u0026#34;admin@anoyi.com\u0026#34;, \u0026#34;timeout\u0026#34;:120000, \u0026#34;db\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;mongo\u0026#34;, \u0026#34;DATABASE\u0026#34;: \u0026#34;yapi\u0026#34;, \u0026#34;port\u0026#34;: 27017, \u0026#34;user\u0026#34;: \u0026#34;anoyi\u0026#34;, \u0026#34;pass\u0026#34;: \u0026#34;anoyi.com\u0026#34;, \u0026#34;authSource\u0026#34;: \u0026#34;admin\u0026#34; } } adminAccount即yapi的管理员账号,密码预设的是ymfe.org\n在官方的教程中有一段说只有超级管理员有权限创建分组,这里的超级管理员就是指的管理员,超级管理员可能是yapi历史的一个角色,但是现在没有了,文档没更新.官方的文档有很多落后的地方,比如其实任意新建的账号都能新建分组的.\nuser和pass是mongodb的管理员账密.\n初始化 YAPI 数据库索引及管理员账号 在/yapi的路径下输入\ndocker run -it --rm \\  --link mongo-yapi:mongo \\  --entrypoint npm \\  --workdir /yapi/vendors \\  -v $PWD/config.json:/yapi/config.json \\  registry.cn-hangzhou.aliyuncs.com/anoyi/yapi \\  run install-server 启动 Yapi 服务 在/yapi的路径下输入\ndocker run -d \\  --name yapi \\  --link mongo-yapi:mongo \\  --workdir /yapi/vendors \\  -p 3000:3000 \\  -v $PWD/config.json:/yapi/config.json \\  registry.cn-hangzhou.aliyuncs.com/anoyi/yapi \\  server/app.js 使用YAPI  访问： http://localhost:3000 登录账号：admin@anoyi.com 密码：ymfe.org  测接口的话要在chrome商店安装YApi-X插件\n","permalink":"http://euthpic.github.io/tech/%E9%83%A8%E7%BD%B2yapi/","summary":"官方给出的部署教程比较简短,只有两行命令: npm install -g yapi-cli --registry https://registry.npm.taobao.org yapi server 如果是新的Linux机器,跟着操作应该没什么问题,但是我在测试机和wsl上实操发现","title":"部署yapi"},{"content":" 堆内内存  索引缓冲index buffer 节点查询缓存Node Query Cache 分片请求缓存Shard Request Cache 字段缓存Fielddata Cache FST缓存 Segments Cache   堆外内存 Segments Memory 断路器和驱逐线  Elasticsearch内存分为on heap以及off heap两部分.Elasticsearch能控制的是On Heap内存部分,这部分由JVM管理;Off Heap由Lucene管理,负责缓存倒排索引数据空间(Segment Memory).\n堆内内存On Heap 堆内主要由写入缓冲,节点查询缓存,分片查询缓存,字段缓存和FST缓存组成.其中只有写入缓冲能被GC,其它缓存都是常驻内存的,满了之后通过LRU来驱逐.\n堆内存最大不能超过32GB。因为在Java中，所有对象都分配在堆上并由指针引用。32位的系统，堆内存大小最大为 4 GB。对于64位系统，可通过内存指针压缩（compressed oops）技术，依旧可以使用32位的指针来指向堆对象，这样可以大大节省CPU 内存带宽，提高操作效率。但当内存大小超过32G时候，对象指针就需要变大，操作效率就大大降低。\n写入缓冲index buffer 新数据写入时不直接写入磁盘,而是先写入index buffer,等缓冲区填满或是经过一定时间再提交为段.\n缓冲区默认大小为堆的10%\nindices.memory.index_buffer_size: 10% 节点查询缓存Node Query Cache Node Query Cache属于node-level级别的缓存,能被当前节点所有分片共享,用于缓存filter的查询结果,即文档在该filter下是true还是false.\n几个参数:\n# 默认大小 indices.queries.cache.size: 10% # 用来控制索引是否启用缓存,默认是开启的,在每个索引的setting里面配置 index.queries.cache.enabled: true # 是否在所有Segment上启用缓存,默认false indices.queries.cache.all_segments: false 分片请求缓存Shard Request Cache Shard Request Cache属于shard-level级别的缓存,默认开启.在索引搜索的query阶段,每个分片独自在本地搜索相关文档,并把结果缓存在这里.\nPUT /my-index-000001 { \u0026#34;settings\u0026#34;: { \u0026#34;index.requests.cache.enable\u0026#34;: true } } 默认情况下,分片请求缓存只缓存size=0的搜索请求,它不保存hits数组,只保存hits.total,aggregations和suggestions,这可能是为了控制缓存的大小.\n即使在索引setting上设置开启了缓存,如果size不为0的话缓存也不会生效.如果要强制为这些请求打开缓存,可以在请求时带上查询参数\nGET /my-index-000001/_search?request_cache=true { \u0026#34;size\u0026#34;: 10, \u0026#34;aggs\u0026#34;: { \u0026#34;popular_colors\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;colors\u0026#34; } } } } 对于还在频繁更新的索引最好不要启用该缓存.\n字段缓存Fielddata Cache 在对Text类型字段排序或聚合时,由于倒排索引保存在磁盘,把相关字段加载到Fielddata Cache可以加速查询.\n# 大小默认无限 indices.fielddata.cache.size: unbounded Fielddata Cache大小默认无限是个很容易踩到的坑,如果任由它无限膨胀,会把内存吃满,持续触发断路器,因此建议一定要给设置它.\n断路器默认是95%触发\nindices.breaker.total.limit: 95%  因此这里建议设置60%~80%.\nFST缓存 Segments Cache Segments Cache是segments FST(Finite State Transducer)数据的缓存，为倒排索引的二级索引.\nFST 永驻堆内内存，无法被 GC 回收。该部分内存无法设置大小，长期占用 50% ~ 70% 的堆内存，只能通过delete index，close index以及force-merge index释放内存。从7.3版本开始，FST由堆外内存来存储。\nES 底层存储采用 Lucene，写入时会根据原始数据的内容，分词，然后生成倒排索引。查询时，先通过 查询倒排索引找到数据地址(DocID)，再读取原始数据（行数据、列数据）。但由于 Lucene 会为原始数据中的每个词都生成倒排索引，数据量较大。所以倒排索引对应的倒排表被存放在磁盘上。这样如果每次查询都直接读取磁盘上的倒排表，再查询目标关键词，会有很多次磁盘 IO，严重影响查询性能。为了解磁盘 IO 问题，Lucene 引入排索引的二级索引 FST。实现原理上可以理解为前缀树，加速查询。\n堆外内存Off Heap 在设置ES堆内存时候至少要预留50%物理内存，因为这部分内存主要用做ES堆外内存，堆外内存主要用来来存储Lucene的segment.\nindex buffer写满了之后就会提交为segment,再通过fsync写回硬盘.\n最上面的图里有Segment Cache，Segment Memory以及Segment,容易混淆,这里对比解释下\n    位置 释义     Segment Cache On Heap Segment的FST字典树,并非Segment本身,用于加速对Segment的查询   Segment Memory Off Heap Segment的缓存,因为Segment很大,不可能全部放在内存里,因此不常用的Segment会被淘汰,这里只保存常用的和新提交的   Segment Disik 原始的Lucene数据,包括倒排索引,docValue等    断路器Circuit Breaker Elasticsearch 在 total，fielddata，request 三个层面上都设计有 circuit breaker 以保护进程不至于发生 OOM 事件。\n# 针对整个堆内存 indices.breaker.total.limit: 95% # 针对单次查询需要新增的fielddata,假如fielddata上限是60%,假设当前fielddata占用是0,当前查询需要新增50%,虽然总大小没超过上限,但是单次新增超了,查询会被拒绝 indices.breaker.fielddata.limit: 40% # 针对单次查询需要使用的总cache,包括fielddata,filter cache等,需要大于上面 indices.breaker.request.limit: 60% 上图中的断路器对应indices.breaker.total.limit,驱逐线对应indices.fielddata.cache.size.\n二者间的空间为可容纳的查询大小,如果需要进行的查询需使用较大内存,就要把这个空间调大\n","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","summary":"堆内内存 索引缓冲index buffer 节点查询缓存Node Query Cache 分片请求缓存Shard Request Cache 字段缓存Fielddata Cache FST缓存 Segments Cache 堆外内存 Segments Memory 断路器和","title":"elasticsearch内存模型"},{"content":"倒排索引 通常提到搜索我们想到的实现都是es,原因是它比MySQL高效得多.\n在MySQL中想检索包含特定关键词的数据,比如最近疫情爆发了,有关部门想找出家在某某街道的同学们,就可以写这样一条SQL:\nselect * from school where address like \u0026#39;%某某街道%\u0026#39; 很显然,像B+树这样的结构需要全表扫描,无法应付数据量较大的场景.\n而es通过倒排索引这种结构高效地解决了这类问题.\n倒排索引原理就像它的名字一样,即把正常索引的映射关系反转\n例如下面是正常索引和它对应的倒排索引\n   id code     1 b,c,e   2 a,c,e   3 c,b,d       code id     a 2   b 1,3   c 1,2,3   d 3   e 1,2    在正常索引中,如果想找出code包含b的数据得遍历全部数据逐条检查,而在倒排索引中只需找到b所在行即可.\n仔细观察,在上面的例子中不仅使用了倒排索引,还把字符串\u0026quot;b,c,e\u0026quot;切分成了三个单独的字母,因此才能建立三条独立的映射.\n可以说,分词是搜索技术的核心,是最关键的实现.\n分词器原理 对于初学者来讲分词并没有那么直观.\n例如某个文档的内容是Eating an apple a day keeps doctor away,然后用eat这个关键词去搜索却没能搜索出预期结果.这是典型从MySQL转过来的\u0026quot;like思维\u0026quot;.\n在es中分词是指把全文本转换成一系列单词(term)的过程,也叫做文本分析(analysis).这个过程通过分词器(analyzer)完成.\nAnalyzer组件 分词器包含下面三个组件,工作顺序自上而下\n character filters(0个或多个) : 对文本进行预处理,比如过滤html标签,标点符号等. tokenizers(恰好1个) : 将文本切分成一个个单词 token filters(0个或多个) : 将切分后的单词进行加工,例如大小写转换,同义词转换等.  这个过程可以想象成跟做菜一样,比如做红烧肉,先要对食材预处理,洗干净猪肉,拔掉上面的猪毛(character filters),然后把猪肉切成一块块(tokenizers),最后倒入锅中烹煮,上色调味(token filters)\nAnalyzer类型  Standard Analyzer - 默认分词器，按词切分，小写处理 Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理 Stop Analyzer - 小写处理，停用词过滤(the,a,is) Whitespace Analyzer - 按照空格切分，不转小写 Keyword Analyzer - 不分词，直接将输入当作输出 Patter Analyzer - 正则表达式，默认\\W+(非字符分割) Language - 提供了30多种常见语言的分词器 Customer Analyzer 自定义分词器(例如ik)  下面是自定义Analyzer的例子\nPUT /test_idx { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;pinyin_full_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;ik_pinyin_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;pinyin_full_filter\u0026#34; ] } } } } } 在创建mapping时可以指定两个分词器\nPOST /test_idx/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;keyword\u0026#34; } } }  analyzer: 写时分词,写入文档时对文档进行分词.文档写入后,分词结果不会随着分词器的更新而更新,只能手动重建索引 search_analyzer: 读时分词,对输入关键词进行分词.当这两个阶段分出来的词相同时视为搜索命中  中文分词 中文一般使用ik分词器.ik中有两种策略\n ik_smart : 做最粗粒度的拆分 ik_max_word : 做最细粒度的拆分  不难推导出ik_smart是ik_max_word的子集,ik_smart中不可能包含不存在于ik_max_word的单词.曾经我在腾讯云上就观察到违背这个结论的现象,提工单过去,然后发现果然是他们的BUG.\n一般search_analyzer使用ik_max_word,让文档能匹配更多的关键词,而analyzer使用ik_smart,让关键词更加精准.\n","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8/","summary":"倒排索引 通常提到搜索我们想到的实现都是es,原因是它比MySQL高效得多. 在MySQL中想检索包含特定关键词的数据,比如最近疫情爆发了,有关","title":"elasticsearch分词入门"},{"content":"基本背景 官方对es的定位是分布式搜索和分析引擎,它专为海量搜索而生.比如想往自己的网站加入一个搜索框,或者做日志的采集和检索时都可以使用es.\nMySQL自从引入ngram解析器之后也支持全文搜索,但是它在性能和功能完备性上与es都相去甚远.MySQL是单机强事务的,而es是天然分布式的;MySQL仅支持简单的分词策略,而es可以对搜索结构做复杂的自定义处理.\n一些概念  Shards: 分片，当索引上的数据量太大的时候，我们通常会将一个索引上的数据进行水平拆分，拆分出来的每个数据库叫作一个分片。 index: 索引,类似于MySQL的表,可以约束其内数据的类型和特性 settings: 定义了index的分片数,副本数等信息 mapping: 定义了index中每个字段的类型,分词器等信息,与setting一起组成index的建表语句 type: 7.0之后已经废弃的一个结构 document: 文档,类似于MySQL的行,即一条条的数据 field: 字段,类似于MySQL的列 analyzer: 分词器,决定如何对文档或输入进行拆分.常用的中文分词器是ik.  基础操作 创建索引 一般使用put方法创建索引\nPUT /product_info { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, \u0026#34;number_of_replicas\u0026#34;: 1 } } 可以在创建时带上mapping\nPUT /product_info { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;products\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;productName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;annual_rate\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34; }, \u0026#34;describe\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } } 新建数据 POST /product_info/_bulk {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;大健康天天理财\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.2200%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;180天定期理财，最低20000起投，收益稳定，可以自助选择消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;西部通宝\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.1100%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;90天定投产品，最低10000起投，每天收益到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;安详畜牧产业\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.3500%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;270天定投产品，最低40000起投，每天收益立即到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;5G设备采购月月盈\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.1200%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;90天定投产品，最低12000起投，每天收益到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;新能源动力理财\u0026#34;,\u0026#34;annual rate\u0026#34;:\u0026#34;3.0100%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;30天定投产品推荐，最低8000起投，每天收益会消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;微贷赚\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;2.7500%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;热门短期产品，3天短期，无须任何手续费用，最低500起投，通过短信提示获取收益消息\u0026#34;} 查询数据 基本查询\nGET /product_info/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;describe\u0026#34;: \u0026#34;每天收益到账消息推送\u0026#34; } } } //在describe字段上查找包含该关键词的文档 范围查询\nGET /product_info/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;annual_rate\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;3.0000%\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;3.1300%\u0026#34; } } } } //查找年化率在3%到3.13%之间的产品 其他复杂的查询语句:\nhttps://n3xtchen.github.io/n3xtchen/elasticsearch/2017/07/05/elasticsearch-23-useful-query-example\nhttps://www.imooc.com/article/71116\n删除数据 DELETE /product_info/doc/3 数据类型  字符串类型: keyword(不分词),text(分词) 整数类型: long,integer,short,byte 浮点型: float,half_float,scaled_float 时间类型: date 布尔类型: boolean 二进制类型: binary 区间类型: integer_range,long_range,float_range,double_range,date_range 数组: array 对象: object 嵌套类型: nested 地理位置: geo-point,point,geo-shape ip类型: ip 范围类型: completion 令牌计数类型: token_count 附件类型: attachment 抽取类型: percolator  ","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%9F%BA%E7%A1%80%E7%AF%87/","summary":"基本背景 官方对es的定位是分布式搜索和分析引擎,它专为海量搜索而生.比如想往自己的网站加入一个搜索框,或者做日志的采集和检索时都可以使用es","title":"elasticsearch基础篇"},{"content":" bm25模型 查询时权重提升 修改查询结构 修改评分  constant_score查询 函数评分 function_score 脚本评分 script_score 更改相似度模型   多字段排序  前面介绍了es的分词原理,分词决定了如何找到匹配的文档,而在全文搜索引擎里面,结果的展示相当重要,不仅要找到结果,还要对结果进行适当的排序.\nes通过相似度对文档进行打分,默认情况下按照评分高低排序,不过如果使用term精确查询的话就不会计算相似度.\nBM25算法 es在5.0之后默认采用的相似度算法是BM25,而需要注意的是官方中文文档是基于2.x版本写的,当时使用的是tf/idf算法.\nbm25公式如下\n公式的详细介绍可以参考lucene官网\nhttps://lucene.apache.org/core/8_0_0/core/org/apache/lucene/search/similarities/BM25Similarity.html\n不过这里并不打算深入分析这条公式,我们可以通过几个简单的概念学会如何利用它\nf(qi,D)表示的是一个词对一个文档的相关性的权重,es使用的公式是idf.\n在idf中有三个概念\n 词频: tf(term frequency),该词在文档中出现的次数越多该值越高 逆向文档频率: idf(inverse document frequency),该词在索引中所有文档出现的次数越多该值越低,可表示关键词的特异程度 字段长度归一化(field-length norm): 字段长度越短,字段权重越高,表示关键词在字段中的占比  也就是tf越高,idf和字段长度归一化越低,文档的评分越高\n在tf/idf中并没有对词频上限作出限制,因此当关键词在某个字段大量出现时文档的得分就会非常高,但是这些得分几乎都来自这个字段,相当于这个字段的权重被提高了,其他字段的影响被削弱了.\n所以bm25引入了k1和b两个可调节参数来惩罚上述的情况. k1控制着词频结果在词频饱和度中的上升速度.默认值为1.2,值越小饱和度变化越快.\nb控制着字段长度归一化的作用.0表示禁用归一化,1表示完全归一化,默认值是0.75\n下面可以看到在词频上升过程中两种模型的曲线变化\n调整排序 下面介绍一些调整文档最终排序的方法\n查询时权重提升 当进行多字段查询时,可以用boost独立控制每个字段的权重.要注意的是权重的比例不直接等于最终评分的比例.\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } }, { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } ] } } } 修改查询结构 es的几种类型的查询是可以自由组合的\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }} ] } } } GET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }}, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }} ] } } ] } } } constant_score查询 有时候我们想使用分词,但又不希望进行复杂的评分,就可以通过constant_score来禁用bm25,这样每个命中字段的得分都是1\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;wifi\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;garden\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;pool\u0026#34; }} }} ] } } } 上面是官网中文文档的例子,不过在6.x版本之后,constant_score里面不能使用query查询,只能使用filter,但由于filter本身恰好是不打分的,所以constant_score现在很少会使用了.\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user.id\u0026#34;: \u0026#34;kimchy\u0026#34; } }, \u0026#34;boost\u0026#34;: 1.2 } } } function_score查询 如果想使用自定义的公式替代bm25,则可以使用function_score.\nfunction_score允许为每个主查询的评分额外应用一个函数,可以修改或者完全替换原来的公式.\n内置的函数包括\n weight : 为每个文档应用一个简单而不被规范化的权重提升值：当 weight 为 2 时，最终结果为 2 * _score 。 field_value_factor :使用这个值来修改 _score ，如将 popularity 或 votes （受欢迎或赞）作为考虑因素。最终结果为 field_value_factor * _score random_score :为每个用户都使用一个不同的随机评分对结果排序，但对某一具体用户来说，看到的顺序始终是一致的。 script_score : 自定义脚本,最终武器,脚本里可以使用tf,idf等变量  GET /blogposts/post/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34; } } } } 上面的语句表示查询在title和content里搜索和\u0026rsquo;popularity\u0026rsquo;相关的内容,并且votes越高评分越高,最终评分为_socore * votes.\n上面的查询还可以改进下,如果点赞数太高,可能相关度低的文章也会排名靠前,可以对点赞数取log,此外votes有可能为空值,得加下默认值\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ], \u0026#34;factor\u0026#34;: 1.2, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;missing\u0026#34;: 1 } } } } function_score默认是与原始评分乘算,可以用boost_mode来改写成加算\nGET /blogposts/post/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 0.1 }, \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } script_score查询 GET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;elasticsearch\u0026#34; } }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;Math.log(2 + doc[\u0026#39;my-int\u0026#39;].value)\u0026#34; } } } } } source里面是自定义脚本,可以用java或者groovy语句\n一个简单的利用tf和idf的脚本如下\n{ \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;double tf = Math.sqrt(doc.freq); double idf = 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\u0026#34; } } 脚本里可以利用的上下文变量可参考官网https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-similarity-context.html\n更改相似度模型 可以在索引设置里自定义相似度算法,内置的算法有BM25,DFR,DFI,IB等\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_bm25\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;BM25\u0026#34;, \u0026#34;b\u0026#34;: 0 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_bm25\u0026#34; }, \u0026#34;body\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;BM25\u0026#34; } } } } } 上面自定义了一个bm25算法,b=0表示禁用了归一化\n多字段排序 es默认使用_socre排序,有时候搜索结果可能有很多文档得分相同,需要加些规则让它们在这种情况下也能排序固定\n{ \u0026#34;sort\u0026#34;: [ { \u0026#34;_score\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;time\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } ","permalink":"http://euthpic.github.io/tech/elasticsearch%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%8E%9F%E7%90%86/","summary":"bm25模型 查询时权重提升 修改查询结构 修改评分 constant_score查询 函数评分 function_score 脚本评分 script_score 更改相似度模型 多字段排序 前面介绍了es的分","title":"elasticsearch相似度与排序原理"},{"content":" 一次写入过程 一次读取过程 (主要是选择协调节点,不用和倒排索引,排序联系) 一次搜索过程 (主要是排序) 不变性 动态更新索引(与MySQL的插入数据进行比较分析) 如何删除和更新数据 近实时搜索 持久化 translog 段合并(提高搜索效率)  节点与分片 es的节点分为三种类型:主节点,数据节点,客户端节点.\n主节点主要负责集群操作相关的内容,如创建或删除索引.\n数据节点负责对文档进行读写.\n客户端节点只能处理路由请求,转发集群操作等.\n索引可以设置多个分片(副本),依旧是分布式主流的\u0026quot;主写副读\u0026quot;的模式,保存数据时主分片先写,然后再同步到各副分片上.读取数据时,主分片收到请求,然后把请求轮询到某个副本分片上,由该分片把文档返回给协调节点.\n每个数据节点都知道集群文档的分布情况,客户端操作文档时可以对任意一个数据节点发送请求,不管文档的主分片在不在该节点上.\n收到请求的节点叫做协调节点,它负责把请求转发给文档主分片所在的节点上.\n一次写入过程  客户端向Node1发送写请求 Node1根据docId和路由算法计算出文档主分片所在节点Node3,把请求转发给Node3 Node3执行请求,写入文档.成功后把数据同步到Node1和Node2的副本分片上 当大部分副本分片都成功时(根据一致性设置判定),视为写入成功,Node3向协调节点Node1报告,最后Node1给客户端返回成功.  使用bulk操作时,主分片是每进行一个操作,就立即异步复制到副本分片,再执行下一个操作,而不是全部执行完了再同步给副本.\n一次读取过程  客户端向Node1发送写请求 Node1根据docId和路由算法计算出文档主分片所在节点Node3,把请求转发给Node3 Node3的主分片接收请求,然后轮询所有副本分片,选择其中一个负责提供数据(有的文章说轮询的范围也包括主分片,这是不对的,不符合主写副读的模式)  官方文档有一段意思写得不太明白:\n\u0026ldquo;在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的.\u0026rdquo;\n我的理解是,当主分片写完,还没同步到副本分片时就进行查询,副本分片会给主分片返回查询失败,然后由主分片负责提供数据.\n一次搜索过程 与上面的读取过程区分开,读取是拿到了docId,因此容易确认文档位置,而搜索是没有docId的.\n可以用query then fetch概括,即先在倒排索引里搜索得到docId,然后再执行读取过程得到文档内容,这里很像MySQL的回表.\n查询阶段  客户端发送请求,协调节点Node3收到后会创建一个大小为from+size的空队列 Node3将搜索请求转发到索引的所有分片,每个分片进行本地搜索并得到一个大小同样为from+size的队列. 各个分片将结果队列返回给Node3,结果包含的内容仅为docId和其对应的排序分值 Node合并各个节点的结果并排序  取回阶段  Node3确认需要返回哪些文档,然后对相关分片发送multi-get请求 接下来就跟查询过程是一样的了  这里需要注意的是,即使我们最终需要的结果大小只是size,但是我们需要对n * (from + size) 的数据进行排序,当from很大的时候执行过程就相当耗时了.所以这里要避免深分页(Deep Pagination),在业务场景中考虑用search after或者scrollbar来替代.\n分片内部原理 下面谈谈文档具体是怎样写入到磁盘的.\nes的分片由若干个segment组成,segment可以看做是es存储的最小管理单元,包含倒排索引和文档原始内容等.\nSegment的不变性 segment一旦写入磁盘后就不可改变.好处如下:\n 不需要锁,不用担心并发修改的情况 由于segment不变,所以相关的缓存,比如当它被读入内存后,或者filter缓存可以一直有效,不需要进行像MySQL刷脏页那种复杂操作.  坏处是一旦索引要修改,就只能整个重建,索引规模越大耗时越长.\n增量更新 为了避免对索引的全量更新,es引入按段搜索的概念,将segment分成一个个小的单元\nes的数据不直接写入磁盘,而是先写入内存的buffer中\n当buffer满了或者经过一段时间,es会将buffer提交成一个新的段,并清空buffer\n进行查询时会对分片内所有segment按顺序查询,所以为了提高查询效率,segment的数量要尽量小(segment内部的倒排索引是按字典树组织的,查询较快)\n进行删除或更新时,由于segment不可变,es会生成一个.del文件标记出被删除文档的segment.\n被标记的文档仍然能被搜索得到(逻辑删除),但是在得到最终结果之前会被过滤掉.\nFST二级索引 由于 Lucene 会为原始数据中的每个词都生成倒排索引，数据量较大。所以倒排索引对应的倒排表被存放在磁盘上。这样如果每次查询都直接读取磁盘上的倒排表，再查询目标关键词，会有很多次磁盘 IO，严重影响查询性能。\n为了解磁盘 IO 问题，Lucene 引入排索引的二级索引 FST Finite State Transducer 。原理上可以理解为前缀树，加速查询。\n其原理如下：\n 将原本的分词表，拆分成多个 Block ，每个 Block 会包含 25 ~ 48 个词（Term）。图中做了简单示意，Allen 和 After组成一个 Block 。 将每个 Block 中所有词的公共前缀抽取出来，如 Allen 和 After 的公共前缀是 A 。 将各个 Block 的公共前缀，按照类似前缀树的逻辑组合成 FST，其叶子节点携带对应 Block 的首地址 。（实际 FST 结构更为复杂，前缀后缀都有压缩，来降低内存占用量） 为了加速查询，FST 永驻堆内内存，无法被 GC 回收。 用户查询时，先通过关键词（Term）查询内存中的 FST ，找到该 Term 对应的 Block 首地址。再读磁盘上的分词表，将该 Block 加载到内存，遍历该 Block ，查找到目标 Term 对应的DocID。再按照一定的排序规则，生成DocID的优先级队列，再按该队列的顺序读取磁盘中的原始数据（行存或列存）。  Refresh与近实时性 像之前提到的,数据先是被写入内存buffer中,然后再提交为新的段,最后再通过fsync写入磁盘.\n由于fsync操作代价大,所以操作间隔一般都很长,如果等到fsync结束后才能搜索到写入的文档就太慢了.\n因此es引入一个refresh的概念,即写入和打开一个新的segment的轻量过程,refresh后新写入的数据就生效了,也就是数据从内存buffer提交到segment后(写入磁盘前)就能搜索得到.\n所以我们称es的搜索是准实时的,这个间隔就是refresh的间隔,由参数refresh_interval控制\nTranslog与持久化 为了保障性能,数据fsync到磁盘的间隔较长,如果期间服务器宕机,这段时间的数据可能丢失.\n于是es引入了Translog机制(类似MySQL的redolog),把每次对索引的操作都记录到translog里面,即便宕机,也能通过translog对数据进行恢复.\n与MySQL不同的是,es是先写Lucene,再写translog,原因是写入Lucene失败概率会高一些,如果先写translog,当写入Lucene失败后得回滚translog.\n当refresh完成后,内存buffer会清空,但是translog不会,translog只在flush后清空.\ntranslog也同样是先写内存,再fsync到磁盘,写入磁盘之前仍然可能丢数据.如果对性能要求高,fsync的间隔就拉长些,如果对安全性要求高,fsync可以设置为同步操作.通过\u0026quot;index.translog.durability\u0026quot;和\u0026quot;index.translog.sync_interval\u0026quot;两个参数控制.\n可能有人会疑惑,写Lucene要fsync,写translog也要fsync,那么translog为什么可以提高性能呢?因为lucene(segment)的数据要大一些,而translog是增量写,fsync的时间会短些.\n段合并 每次refresh都会产生一个新的segment,这会导致segment的数量暴增,因此es开了个后台线程去进行段合并.\n 段合并自动进行,优先选择一部分大小相似的段来合并成更大的新的段.旧的段此时没有删除,不影响其它正常操作. 当合并后新的段提交后,旧的段会被删除(物理删除),合并结束.  Lucene文件 ","permalink":"http://euthpic.github.io/tech/elasticsearch%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86/","summary":"一次写入过程 一次读取过程 (主要是选择协调节点,不用和倒排索引,排序联系) 一次搜索过程 (主要是排序) 不变性 动态更新索引(与MySQL的插入数据","title":"elasticsearch读写原理"},{"content":"为什么是hugo? 其实无所谓hugo还是hexo,我是为了paperMod这个皮肤才搭的这个博客.它排版简洁素雅,让注意力容易集中到内容本身,而且我特别喜欢点击文章的这个特效,有种按钮按下去的感觉.\n这些天在\u0026quot;十年之约\u0026quot;和\u0026quot;开往\u0026quot;中逛了很多博客,也算增长了一些见力.很多博客整得花里胡哨的,这种特效那种特效一个劲儿的往上面加,动画加载时间拉得那么长,完全不顾游客体验.就像刚学会化妆的小姑娘,妆容夸张而突兀,如果玩了一段时间博客后还是这种调调,也实在太没品味了.你品品,挂个天气在博客上面是想干嘛呢?\n回归正题,本文整理了搭建hugo,使用paperMod皮肤并发布到github page的简单过程.你看官网也是可以的,不过有些小坑官网的新手教程没提.\nhugo中文文档\npaperMod\n安装Hugo 二进制安装（推荐：简单、快速）  到 Hugo Releases 下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe） 把hugo.exe加到环境变量里  生成站点 $ hugo new site /my-blog $ cd /my-blog 站点目录结构:\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml content是文章存放的目录,这里有个小坑,文档里面示范用的hyde皮肤能扫描content目录下的全部md文档,但是paperMod不行,paperMod只能读取/content/posts/下面的md文档,然后我搜了一下,还有些皮肤只支持/content/post,这里不同皮肤的规则比较混乱,使用时要注意文档位置是否正确.\nconfig.toml是配置文件,hugo官方用的是toml格式,有点冷门,然后paperMod又推荐回yml配置,得把toml改成yml.\n这里还需要自己创建个theme目录放皮肤.\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ ▸ theme/ config.toml 创建文章 创建第一篇文章放到post目录.\n$ hugo new posts/first.md 通过hugo创建的文章会有个默认头\ndate = \u0026quot;2015-10-25T08:36:54-07:00\u0026quot; title = \u0026quot;about\u0026quot; draft = true draft = true 表示草稿,得把这里改成false或者删掉后发布github才可见,或者想本地预览的话hugo server得加个\u0026ndash;buildDrafts参数\n安装皮肤 挑选一个皮肤然后clone下来.我有选择困难症,所以一开始是跳过下载皮肤直接运行hugo的,结果发现文章显示不了,一定得先下载个皮肤\n$ cd /theme $ git clone https://github.com/adityatelange/hugo-PaperMod.git 运行Hugo 在站点根目录启动本地预览\n$ hugo server --theme=hugo-PaperMod --buildDrafts 在config.yml里面指定皮肤后这里就不需要了.\n浏览器里打开： http://localhost:1313\n部署到Github 假设你需要部署在 GitHub Pages 上，首先在GitHub上创建一个Repository，命名为：xxxx.github.io （xxxx替换为你的github用户名）。\n在站点根目录执行 Hugo 命令生成最终页面：\n$ hugo --theme=hugo-PaperMod --baseUrl=\u0026#34;http://xxxx.github.io/\u0026#34; （注意，以上命令并不会生成草稿页面，如果未生成任何文章，请去掉文章头部的 draft=true 再重新生成。）\n如果一切顺利，所有静态页面都会生成到 public 目录，将pubilc目录里所有文件 push 到刚创建的Repository的 master 分支。\n$ cd public $ git init $ git remote add origin https://github.com/xxxx/xxxx.github.io.git $ git add -A $ git commit -m \u0026#34;first commit\u0026#34; $ git push -u origin master 浏览器里访问：http://xxxx.github.io/\n然后创建另一个github项目保存我们的源码,不要忘了在.gitignore里面把public目录加上.\n如果你需要在另一个设备上下载源码然后运行它,也不要忘了重新在theme目录里面重新clone皮肤\n","permalink":"http://euthpic.github.io/tech/%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/","summary":"为什么是hugo? 其实无所谓hugo还是hexo,我是为了paperMod这个皮肤才搭的这个博客.它排版简洁素雅,让注意力容易集中到内容本身","title":"用hugo搭建自己的博客"},{"content":"","permalink":"http://euthpic.github.io/posts/%E8%BF%99%E9%87%8C%E5%95%A5%E9%83%BD%E6%B2%A1%E6%9C%89/","summary":"","title":"这里啥都没有"}]