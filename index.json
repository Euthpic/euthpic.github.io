[{"content":"https的出现是为了修复http安全方面的隐患.\nhttp并未对传输的报文进行充分的校验,它能做的只有把数据放到body部分,加密后再发送出去,但是中途可以被人拦截,篡改,冒认,接收方无法分辨收到的是否原始报文,也无法分辨是否正在与本人进行通信.这个叫中间人攻击.\nhttps比http多的的这个s就是ssl/tls.ssl是tls的前身,tls是ssl的升级,我简单搜索了下,并未看见有哪篇文章详细介绍它们的区别,所以这应该不重要,我们只需记住tls是更加安全的ssl即可.\ntls的作用是对通信的报文进行加密和验证,既然无法直接从收到的信息确认对方身份和信息是否原始无误,那么那么就要引入可靠的第三方来帮我们鉴定,这个第三方就是CA证书中心.\n与此同时还有几个核心概念:\n 密钥对: 分为公钥和私钥,公钥公开并存放在CA中心,私钥则自己保管任何人不可知,密钥对一一对应,服务端用自己的私钥进行对报文摘要进行加密得到数字签名,客户端从CA中心得到公钥,然后对签名进行解密,将解密后的摘要与报文作比较即可判断报文有无篡改以及对方身份是否合法.然后客户端用服务端公钥对自己将要发送的报文进行加密,服务端收到后用自己的私钥进行解密得到消息,这份报文里包含客户端的私钥,下一次服务端发送消息时就可以带上客户端的私钥用来证明自己的身份. 数字签名: 对报文本体进行hash计算得到一份摘要,然后用自己的私钥对摘要进行加密得到数字签名,接收方收到签名后使用发送方的公钥进行解密,再与报文内容进行比较即可确认对方身份和报文有无被篡改.由于私钥只有本人持有,所以数字签名无法伪造. 数字证书: 由CA中心颁发,包含发布机构,有效日期,证书持有者,持有者公钥,数字签名等信息,用于验证证书持有者身份.  具体流程如下:\n 客户端请求与服务端建立连接,并告知对方自己支持的hash算法和协议版本. 服务端用私钥进行签名,把证书发给客户端 客户端收到证书后前往CA中心求证,用CA中心提供的公钥来解密签名并与报文内容进行对比,如果无误则说明对方身份合法可信,报文未收到破坏.值得一提的是CA中心同样有自己的证书,它的身份由上级CA中心鉴定.我们一定要找到一个一定可靠不需鉴定的CA中心,那么最顶级的CA中心在哪呢?答案是已经内置在操作系统和浏览器里面. 确认证书没有问题后,客户端把自己的私钥放进报文,并用服务端的公钥进行加密后发送回去,由于只有持有私钥的服务端才能解密,因此不用担心消息泄露 服务端收到消息后用自己的私钥解密得到客户端的私钥,然后通知客户端连接已经建立,由于双方都持有客户端的私钥,所以从这里转为对称加密,消息都用客户端的私钥加密解密.这一步也不用担心被攻击,中间人得到消息但缺乏客户端私钥,无法解密,他也无法伪造成客户端去给服务端发消息  ","permalink":"http://euthpic.github.io/tech/https%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6%E5%92%8C%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D/","summary":"https的出现是为了修复http安全方面的隐患. http并未对传输的报文进行充分的校验,它能做的只有把数据放到body部分,加密后再发送出","title":"Https,数字证书和数字签名"},{"content":"官方给出的部署教程比较简短,只有两行命令:\nnpm install -g yapi-cli --registry https://registry.npm.taobao.org yapi server 如果是新的Linux机器,跟着操作应该没什么问题,但是我在测试机和wsl上实操发现了一堆依赖问题,又是要更新node又是要装mongodb,所以不建议用yapi-cli来部署,能用docker尽量用docker.\n官方推荐的一个非官方docker部署方案: https://www.jianshu.com/p/a97d2efb23c5\n我部署完后发现这里还是有些小坑,所以顺带补充一些说明.\n运行 MongoDB # 创建存储卷 docker volume create mongo-data # 启动 MongoDB docker run -d \\  --name mongo-yapi \\  -v mongo-data:/data/db \\  -e MONGO_INITDB_ROOT_USERNAME=anoyi \\  -e MONGO_INITDB_ROOT_PASSWORD=anoyi.com \\  mongo # 进入mongodb docker exec -it mongo-yapi /bin/bash mongo #校验管理员账号,成功会返回1 use admin db.auth(\u0026#34;anoyi\u0026#34;,\u0026#34;anoyi.com\u0026#34;) 部署都尽可能用root账号,避免出现权限问题,如果命令中指定了目录,要确认对应的目录是否存在,否则也可能出问题.比如,这里要先创建好/data/db这个目录,否则校验mongodb账号的时候会失败.\nanoyi和anoyi.com就是mongodb的管理员账密,自己看着改\n获取YAPI镜像 docker pull registry.cn-hangzhou.aliyuncs.com/anoyi/yapi 自定义配置文件 vi /yapi/config.json # 输入以下内容 { \u0026#34;port\u0026#34;: \u0026#34;3000\u0026#34;, \u0026#34;adminAccount\u0026#34;: \u0026#34;admin@anoyi.com\u0026#34;, \u0026#34;timeout\u0026#34;:120000, \u0026#34;db\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;mongo\u0026#34;, \u0026#34;DATABASE\u0026#34;: \u0026#34;yapi\u0026#34;, \u0026#34;port\u0026#34;: 27017, \u0026#34;user\u0026#34;: \u0026#34;anoyi\u0026#34;, \u0026#34;pass\u0026#34;: \u0026#34;anoyi.com\u0026#34;, \u0026#34;authSource\u0026#34;: \u0026#34;admin\u0026#34; } } adminAccount即yapi的管理员账号,密码预设的是ymfe.org\n在官方的教程中有一段说只有超级管理员有权限创建分组,这里的超级管理员就是指的管理员,超级管理员可能是yapi历史的一个角色,但是现在没有了,文档没更新.官方的文档有很多落后的地方,比如其实任意新建的账号都能新建分组的.\nuser和pass是mongodb的管理员账密.\n初始化 YAPI 数据库索引及管理员账号 在/yapi的路径下输入\ndocker run -it --rm \\  --link mongo-yapi:mongo \\  --entrypoint npm \\  --workdir /yapi/vendors \\  -v $PWD/config.json:/yapi/config.json \\  registry.cn-hangzhou.aliyuncs.com/anoyi/yapi \\  run install-server 启动 Yapi 服务 在/yapi的路径下输入\ndocker run -d \\  --name yapi \\  --link mongo-yapi:mongo \\  --workdir /yapi/vendors \\  -p 3000:3000 \\  -v $PWD/config.json:/yapi/config.json \\  registry.cn-hangzhou.aliyuncs.com/anoyi/yapi \\  server/app.js 使用YAPI  访问： http://localhost:3000 登录账号：admin@anoyi.com 密码：ymfe.org  测接口的话要在chrome商店安装YApi-X插件\n","permalink":"http://euthpic.github.io/tech/%E9%83%A8%E7%BD%B2yapi/","summary":"官方给出的部署教程比较简短,只有两行命令: npm install -g yapi-cli --registry https://registry.npm.taobao.org yapi server 如果是新的Linux机器,跟着操作应该没什么问题,但是我在测试机和wsl上实操发现","title":"部署yapi"},{"content":" bm25模型 查询时权重提升 修改查询结构 修改评分  constant_score查询 函数评分 function_score 脚本评分 script_score 更改相似度模型   多字段排序  前面介绍了es的分词原理,分词决定了如何找到匹配的文档,而在全文搜索引擎里面,结果的展示相当重要,不仅要找到结果,还要对结果进行适当的排序.\nes通过相似度对文档进行打分,默认情况下按照评分高低排序,不过如果使用term精确查询的话就不会计算相似度.\nBM25算法 es在5.0之后默认采用的相似度算法是BM25,而需要注意的是官方中文文档是基于2.x版本写的,当时使用的是tf/idf算法.\nbm25公式如下\n公式的详细介绍可以参考lucene官网\nhttps://lucene.apache.org/core/8_0_0/core/org/apache/lucene/search/similarities/BM25Similarity.html\n不过这里并不打算深入分析这条公式,我们可以通过几个简单的概念学会如何利用它\nf(qi,D)表示的是一个词对一个文档的相关性的权重,es使用的公式是idf.\n在idf中有三个概念\n 词频: tf(term frequency),该词在文档中出现的次数越多该值越高 逆向文档频率: idf(inverse document frequency),该词在索引中所有文档出现的次数越多该值越低,可表示关键词的特异程度 字段长度归一化(field-length norm): 字段长度越短,字段权重越高,表示关键词在字段中的占比  也就是tf越高,idf和字段长度归一化越低,文档的评分越高\n在tf/idf中并没有对词频上限作出限制,因此当关键词在某个字段大量出现时文档的得分就会非常高,但是这些得分几乎都来自这个字段,相当于这个字段的权重被提高了,其他字段的影响被削弱了.\n所以bm25引入了k1和b两个可调节参数来惩罚上述的情况. k1控制着词频结果在词频饱和度中的上升速度.默认值为1.2,值越小饱和度变化越快.\nb控制着字段长度归一化的作用.0表示禁用归一化,1表示完全归一化,默认值是0.75\n下面可以看到在词频上升过程中两种模型的曲线变化\n调整排序 下面介绍一些调整文档最终排序的方法\n查询时权重提升 当进行多字段查询时,可以用boost独立控制每个字段的权重.要注意的是权重的比例不直接等于最终评分的比例.\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;quick brown fox\u0026#34;, \u0026#34;boost\u0026#34;: 2 } } }, { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;quick brown fox\u0026#34; } } ] } } } 修改查询结构 es的几种类型的查询是可以自由组合的\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }} ] } } } GET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;quick\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;fox\u0026#34; }}, { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;brown\u0026#34; }}, { \u0026#34;term\u0026#34;: { \u0026#34;text\u0026#34;: \u0026#34;red\u0026#34; }} ] } } ] } } } constant_score查询 有时候我们想使用分词,但又不希望进行复杂的评分,就可以通过constant_score来禁用bm25,这样每个命中字段的得分都是1\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;wifi\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;garden\u0026#34; }} }}, { \u0026#34;constant_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;pool\u0026#34; }} }} ] } } } 上面是官网中文文档的例子,不过在6.x版本之后,constant_score里面不能使用query查询,只能使用filter,但由于filter本身恰好是不打分的,所以constant_score现在很少会使用了.\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;user.id\u0026#34;: \u0026#34;kimchy\u0026#34; } }, \u0026#34;boost\u0026#34;: 1.2 } } } function_score查询 如果想使用自定义的公式替代bm25,则可以使用function_score.\nfunction_score允许为每个主查询的评分额外应用一个函数,可以修改或者完全替换原来的公式.\n内置的函数包括\n weight : 为每个文档应用一个简单而不被规范化的权重提升值：当 weight 为 2 时，最终结果为 2 * _score 。 field_value_factor :使用这个值来修改 _score ，如将 popularity 或 votes （受欢迎或赞）作为考虑因素。最终结果为 field_value_factor * _score random_score :为每个用户都使用一个不同的随机评分对结果排序，但对某一具体用户来说，看到的顺序始终是一致的。 script_score : 自定义脚本,最终武器,脚本里可以使用tf,idf等变量  GET /blogposts/post/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34; } } } } 上面的语句表示查询在title和content里搜索和\u0026rsquo;popularity\u0026rsquo;相关的内容,并且votes越高评分越高,最终评分为_socore * votes.\n上面的查询还可以改进下,如果点赞数太高,可能相关度低的文章也会排名靠前,可以对点赞数取log,此外votes有可能为空值,得加下默认值\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ], \u0026#34;factor\u0026#34;: 1.2, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;missing\u0026#34;: 1 } } } } function_score默认是与原始评分乘算,可以用boost_mode来改写成加算\nGET /blogposts/post/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;popularity\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title\u0026#34;, \u0026#34;content\u0026#34; ] } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;votes\u0026#34;, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;factor\u0026#34;: 0.1 }, \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } script_score查询 GET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;elasticsearch\u0026#34; } }, \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;Math.log(2 + doc[\u0026#39;my-int\u0026#39;].value)\u0026#34; } } } } } source里面是自定义脚本,可以用java或者groovy语句\n一个简单的利用tf和idf的脚本如下\n{ \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;double tf = Math.sqrt(doc.freq); double idf = 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\u0026#34; } } 脚本里可以利用的上下文变量可参考官网https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-similarity-context.html\n更改相似度模型 可以在索引设置里自定义相似度算法,内置的算法有BM25,DFR,DFI,IB等\nPUT /my_index { \u0026#34;settings\u0026#34;: { \u0026#34;similarity\u0026#34;: { \u0026#34;my_bm25\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;BM25\u0026#34;, \u0026#34;b\u0026#34;: 0 } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;my_bm25\u0026#34; }, \u0026#34;body\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;similarity\u0026#34;: \u0026#34;BM25\u0026#34; } } } } } 上面自定义了一个bm25算法,b=0表示禁用了归一化\n多字段排序 es默认使用_socre排序,有时候搜索结果可能有很多文档得分相同,需要加些规则让它们在这种情况下也能排序固定\n{ \u0026#34;sort\u0026#34;: [ { \u0026#34;_score\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;time\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } }, { \u0026#34;_id\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } ","permalink":"http://euthpic.github.io/tech/elasticsearch%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%8E%9F%E7%90%86/","summary":"bm25模型 查询时权重提升 修改查询结构 修改评分 constant_score查询 函数评分 function_score 脚本评分 script_score 更改相似度模型 多字段排序 前面介绍了es的分","title":"elasticsearch相似度与排序原理"},{"content":" 一次写入过程 一次读取过程 (主要是选择协调节点,不用和倒排索引,排序联系) 一次搜索过程 (主要是排序) 不变性 动态更新索引(与MySQL的插入数据进行比较分析) 如何删除和更新数据 近实时搜索 持久化 translog 段合并(提高搜索效率)  节点与分片 es的节点分为三种类型:主节点,数据节点,客户端节点.\n主节点主要负责集群操作相关的内容,如创建或删除索引.\n数据节点负责对文档进行读写.\n客户端节点只能处理路由请求,转发集群操作等.\n索引可以设置多个分片(副本),依旧是分布式主流的\u0026quot;主写副读\u0026quot;的模式,保存数据时主分片先写,然后再同步到各副分片上.读取数据时,主分片收到请求,然后把请求轮询到某个副本分片上,由该分片把文档返回给协调节点.\n每个数据节点都知道集群文档的分布情况,客户端操作文档时可以对任意一个数据节点发送请求,不管文档的主分片在不在该节点上.\n收到请求的节点叫做协调节点,它负责把请求转发给文档主分片所在的节点上.\n一次写入过程  客户端向Node1发送写请求 Node1根据docId和路由算法计算出文档主分片所在节点Node3,把请求转发给Node3 Node3执行请求,写入文档.成功后把数据同步到Node1和Node2的副本分片上 当大部分副本分片都成功时(根据一致性设置判定),视为写入成功,Node3向协调节点Node1报告,最后Node1给客户端返回成功.  使用bulk操作时,主分片是每进行一个操作,就立即异步复制到副本分片,再执行下一个操作,而不是全部执行完了再同步给副本.\n一次读取过程  客户端向Node1发送写请求 Node1根据docId和路由算法计算出文档主分片所在节点Node3,把请求转发给Node3 Node3的主分片接收请求,然后轮询所有副本分片,选择其中一个负责提供数据(有的文章说轮询的范围也包括主分片,这是不对的,不符合主写副读的模式)  官方文档有一段意思写得不太明白:\n\u0026ldquo;在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的.\u0026rdquo;\n我的理解是,当主分片写完,还没同步到副本分片时就进行查询,副本分片会给主分片返回查询失败,然后由主分片负责提供数据.\n一次搜索过程 与上面的读取过程区分开,读取是拿到了docId,因此容易确认文档位置,而搜索是没有docId的.\n可以用query then fetch概括,即先在倒排索引里搜索得到docId,然后再执行读取过程得到文档内容,这里很像MySQL的回表.\n查询阶段  客户端发送请求,协调节点Node3收到后会创建一个大小为from+size的空队列 Node3将搜索请求转发到索引的所有分片,每个分片进行本地搜索并得到一个大小同样为from+size的队列. 各个分片将结果队列返回给Node3,结果包含的内容仅为docId和其对应的排序分值 Node合并各个节点的结果并排序  取回阶段  Node3确认需要返回哪些文档,然后对相关分片发送multi-get请求 接下来就跟查询过程是一样的了  这里需要注意的是,即使我们最终需要的结果大小只是size,但是我们需要对n * (from + size) 的数据进行排序,当from很大的时候执行过程就相当耗时了.所以这里要避免深分页(Deep Pagination),在业务场景中考虑用search after或者scrollbar来替代.\n分片内部原理 下面谈谈文档具体是怎样写入到磁盘的.\nes的分片由若干个segment组成,segment可以看做是es存储的最小管理单元,包含倒排索引和文档原始内容等.\nSegment的不变性 segment一旦写入磁盘后就不可改变.好处如下:\n 不需要锁,不用担心并发修改的情况 由于segment不变,所以相关的缓存,比如当它被读入内存后,或者filter缓存可以一直有效,不需要进行像MySQL刷脏页那种复杂操作.  坏处是一旦索引要修改,就只能整个重建,索引规模越大耗时越长.\n增量更新 为了避免对索引的全量更新,es引入按段搜索的概念,将segment分成一个个小的单元\nes的数据不直接写入磁盘,而是先写入内存的buffer中\n当buffer满了或者经过一段时间,es会将buffer提交成一个新的段,并清空buffer\n进行查询时会对分片内所有segment按顺序查询,所以为了提高查询效率,segment的数量要尽量小(segment内部的倒排索引是按字典树组织的,查询较快)\n进行删除或更新时,由于segment不可变,es会生成一个.del文件标记出被删除文档的segment.\n被标记的文档仍然能被搜索得到(逻辑删除),但是在得到最终结果之前会被过滤掉.\nFST二级索引 由于 Lucene 会为原始数据中的每个词都生成倒排索引，数据量较大。所以倒排索引对应的倒排表被存放在磁盘上。这样如果每次查询都直接读取磁盘上的倒排表，再查询目标关键词，会有很多次磁盘 IO，严重影响查询性能。\n为了解磁盘 IO 问题，Lucene 引入排索引的二级索引 FST Finite State Transducer 。原理上可以理解为前缀树，加速查询。\n其原理如下：\n 将原本的分词表，拆分成多个 Block ，每个 Block 会包含 25 ~ 48 个词（Term）。图中做了简单示意，Allen 和 After组成一个 Block 。 将每个 Block 中所有词的公共前缀抽取出来，如 Allen 和 After 的公共前缀是 A 。 将各个 Block 的公共前缀，按照类似前缀树的逻辑组合成 FST，其叶子节点携带对应 Block 的首地址 。（实际 FST 结构更为复杂，前缀后缀都有压缩，来降低内存占用量） 为了加速查询，FST 永驻堆内内存，无法被 GC 回收。 用户查询时，先通过关键词（Term）查询内存中的 FST ，找到该 Term 对应的 Block 首地址。再读磁盘上的分词表，将该 Block 加载到内存，遍历该 Block ，查找到目标 Term 对应的DocID。再按照一定的排序规则，生成DocID的优先级队列，再按该队列的顺序读取磁盘中的原始数据（行存或列存）。  Refresh与近实时性 像之前提到的,数据先是被写入内存buffer中,然后再提交为新的段,最后再通过fsync写入磁盘.\n由于fsync操作代价大,所以操作间隔一般都很长,如果等到fsync结束后才能搜索到写入的文档就太慢了.\n因此es引入一个refresh的概念,即写入和打开一个新的segment的轻量过程,refresh后新写入的数据就生效了,也就是数据从内存buffer提交到segment后(写入磁盘前)就能搜索得到.\n所以我们称es的搜索是准实时的,这个间隔就是refresh的间隔,由参数refresh_interval控制\nTranslog与持久化 为了保障性能,数据fsync到磁盘的间隔较长,如果期间服务器宕机,这段时间的数据可能丢失.\n于是es引入了Translog机制(类似MySQL的redolog),把每次对索引的操作都记录到translog里面,即便宕机,也能通过translog对数据进行恢复.\n与MySQL不同的是,es是先写Lucene,再写translog,原因是写入Lucene失败概率会高一些,如果先写translog,当写入Lucene失败后得回滚translog.\n当refresh完成后,内存buffer会清空,但是translog不会,translog只在flush后清空.\ntranslog也同样是先写内存,再fsync到磁盘,写入磁盘之前仍然可能丢数据.如果对性能要求高,fsync的间隔就拉长些,如果对安全性要求高,fsync可以设置为同步操作.通过\u0026quot;index.translog.durability\u0026quot;和\u0026quot;index.translog.sync_interval\u0026quot;两个参数控制.\n可能有人会疑惑,写Lucene要fsync,写translog也要fsync,那么translog为什么可以提高性能呢?因为lucene(segment)的数据要大一些,而translog是增量写,fsync的时间会短些.\n段合并 每次refresh都会产生一个新的segment,这会导致segment的数量暴增,因此es开了个后台线程去进行段合并.\n 段合并自动进行,优先选择一部分大小相似的段来合并成更大的新的段.旧的段此时没有删除,不影响其它正常操作. 当合并后新的段提交后,旧的段会被删除(物理删除),合并结束.  Lucene文件 ","permalink":"http://euthpic.github.io/tech/elasticsearch%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86/","summary":"一次写入过程 一次读取过程 (主要是选择协调节点,不用和倒排索引,排序联系) 一次搜索过程 (主要是排序) 不变性 动态更新索引(与MySQL的插入数据","title":"elasticsearch读写原理"},{"content":" 堆内内存  索引缓冲index buffer 节点查询缓存Node Query Cache 分片请求缓存Shard Request Cache 字段缓存Fielddata Cache FST缓存 Segments Cache   堆外内存 Segments Memory 断路器和驱逐线  Elasticsearch内存分为on heap以及off heap两部分.Elasticsearch能控制的是On Heap内存部分,这部分由JVM管理;Off Heap由Lucene管理,负责缓存倒排索引数据空间(Segment Memory).\n堆内内存On Heap 堆内主要由写入缓冲,节点查询缓存,分片查询缓存,字段缓存和FST缓存组成.其中只有写入缓冲能被GC,其它缓存都是常驻内存的,满了之后通过LRU来驱逐.\n堆内存最大不能超过32GB。因为在Java中，所有对象都分配在堆上并由指针引用。32位的系统，堆内存大小最大为 4 GB。对于64位系统，可通过内存指针压缩（compressed oops）技术，依旧可以使用32位的指针来指向堆对象，这样可以大大节省CPU 内存带宽，提高操作效率。但当内存大小超过32G时候，对象指针就需要变大，操作效率就大大降低。\n写入缓冲index buffer 新数据写入时不直接写入磁盘,而是先写入index buffer,等缓冲区填满或是经过一定时间再提交为段.\n缓冲区默认大小为堆的10%\nindices.memory.index_buffer_size: 10% 节点查询缓存Node Query Cache Node Query Cache属于node-level级别的缓存,能被当前节点所有分片共享,用于缓存filter的查询结果,即文档在该filter下是true还是false.\n几个参数:\n# 默认大小 indices.queries.cache.size: 10% # 用来控制索引是否启用缓存,默认是开启的,在每个索引的setting里面配置 index.queries.cache.enabled: true # 是否在所有Segment上启用缓存,默认false indices.queries.cache.all_segments: false 分片请求缓存Shard Request Cache Shard Request Cache属于shard-level级别的缓存,默认开启.在索引搜索的query阶段,每个分片独自在本地搜索相关文档,并把结果缓存在这里.\nPUT /my-index-000001 { \u0026#34;settings\u0026#34;: { \u0026#34;index.requests.cache.enable\u0026#34;: true } } 默认情况下,分片请求缓存只缓存size=0的搜索请求,它不保存hits数组,只保存hits.total,aggregations和suggestions,这可能是为了控制缓存的大小.\n即使在索引setting上设置开启了缓存,如果size不为0的话缓存也不会生效.如果要强制为这些请求打开缓存,可以在请求时带上查询参数\nGET /my-index-000001/_search?request_cache=true { \u0026#34;size\u0026#34;: 10, \u0026#34;aggs\u0026#34;: { \u0026#34;popular_colors\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;colors\u0026#34; } } } } 对于还在频繁更新的索引最好不要启用该缓存.\n字段缓存Fielddata Cache 在对Text类型字段排序或聚合时,由于倒排索引保存在磁盘,把相关字段加载到Fielddata Cache可以加速查询.\n# 大小默认无限 indices.fielddata.cache.size: unbounded Fielddata Cache大小默认无限是个很容易踩到的坑,如果任由它无限膨胀,会把内存吃满,持续触发断路器,因此建议一定要给设置它.\n断路器默认是95%触发\nindices.breaker.total.limit: 95%  因此这里建议设置60%~80%.\nFST缓存 Segments Cache Segments Cache是segments FST(Finite State Transducer)数据的缓存，为倒排索引的二级索引.\nFST 永驻堆内内存，无法被 GC 回收。该部分内存无法设置大小，长期占用 50% ~ 70% 的堆内存，只能通过delete index，close index以及force-merge index释放内存。从7.3版本开始，FST由堆外内存来存储。\nES 底层存储采用 Lucene，写入时会根据原始数据的内容，分词，然后生成倒排索引。查询时，先通过 查询倒排索引找到数据地址(DocID)，再读取原始数据（行数据、列数据）。但由于 Lucene 会为原始数据中的每个词都生成倒排索引，数据量较大。所以倒排索引对应的倒排表被存放在磁盘上。这样如果每次查询都直接读取磁盘上的倒排表，再查询目标关键词，会有很多次磁盘 IO，严重影响查询性能。为了解磁盘 IO 问题，Lucene 引入排索引的二级索引 FST。实现原理上可以理解为前缀树，加速查询。\n堆外内存Off Heap 在设置ES堆内存时候至少要预留50%物理内存，因为这部分内存主要用做ES堆外内存，堆外内存主要用来来存储Lucene的segment.\nindex buffer写满了之后就会提交为segment,再通过fsync写回硬盘.\n最上面的图里有Segment Cache，Segment Memory以及Segment,容易混淆,这里对比解释下\n    位置 释义     Segment Cache On Heap Segment的FST字典树,并非Segment本身,用于加速对Segment的查询   Segment Memory Off Heap Segment的缓存,因为Segment很大,不可能全部放在内存里,因此不常用的Segment会被淘汰,这里只保存常用的和新提交的   Segment Disik 原始的Lucene数据,包括倒排索引,docValue等    断路器Circuit Breaker Elasticsearch 在 total，fielddata，request 三个层面上都设计有 circuit breaker 以保护进程不至于发生 OOM 事件。\n# 针对整个堆内存 indices.breaker.total.limit: 95% # 针对单次查询需要新增的fielddata,假如fielddata上限是60%,假设当前fielddata占用是0,当前查询需要新增50%,虽然总大小没超过上限,但是单次新增超了,查询会被拒绝 indices.breaker.fielddata.limit: 40% # 针对单次查询需要使用的总cache,包括fielddata,filter cache等,需要大于上面 indices.breaker.request.limit: 60% 上图中的断路器对应indices.breaker.total.limit,驱逐线对应indices.fielddata.cache.size.\n二者间的空间为可容纳的查询大小,如果需要进行的查询需使用较大内存,就要把这个空间调大\n","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","summary":"堆内内存 索引缓冲index buffer 节点查询缓存Node Query Cache 分片请求缓存Shard Request Cache 字段缓存Fielddata Cache FST缓存 Segments Cache 堆外内存 Segments Memory 断路器和","title":"elasticsearch内存模型"},{"content":"倒排索引 通常提到搜索我们想到的实现都是es,原因是它比MySQL高效得多.\n在MySQL中想检索包含特定关键词的数据,比如最近疫情爆发了,有关部门想找出家在某某街道的同学们,就可以写这样一条SQL:\nselect * from school where address like \u0026#39;%某某街道%\u0026#39; 很显然,像B+树这样的结构需要全表扫描,无法应付数据量较大的场景.\n而es通过倒排索引这种结构高效地解决了这类问题.\n倒排索引原理就像它的名字一样,即把正常索引的映射关系反转\n例如下面是正常索引和它对应的倒排索引\n   id code     1 b,c,e   2 a,c,e   3 c,b,d       code id     a 2   b 1,3   c 1,2,3   d 3   e 1,2    在正常索引中,如果想找出code包含b的数据得遍历全部数据逐条检查,而在倒排索引中只需找到b所在行即可.\n仔细观察,在上面的例子中不仅使用了倒排索引,还把字符串\u0026quot;b,c,e\u0026quot;切分成了三个单独的字母,因此才能建立三条独立的映射.\n可以说,分词是搜索技术的核心,是最关键的实现.\n分词器原理 对于初学者来讲分词并没有那么直观.\n例如某个文档的内容是Eating an apple a day keeps doctor away,然后用eat这个关键词去搜索却没能搜索出预期结果.这是典型从MySQL转过来的\u0026quot;like思维\u0026quot;.\n在es中分词是指把全文本转换成一系列单词(term)的过程,也叫做文本分析(analysis).这个过程通过分词器(analyzer)完成.\nAnalyzer组件 分词器包含下面三个组件,工作顺序自上而下\n character filters(0个或多个) : 对文本进行预处理,比如过滤html标签,标点符号等. tokenizers(恰好1个) : 将文本切分成一个个单词 token filters(0个或多个) : 将切分后的单词进行加工,例如大小写转换,同义词转换等.  这个过程可以想象成跟做菜一样,比如做红烧肉,先要对食材预处理,洗干净猪肉,拔掉上面的猪毛(character filters),然后把猪肉切成一块块(tokenizers),最后倒入锅中烹煮,上色调味(token filters)\nAnalyzer类型  Standard Analyzer - 默认分词器，按词切分，小写处理 Simple Analyzer - 按照非字母切分(符号被过滤), 小写处理 Stop Analyzer - 小写处理，停用词过滤(the,a,is) Whitespace Analyzer - 按照空格切分，不转小写 Keyword Analyzer - 不分词，直接将输入当作输出 Patter Analyzer - 正则表达式，默认\\W+(非字符分割) Language - 提供了30多种常见语言的分词器 Customer Analyzer 自定义分词器(例如ik)  下面是自定义Analyzer的例子\nPUT /test_idx { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;pinyin_full_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pinyin\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;ik_pinyin_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;pinyin_full_filter\u0026#34; ] } } } } } 在创建mapping时可以指定两个分词器\nPOST /test_idx/_mapping { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;keyword\u0026#34; } } }  analyzer: 写时分词,写入文档时对文档进行分词.文档写入后,分词结果不会随着分词器的更新而更新,只能手动重建索引 search_analyzer: 读时分词,对输入关键词进行分词.当这两个阶段分出来的词相同时视为搜索命中  中文分词 中文一般使用ik分词器.ik中有两种策略\n ik_smart : 做最粗粒度的拆分 ik_max_word : 做最细粒度的拆分  不难推导出ik_smart是ik_max_word的子集,ik_smart中不可能包含不存在于ik_max_word的单词.曾经我在腾讯云上就观察到违背这个结论的现象,提工单过去,然后发现果然是他们的BUG.\n一般search_analyzer使用ik_max_word,让文档能匹配更多的关键词,而analyzer使用ik_smart,让关键词更加精准.\n","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8/","summary":"倒排索引 通常提到搜索我们想到的实现都是es,原因是它比MySQL高效得多. 在MySQL中想检索包含特定关键词的数据,比如最近疫情爆发了,有关","title":"elasticsearch分词入门"},{"content":"基本背景 官方对es的定位是分布式搜索和分析引擎,它专为海量搜索而生.比如想往自己的网站加入一个搜索框,或者做日志的采集和检索时都可以使用es.\nMySQL自从引入ngram解析器之后也支持全文搜索,但是它在性能和功能完备性上与es都相去甚远.MySQL是单机强事务的,而es是天然分布式的;MySQL仅支持简单的分词策略,而es可以对搜索结构做复杂的自定义处理.\n一些概念  Shards: 分片，当索引上的数据量太大的时候，我们通常会将一个索引上的数据进行水平拆分，拆分出来的每个数据库叫作一个分片。 index: 索引,类似于MySQL的表,可以约束其内数据的类型和特性 settings: 定义了index的分片数,副本数等信息 mapping: 定义了index中每个字段的类型,分词器等信息,与setting一起组成index的建表语句 type: 7.0之后已经废弃的一个结构 document: 文档,类似于MySQL的行,即一条条的数据 field: 字段,类似于MySQL的列 analyzer: 分词器,决定如何对文档或输入进行拆分.常用的中文分词器是ik.  基础操作 创建索引 一般使用put方法创建索引\nPUT /product_info { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, \u0026#34;number_of_replicas\u0026#34;: 1 } } 可以在创建时带上mapping\nPUT /product_info { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 5, \u0026#34;number_of_replicas\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;products\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;productName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;annual_rate\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;keyword\u0026#34; }, \u0026#34;describe\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34; } } } } } 新建数据 POST /product_info/_bulk {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;大健康天天理财\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.2200%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;180天定期理财，最低20000起投，收益稳定，可以自助选择消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;西部通宝\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.1100%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;90天定投产品，最低10000起投，每天收益到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;安详畜牧产业\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.3500%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;270天定投产品，最低40000起投，每天收益立即到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;5G设备采购月月盈\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;3.1200%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;90天定投产品，最低12000起投，每天收益到账消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;新能源动力理财\u0026#34;,\u0026#34;annual rate\u0026#34;:\u0026#34;3.0100%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;30天定投产品推荐，最低8000起投，每天收益会消息推送\u0026#34;} {\u0026#34;index\u0026#34;:{}} {\u0026#34;productName\u0026#34;:\u0026#34;微贷赚\u0026#34;,\u0026#34;annual_rate\u0026#34;:\u0026#34;2.7500%\u0026#34;,\u0026#34;describe\u0026#34;:\u0026#34;热门短期产品，3天短期，无须任何手续费用，最低500起投，通过短信提示获取收益消息\u0026#34;} 查询数据 基本查询\nGET /product_info/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;describe\u0026#34;: \u0026#34;每天收益到账消息推送\u0026#34; } } } //在describe字段上查找包含该关键词的文档 范围查询\nGET /product_info/_search { \u0026#34;query\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;annual_rate\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;3.0000%\u0026#34;, \u0026#34;lte\u0026#34;: \u0026#34;3.1300%\u0026#34; } } } } //查找年化率在3%到3.13%之间的产品 其他复杂的查询语句:\nhttps://n3xtchen.github.io/n3xtchen/elasticsearch/2017/07/05/elasticsearch-23-useful-query-example\nhttps://www.imooc.com/article/71116\n删除数据 DELETE /product_info/doc/3 数据类型  字符串类型: keyword(不分词),text(分词) 整数类型: long,integer,short,byte 浮点型: float,half_float,scaled_float 时间类型: date 布尔类型: boolean 二进制类型: binary 区间类型: integer_range,long_range,float_range,double_range,date_range 数组: array 对象: object 嵌套类型: nested 地理位置: geo-point,point,geo-shape ip类型: ip 范围类型: completion 令牌计数类型: token_count 附件类型: attachment 抽取类型: percolator  ","permalink":"http://euthpic.github.io/tech/elasticsearch%E5%9F%BA%E7%A1%80%E7%AF%87/","summary":"基本背景 官方对es的定位是分布式搜索和分析引擎,它专为海量搜索而生.比如想往自己的网站加入一个搜索框,或者做日志的采集和检索时都可以使用es","title":"elasticsearch基础篇"},{"content":"为什么是hugo? 其实无所谓hugo还是hexo,我是为了paperMod这个皮肤才搭的这个博客.它排版简洁素雅,让注意力容易集中到内容本身,而且我特别喜欢点击文章的这个特效,有种按钮按下去的感觉.\n这些天在\u0026quot;十年之约\u0026quot;和\u0026quot;开往\u0026quot;中逛了很多博客,也算增长了一些见力.很多博客整得花里胡哨的,这种特效那种特效一个劲儿的往上面加,动画加载时间拉得那么长,完全不顾游客体验.就像刚学会化妆的小姑娘,妆容夸张而突兀,如果玩了一段时间博客后还是这种调调,也实在太没品味了.你品品,挂个天气在博客上面是想干嘛呢?\n回归正题,本文整理了搭建hugo,使用paperMod皮肤并发布到github page的简单过程.你看官网也是可以的,不过有些小坑官网的新手教程没提.\nhugo中文文档\npaperMod\n安装Hugo 二进制安装（推荐：简单、快速）  到 Hugo Releases 下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe） 把hugo.exe加到环境变量里  生成站点 $ hugo new site /my-blog $ cd /my-blog 站点目录结构:\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml content是文章存放的目录,这里有个小坑,文档里面示范用的hyde皮肤能扫描content目录下的全部md文档,但是paperMod不行,paperMod只能读取/content/posts/下面的md文档,然后我搜了一下,还有些皮肤只支持/content/post,这里不同皮肤的规则比较混乱,使用时要注意文档位置是否正确.\nconfig.toml是配置文件,hugo官方用的是toml格式,有点冷门,然后paperMod又推荐回yml配置,得把toml改成yml.\n这里还需要自己创建个theme目录放皮肤.\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ ▸ theme/ config.toml 创建文章 创建第一篇文章放到post目录.\n$ hugo new posts/first.md 通过hugo创建的文章会有个默认头\ndate = \u0026quot;2015-10-25T08:36:54-07:00\u0026quot; title = \u0026quot;about\u0026quot; draft = true draft = true 表示草稿,得把这里改成false或者删掉后发布github才可见,或者想本地预览的话hugo server得加个\u0026ndash;buildDrafts参数\n安装皮肤 挑选一个皮肤然后clone下来.我有选择困难症,所以一开始是跳过下载皮肤直接运行hugo的,结果发现文章显示不了,一定得先下载个皮肤\n$ cd /theme $ git clone https://github.com/adityatelange/hugo-PaperMod.git 运行Hugo 在站点根目录启动本地预览\n$ hugo server --theme=hugo-PaperMod --buildDrafts 在config.yml里面指定皮肤后这里就不需要了.\n浏览器里打开： http://localhost:1313\n部署到Github 假设你需要部署在 GitHub Pages 上，首先在GitHub上创建一个Repository，命名为：xxxx.github.io （xxxx替换为你的github用户名）。\n在站点根目录执行 Hugo 命令生成最终页面：\n$ hugo --theme=hugo-PaperMod --baseUrl=\u0026#34;http://xxxx.github.io/\u0026#34; （注意，以上命令并不会生成草稿页面，如果未生成任何文章，请去掉文章头部的 draft=true 再重新生成。）\n如果一切顺利，所有静态页面都会生成到 public 目录，将pubilc目录里所有文件 push 到刚创建的Repository的 master 分支。\n$ cd public $ git init $ git remote add origin https://github.com/xxxx/xxxx.github.io.git $ git add -A $ git commit -m \u0026#34;first commit\u0026#34; $ git push -u origin master 浏览器里访问：http://xxxx.github.io/\n然后创建另一个github项目保存我们的源码,不要忘了在.gitignore里面把public目录加上.\n如果你需要在另一个设备上下载源码然后运行它,也不要忘了重新在theme目录里面重新clone皮肤\n","permalink":"http://euthpic.github.io/tech/%E7%94%A8hugo%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/","summary":"为什么是hugo? 其实无所谓hugo还是hexo,我是为了paperMod这个皮肤才搭的这个博客.它排版简洁素雅,让注意力容易集中到内容本身","title":"用hugo搭建自己的博客"},{"content":"","permalink":"http://euthpic.github.io/posts/%E8%BF%99%E9%87%8C%E5%95%A5%E9%83%BD%E6%B2%A1%E6%9C%89/","summary":"","title":"这里啥都没有"},{"content":"Znode 结构:\n path:唯一路径 childNode：子节点 stat:状态属性 type:节点类型  节点类型:\n   类型 描述     PERSISTENT 持久节点(默认节点)   PERSISTENT_SEQUENTIAL 持久序号节点   EPHEMERAL 临时节点(不可再拥有子节点)   EPHEMERAL_SEQUENTIAL 临时序号节点(不可再拥有子节点)    不过看了源码后发现不止这些.\nZK集群 集群角色:\n leader : 主节点,写节点.选举产生 follower : 从节点,读节点. observer : 读节点,没有投票权,用于缓解集群的读压力,但同时又不增加选举的复杂度(像不像外包?)  集群选举:\n  触发条件 :\n 服务节点初始化启动 半数以上节点无法和leader建立连接    投票规则 : 第一轮投票全部投给自己 第二轮投票给myid比自己大的相邻节点\n  选举完成条件 : 当某节点得票超过半数,则其当选leader,选举结束\n  Watcher 事件监听器\n ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性  崩溃恢复\n 当leader出现异常或者网络异常,导致过半的节点无法与leader取得联系,那么集群会进入崩溃恢复阶段,重新选举出新的leader 选出新leader后,新leader要完成整个集群的同步工作,也就是保证过半的节点能够和leader保持数据状态一致  消息广播\n 当集群数据同步完成后,才进入消息广播阶段,才能对外开放服务  zk写数据过程\n 用户找到其中一个follower/observer,它会将写请求转发给leader leader会为这个请求生成一个全局唯一的zxid(事务编号),然后向全体follower发起投票来决定接不接受这个写请求 如果follower收到的写请求的zxid大于本地记录的最大zxid,那么就会向leader回复一个ack 如果同意过半,那么leader会向全体follower发起无条件写请求,follower执行之后会更新本地记录中的zxid 考虑一种情况,如果leader向follower发送一个请求A,但是它丢失了,之后leader又发送了一个请求B,此时follower的ack实际是回复请求A的,但leader可能会误会成同意执行请求A.为了解决这种问题,zk的做法是为每个follower维护一个滑动窗口,全为唯一的zxid作为窗口编号,只接收按序到达的ack.  分布式锁\n 首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。 zk 中不需要向 redis 那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了  ","permalink":"http://euthpic.github.io/tech/zookeeper%E7%AC%94%E8%AE%B0/","summary":"Znode 结构: path:唯一路径 childNode：子节点 stat:状态属性 type:节点类型 节点类型: 类型 描述 PERSISTENT 持久节点(默认节点) PERSISTENT_SEQUENTIAL 持久序号节","title":"Zookeeper笔记"},{"content":"需求背景 我们的集群迁移到云上的k8s后,本地无法继续直连调试,需要新的远程调试方案.\n同事先调研了阿里出品的kt-connect,给出的结论是由于协议转换的问题,在使用redis,kafka等中间件时会受影响,于是这个方案被否定了.然后我抽空试了下telepresence(和ambassador同源),基本需求都能满足,所以就把这个推给其他人了.\n安装traffic-manager 这类工具都需要在集群和本地创建代理,集群上的代理客户端叫traffic-manager,本地的叫telepresence.\n需要确保telepresence和traffic-manager版本一致.我们使用的版本是2.4.6\nhelm repo add datawire https://app.getambassador.io helm repo update kubectl create namespace ambassador helm install traffic-manager --namespace ambassador datawire/telepresence 安装telepresence macOS # Install via brew: brew install datawire/blackbird/telepresence # OR install manually: # 1. Download the latest binary (~60 MB): sudo curl -fL https://app.getambassador.io/download/tel2/darwin/amd64/2.4.6/telepresence -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence Linux # 1. Download the latest binary (~50 MB): sudo curl -fL https://app.getambassador.io/download/tel2/linux/amd64/2.4.6/telepresence -o /usr/local/bin/telepresence # 2. Make the binary executable: sudo chmod a+x /usr/local/bin/telepresence Windows 接触的时候他们刚推出windows的开发者预览版本,在安装脚本和使用过程中可能都会有一些小问题.\n  下载官方安装包: https://app.getambassador.io/download/tel2/windows/amd64/2.4.6/telepresence.zip\n  安装命令请在Powershell内以管理员身份执行(需要把安装包放到盘根目录再解压,否则可能失败)\nExpand-Archive -Path telepresence.zip Remove-Item \u0026#39;telepresence.zip\u0026#39; cd telepresence   默认安装到C:\\telepresence,可以通过编辑install-telepresence.ps1来修改安装路径\nSet-ExecutionPolicy Bypass -Scope Process .\\install-telepresence.ps1   安装完成后系统环境变量中会多出两行\nC:\\telepresence C:\\Program Files\\SSHFS-Win\\bin   确认是否安装成功\nPS C:\\Users\\iplas\u0026gt; telepresence.exe status Root Daemon: Not running User Daemon: Not running   安装kubectl(新版本的docker desktop v20.10.7自动安装了kubectl) https://kubernetes.io/zh/docs/tasks/tools/install-kubectl-windows/\n  验证\n$ kubectl.exe version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.2\u0026#34;, GitCommit:\u0026#34;092fbfbf53427de67cac1e9fa54aaa09a28371d7\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-06-16T12:59:11Z\u0026#34;, GoVersion:\u0026#34;go1.16.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;windows/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;18+\u0026#34;, GitVersion:\u0026#34;v1.18.4-tke.6\u0026#34;, GitCommit:\u0026#34;194201819cf1e5cf45d38f72ce1aac9efca4c7ff\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-12-29T09:13:24Z\u0026#34;, GoVersion:\u0026#34;go1.15.6\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;}   如果没有server的信息是因为kubeconfig没有配置,配置文件可以在Rancher获取并保存本地\napiVersion: v1 kind: Config clusters: - name: \u0026#34;testk8s\u0026#34; cluster: server: \u0026#34;https://rancher.xxx.com.cn/k8s/clusters/local\u0026#34; users: - name: \u0026#34;testk8s\u0026#34; user: token: \u0026#34;kubeconfig-u-sr6p9:xxxx\u0026#34; contexts: - name: \u0026#34;testk8s\u0026#34; context: user: \u0026#34;testk8s\u0026#34; cluster: \u0026#34;testk8s\u0026#34; current-context: \u0026#34;testk8s\u0026#34;   添加系统变量,变量名=KUBECONFIG 变量值=${kubeconfig.yml保存位置}\n  验证集群连通性\n$ kubectl cluster-info Kubernetes control plane is running at https://rancher.xxx.com.cn/k8s/clusters/local CoreDNS is running at https://rancher.xxx.com.cn/k8s/clusters/local/api/v1/namespaces/kube-system/services/kube-dns:dns-tcp/proxy KubeDNSUpstream is running at https://rancher.xxx.com.cn/k8s/clusters/local/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   升级   退出telepresence进程\n$ telepresence.exe quit Telepresence Root Daemon quitting... done Telepresence User Daemon quitting... done 2021-11-10 14:33:36 iplas@q /e/cmd $ telepresence.exe status Root Daemon: Not running User Daemon: Not running   重新安装新的版本(不用卸载)\n  使用 全量拦截   连接traffic-manager\n$ telepresence.exe connect Launching Telepresence Root Daemon Launching Telepresence User Daemon Connected to context testk8s (https://rancher.xxx.com.cn/k8s/clusters/local)   拦截指定deploy的全部流量,注意这里的service-name指deploy的name,remote-port指svc的port.\ntelepresence intercept \u0026lt;service-name\u0026gt; --port \u0026lt;local-port\u0026gt;[:\u0026lt;remote-port\u0026gt;] --env-file \u0026lt;path-to-env-file\u0026gt;   以test1-oss的mobile-oss为例(如果要拦截的svc只有一个端口,可以不指定远程端口)\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080 Using Deployment mobile-oss-deploy intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   检查拦截情况\n$ telepresence.exe list -n test1-oss activiti-service-deployment : ready to intercept (traffic-agent not yet installed) admin-service-deployment : ready to intercept (traffic-agent not yet installed) attachment-service-deployment : ready to intercept (traffic-agent not yet installed) crm-service-deployment : ready to intercept (traffic-agent not yet installed) crmdb-services-deployment : ready to intercept (traffic-agent not yet installed) mobile-oss-deploy : intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   此时访问该svc会发现流量流向本地了\n$ curl https://xxx-test1.xxx.com.cn/ /index   解除拦截\n$ telepresence.exe leave mobile-oss-deploy-test1-oss   再次检查拦截情况\n$ telepresence.exe list -n test1-oss activiti-service-deployment : ready to intercept (traffic-agent not yet installed) admin-service-deployment : ready to intercept (traffic-agent not yet installed) attachment-service-deployment : ready to intercept (traffic-agent not yet installed) crm-service-deployment : ready to intercept (traffic-agent not yet installed) crmdb-services-deployment : ready to intercept (traffic-agent not yet installed) mobile-oss-deploy : ready to intercept (traffic-agent already installed)   可以看到mobile-oss-deploy的状态从intercepted变为ready to intercept,括号内容从traffic-agent not yet installed变为traffic-agent already installed,这是因为每个被代理的deploy都会安装一个名为traffic-agent的sidecar,占用内存约50m.这个sidecar不会因解除拦截而销毁,只能手动销毁\n##销毁指定deploy $ telepresence.exe -n test1-oss uninstall -d mobile-oss-deploy ##销毁全部 $ telepresence uninstall --everything   局部拦截   telepresence还提供了名为preview url的拦截模式,该模式不影响集群原有流量,仅把通过该url访问的流量导向本地.\n  先要登录ambassador(只有登录状态下才能用preview url)\n$ telepresence.exe login Launching browser authentication flow... Login successful.   确认本地服务已启动的情况下,再次创建拦截\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080:80   然后访问https://reverent-dhawan-659.preview.edgestack.me/验证拦截情况(访问需要登录状态,如果是浏览器之外的访问方式记得带上cookie)\n  使用deploy的环境变量启动本地服务   维护本地环境变量是个繁琐的事情,IntelliJ可以通过EnvFile插件来使用集群的环境变量\n  创建拦截时通过-e参数把集群变量写入到本地文件\n$ telepresence.exe intercept -n test1-oss mobile-oss-deploy --port 8080 -e /e/envfile/mobile-oss.env Using Deployment mobile-oss-deploy intercepted Intercept name : mobile-oss-deploy-test1-oss State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:8080 Volume Mount Point: T: Intercepting : all TCP connections   spring boot/tomcat启动: 安装好EnvFile后,运行配置里会多出一列\u0026quot;EnvFile\u0026quot;,选择上一步保存的配置文件\n  maven启动: 用上一步的配置文件覆盖本地项目配置文件\n  使用场景  本地接口自测: 不需要创建拦截,直接用api doc来测试 调试服务调用的接口(东西流量): 只能全量拦截 前后端联调,接口的入口可以在前端修改(南北流量):可以全量拦截,也可以用preview url  FAQs telepresence连接失败怎么排查  检查telepresence的版本和traffic-manager的版本是否一致,目前用的2.4.0 检查是否开了socks代理,把系统变量里的http_proxy和https_proxy删掉 重启大法,试下依次重启telepresence进程,要拦截的deploy,本地主机  创建拦截失败怎么排查  如果svc暴露了多个port,需要显示指定远程的port(默认是80) 检查svc的port是否和deploy的port对应 traffic-agent只能绑定到一个port,如果之前创建的拦截是绑定到A端口,现在要改成B端口,要先卸载原先的traffic-agent,再创建拦截 创建preview url前本地服务需要先启动,并确保服务端口已经开放.创建了preview url后,可以访问 https://app.getambassador.io/cloud/services 查看拦截情况. 如果报错中有conflict关键字,可能是别人先抢占了环境,得等别人leave之后才能连(好像自己无法主动断开其他人的连接,所以要养成每次用完主动leave的习惯)  一些补充  本地的启动配置替换成k8s,不过由于本地没有namespace.所以svc要补全namespace 对于服务注册而言,像xxl-job和nacos这类组件可以手动配置注册ip,使用是正常的,但是如果遇到像我们自研框架的zookeeper这样会把本地ip注册上去(云上无法识别内网ip)的情况,就需要改造成注册ip可配置了. 如果已经telepresence login,那么可以创建preview url(只会拦截通过该url访问的流量,不影响集群),否则拦截全部流量. preview url依赖ambassador cloud,需要登录状态(小程序可能得考虑封装个全局的cookie) 创建preview url前本地服务需要先开启,并确保服务端口已经开放(可能ambassador cloud要检测什么) preview url原理: 生成一个请求头带标记的request,然后telepresence将请求转发到ambassador cloud(因为这个url由ambassador创建并公开),然后ambassador再转发回集群,集群内的traffic agent查看头部并拦截请求,将它转发回本地机器 曾被拦截过的deploy里面都会创建一个traffic-agent(一个agent消耗50m内存,如果每个环境的每个deploy都创建了一个agent,需要考虑内存占用的问题) 如果deploy对应的svc暴露了多个端口,需要在冒号后面指定拦截哪个端口(svc的port)  ","permalink":"http://euthpic.github.io/tech/telepresence/","summary":"需求背景 我们的集群迁移到云上的k8s后,本地无法继续直连调试,需要新的远程调试方案. 同事先调研了阿里出品的kt-connect,给出的结论是","title":"Telepresence"},{"content":" 容器本身没有价值,有价值的是容器编排技术 容器(docker)其实是一种沙盒技术,一是可以将应用之间隔离开来,二是方便地将应用\u0026quot;搬来搬去\u0026quot;(快速装载,快速卸载) 容器其实是一种特殊的进程,在容器外面观察它时(ps),它是普通的进程,在容器里面观察它时,外部的细节被屏蔽掉(namespace),使得它以为自己是独立存在的.容器本质上就是一个加了限定参数(namespace)的进程 docker是没有上过历史课的进程 容器较轻量,虚拟机较重.容器底层依赖的仍然是宿主机的硬件/驱动,而虚拟机自己模拟了这些硬件/驱动.容器实现的仅是视图隔离 cgroups 是Linux内核提供的一种可以限制单个进程或者多个进程所使用资源的机制，可以对 cpu，内存等资源实现精细化的控制，目前越来越火的轻量级容器 Docker 就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制 虽然docker通过cgroups实现了容器的视图隔离,但是在容器内使用/proc下的命令,例如top/free等,看到的仍然是宿主机的信息,这是因为容器没有做到对/proc,/sys等文件系统的资源的视图隔离.可以通过lxcfs来解决这个问题 cgroup只能对容器使用的资源上限做限定,但不能锁定下限,这很容易导致被其他容器抢占资源.k8s完善了这点 rootfs : 根文件系统,挂载在容器的根目录上,用来为容器进程提供隔离后执行环境的文件系统.也就是所谓的\u0026quot;容器镜像\u0026quot; .包括的目录和文件有/bin , /etc , /proc等等.进入容器后执行的/bin/bash,就是这个目录下的/bin目录下的文件,与宿主机的/bin/bash不同 docker容器使用了多个增量rootfs联合挂载一个完整rootfs的方案,也就是容器镜像中\u0026quot;层\u0026quot;的概念,包括可读写层,init层,只读层.只读层是共享层,我们使用docker commit提交本地修改后的docker镜像,实际上提交的是可读写层的内容,我们的修改都保存在读写层(分层复用). 上面的读写层通常也称为容器层，下面的只读层称为镜像层，所有的增删查改操作都只会作用在容器层，相同的文件上层会覆盖掉下层。知道这一点，就不难理解镜像文件的修改，比如修改一个文件的时候，首先会从上到下查找有没有这个文件，找到，就复制到容器层中，修改，修改的结果就会作用到下层的文件，这种方式也被称为copy-on-write。 一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理 快速部署k8s的工具是kubeadm,它直接运行在宿主机上,然后k8s的其他组件以容器的方式被kubeadm调用(封装成pod) pod的定义文件时yaml.master等组件的yaml文件在/etc/kubernetes.manifests路径下 快速部署k8s的工具:kops,ranche,minikube,katacoda提供的在线学习平台 创建pod的命令: kubectl apply -f [url] ,可以使用的文件包括.yaml/.yml/.json rook:一个基于 Ceph 的 Kubernetes 存储插件 基于 Kubernetes 开展工作时，你一定要优先考虑这两个问题：我的工作是不是可以容器化？我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成？ 我们在yaml文件中定义API对象(pod),每一个API对象都有一个叫做Metadata的字段,即元数据,它是API对象的\u0026quot;标识\u0026quot;,也是我们从k8s里找到这个对象的主要依据.这其中主要用到的字段是Labels,它是一组k-v格式的标签 kubectl describe用于查看一个API对象的细节.在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中.所以，这个部分正是我们将来进行 Debug 的重要依据。如果有异常发生，你一定要第一时间查看这些 Events，往往可以看到非常详细的错误信息。 k8s鼓励开发者使用kubectl apply YAML文件这样的声明式API去替代docker run这种基于命令行的操作,因为这样可以以文件的形式记录下对k8s的操作,这样运维人员和开发人员可以通过yaml来进行交流沟通. 开发k8s应用,需要自己制作docker镜像? 容器是单进程模型(并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力) pod中可以有多个容器 Pod 的实现需要使用一个中间容器Infra.在这个pod中,infra容器永远是第一个被创建的容器,而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。 如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了  ","permalink":"http://euthpic.github.io/tech/k8s%E7%AC%94%E8%AE%B0/","summary":"容器本身没有价值,有价值的是容器编排技术 容器(docker)其实是一种沙盒技术,一是可以将应用之间隔离开来,二是方便地将应用\u0026quot;搬来","title":"K8s笔记"},{"content":"Naocs NacosServiceRegistryAutoConfiguration : 服务注册入口类 NacosAutoServiceRegistration : 自动注册服务流程 NacosServiceRegistry : 服务注册实现类 NacosRegistration : 服务注册实体,包含注册服务所需要的信息 NacosNamingMaintainService : 服务治理类(service) NacosNamingService : 服务实例(instance) NamingProxy : 网络代理类 NacosRegistrationCustomizer : 2020.9新版本中加入,用于向NacosRegistration中注入配置 CacheData : 维护配置项和其下注册的所有监听器的实例 BeatInfo : 心跳包信息 NacosDiscoveryAutoConfiguration: 服务发现入口 NacosDiscoveryProperties : 服务注册,发现相关的配置类 NacosServiceDiscovery : 服务发现的实现过程,为NacosDiscovery模块提供nacos上的service和instance信息. NacosDiscoveryClientConfiguration : NacosServiceInstance : 服务实例(instance)实体信息,只在服务发现模块使用 NacosDiscoveryClient : 服务信息提供类,提供getInstances和getServices方法 EventDispatcher : 当instances信息更新时,给监听者们发布事件消息. NacosWatch : 监听EventDispatcher发布的消息,并同步到NacosDiscoveryProperties的元数据中 配置管理 服务注册  服务注册的入口类是NacosServiceRegistryAutoConfiguration,这里定义了三个重要的bean :NacosServiceRegistry,NacosRegistration,NacosAutoServiceRegistration 如果项目集成了spring-cloud-starter-alibaba-nacos-discovery，服务启动后默认是自动注册的。如果想看自动注册的过程，可以从NacosAutoServiceRegistration开始着手 NacosServiceRegistry : 提供了register()方法给NacosAutoServiceRegistration进行服务注册.依赖NacosNamingService NacosRegistration : 服务注册实体,包含注册服务所需要的信息.提供了customize()方法给用户注入配置. NacosRegistrationCustomizer : 方便用户自定义 NacosRegistrationCustomizer 实现，向 NacosRegistration 中注入配置 NacosDiscoveryProperties : 服务注册相关的配置类,读取application.yml里以spring.cloud.nacos.discovery为前缀的配置项 NacosNamingMaintainService : 服务治理类(service),包含在nacos上创建服务,查询服务,删除服务等功能.通过调用关系可以发现该类是在NacosServiceRegistry的setStatus()方法(控制instance上线或下线)中被创建的,也就是nacos的服务是懒加载的,当需要对实例进行上线下线时,才会去获取对应的service,如果service不存在才会创建.而setStauts()是在ServiceRegistryEndpoint中被调用(spring cloud的类) NacosNamingService : 服务实例(instance)管理,包括把实例注册到服务上,订阅/取消订阅服务等功能. NamingProxy : 发起http请求的方法,基本上对于nacos服务器的所有网络请求都需要调用这里的callServer()  服务注册主流程 :\n/** *NacosAutoServiceRegistration * Register the local service with the {@link ServiceRegistry}. */ protected void register() { this.serviceRegistry.register(getRegistration()); } /** * NacosServiceRegistry */ public void register(Registration registration) { namingService.registerInstance(serviceId, group, instance); } /** * NacosNamingService */ public void registerInstance(String serviceName, String groupName, Instance instance) throws NacosException { serverProxy.registerService(groupedServiceName, groupName, instance); } NacosAutoServiceRegistration中有两个事件监听器.\n一个是onNacosDiscoveryInfoChangedEvent().当监听到NacosDiscoveryProperties的配置信息改变时就会重新注册.\n@EventListener public void onNacosDiscoveryInfoChangedEvent(NacosDiscoveryInfoChangedEvent event) { restart(); } private void restart() { this.stop(); this.start(); } 另一个是在父类AbstractAutoServiceRegistration中定义的事件监听器,这是因为实现了ApplicationListener接口(需要先了解spring boot的事件监听机制).直接看onApplicationEvent()即可.\n@Override public void onApplicationEvent(WebServerInitializedEvent event) { bind(event); } @Deprecated public void bind(WebServerInitializedEvent event) { ApplicationContext context = event.getApplicationContext(); if (context instanceof ConfigurableWebServerApplicationContext) { if (\u0026#34;management\u0026#34;.equals(((ConfigurableWebServerApplicationContext) context) .getServerNamespace())) { return; } } this.port.compareAndSet(0, event.getWebServer().getPort()); this.start(); } public void start() { if (!isEnabled()) { if (logger.isDebugEnabled()) { logger.debug(\u0026#34;Discovery Lifecycle disabled. Not starting\u0026#34;); } return; } // only initialize if nonSecurePort is greater than 0 and it isn\u0026#39;t already running \t// because of containerPortInitializer below \tif (!this.running.get()) { this.context.publishEvent( new InstancePreRegisteredEvent(this, getRegistration())); register(); if (shouldRegisterManagement()) { registerManagement(); } this.context.publishEvent( new InstanceRegisteredEvent\u0026lt;\u0026gt;(this, getConfiguration())); this.running.compareAndSet(false, true); } } 可以看到,服务注册是在WebServer启动完毕之后调用的.\n此外,spring boot和nacos大量使用了**事件发布(ApplicationContext.publishEvent()) -\u0026gt; 事件监听(@EventListener)  ) **的模式来控制各流程的执行顺序,这个特性需要好好掌握.\nSpring 在实现的时候,通过设计模式(装饰),实现流程,又把关键部分丢给开发自己实现,提高了拓展性,然后巧妙的结合了观察者模式(变种),在合适的时间注册服务.\n服务发现  NacosDiscoveryAutoConfiguration : 服务发现的入口,包含NacosDiscoveryProperties和NacosServiceDiscovery两个bean.该类上有两个注解 : @ConditionalOnDiscoveryEnabled和@ConditionalOnNacosDiscoveryEnabled(都引入了@ConditionalOnProperty),也就是当配置项\u0026quot;spring.cloud.discovery.enabled\u0026quot;和\u0026quot;spring.cloud.nacos.discovery.enabled\u0026quot;都为true时才进行自动装配.由于这里设置了matchIfMissing = true,缺省值是true,所以默认是自动装配,不需要填写相关配置.也就是引入了nacos的相关包之后服务发现就自动开启了,不需要在Application类加上@EnableDiscoveryClient注解. @EnableDiscoveryClient原先是用于自动启用服务注册功能,以便其他服务发现本服务.在Spring的官方文档中已经声明**@EnableDiscoveryClient不是必须的了(从Spring Cloud Edgware开始),所有DiscoveryClient类型的bean都会自动注册**. NacosDiscoveryProperties : 是nacos的配置类,虽然从名字上看只属于服务发现这一模块,但实际上服务发现的配置也是由该类提供.负责读取并提供\u0026quot;spring.cloud.nacos.discovery\u0026quot;前缀的配置.overrideFromEnv()方法负责注入配置属性.此外,enrichNacosDiscoveryProperties()允许用户在配置文件里自定义额外的nacos配置. @PostConstruct  : 上面这个类的init()方法里面用到,作用是当该类的依赖注入完成后,且该类被投入使用前,执行包含该注解的方法.作用类似于构造方法/构造代码块,目的是完成该类的初始化工作,之后才能服务于其他类. NacosServiceInstance : 服务实例(instance)实体信息,只在服务发现模块使用. NacosServiceDiscovery : 服务发现实现类,包含服务发现的过程,为NacosDiscovery模块提供nacos上的service和instance信息,并实现instance向serviceInstance的转换.依赖NacosServiceManager提供的namingService.这里需要注意的是虽然可以直接从NacosServiceManager得到服务信息,但是用户直接依赖的是NacosServiceDiscovery而不是NacosServiceManager,这样的设计的依据可能是单一职责原则,NacosServiceManager负责管理service,而对外提供服务的职责由NacosServiceDiscovery承担. NacosDiscoveryClientConfiguration : 加载NacosServiceDiscovery和NacosWatch两个bean. NacosDiscoveryClient : 服务信息提供类,仅对外提供NacosServiceDiscovery的getInstances和getServices方法(职责划分得很细嘛).也就是说nacos组件外想获取服务和实例信息的话都应该通过这个类获取. NacosWatch: 负责监听EventDispatcher中发布的instances更新事件,并同步到NacosDiscoveryProperties的元数据中.   Feign FeignClientsRegistrar : 注册实现类 @EnableFeignClients @EnableDiscoveryClient @FeignClient ","permalink":"http://euthpic.github.io/tech/nacos%E6%BA%90%E7%A0%81%E7%9A%84%E5%B0%8F%E5%B0%8F%E7%A0%94%E7%A9%B6/","summary":"Naocs NacosServiceRegistryAutoConfiguration : 服务注册入口类 NacosAutoServiceRegistration : 自动注册服务流程 NacosServiceRegistry : 服务注册实现类 NacosRegistration : 服务注册实体,包含注册服务所需要的信息 NacosNamingMaintainService : 服务治理类(service) NacosNamingService : 服务","title":"Nacos源码的小小研究"},{"content":"调研目标  项目结构,项目配置,启动脚本的变化 dapeng-sc的主要修改点 nacos(feign)如何注册和发现服务  项目结构,项目配置,启动脚本的变化 项目结构   dapeng-demo就是我们的dapeng项目,包括api和service,它们可以集成到一个modules统一管理,也可以像现行这样分离开来.\n  sc-demo就是spring cloud项目.可以和dapeng service互相发现调用.\n  dapeng-api没有变化,仍然使用thrift和相应插件生成service和client.\n  dapeng-service已经是spring boot架构了,有独立的网关controller和启动基类Application(不过这个类仍然由dapeng container来调用).\n  配置文件也从services.xml变成application.yml(也可以是application.properties或者bootstrap.yml,读取文件的功能是由spring boot实现的)\n  项目配置 application.yml :\nserver: port: 9203 spring: application: name: account-service cloud: nacos: discovery: server-addr: 127.0.0.1:8848 dapeng: scan: base-packages: com.dapeng.demo.account  除了前缀为dapeng.scan的配置都是springboot和nacos的配置. 前缀为dapeng.scan的配置项是dapeng-sc新增的自定义配置项,作用是指定扫描哪些路径来获取含@DapengService的服务类.(目前也仅自定义了该项配置) 配置了dapeng.scan.base-packages之后,就不需要在Application类里面加上@DapengComponentScan注解了,两者的作用应该是相同的  maven命令行 新:\ncompile com.github.dapeng-soa:dapeng-maven-plugin:2.2.0-SNAPSHOT:run2 -Dsoa.freq.limit.enable=false -Dsoa.transactional.enable=false -Dspringboot.main.class=com.dapeng.demo.account.AccountAnnotationApplication -Dsoa.apidoc.port=9876 -Dsoa.container.port=9071 -Dspring.config.location=E:/scala-project/mydapeng-demo/dapeng-demo/account-service/src/main/resources/application.yml 旧:\ncompile com.github.dapeng:dapeng-maven-plugin:2.1.1.1:run -Dsoa.zookeeper.kafka.host=127.0.0.1:2181 -Dsoa.kafka.host=127.0.0.1:9092 -Dsoa.zookeeper.host=127.0.0.1:2181 -Dsoa.transactional.enable=false -Dsoa.container.port=11001 -Dsoa.apidoc.port=12001 -Dsoa.container.ip=localhost  -D开头的都会写入到系统配置里面去,例如-Dsoa.apidoc.port=9876,那么就有System.getProperty(\u0026ldquo;soa.apidoc.port\u0026rdquo;)=9876 所以可以看到,新增了两个系统配置springboot.main.class和spring.config.location dapeng-maven-plugin:2.2.0-SNAPSHOT:run2 这里的run2对应dapeng-maven-plugin下新增的RunContainer2Plugin类(还没看实现逻辑)  dapeng-sc的主要修改点  在Dapeng Container中,spring,zookeeper,netty等第三方组件被封装成plugin(实现了start和stop接口) spring plugin的实现类是SpringApplicationLoader,替换了原有的SpringAppLoader.  SpringAppLoader\n@Override public void start() { LOGGER.warn(\u0026#34;Plugin::\u0026#34; + getClass().getSimpleName() + \u0026#34;::start\u0026#34;); String configPath = \u0026#34;META-INF/spring/services.xml\u0026#34;; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); for (ClassLoader appClassLoader : appClassLoaders) { try { // ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new Object[]{xmlPaths.toArray(new String[0])});  // context.start(); Class\u0026lt;?\u0026gt; appCtxClass = appClassLoader.loadClass(\u0026#34;org.springframework.context.support.ClassPathXmlApplicationContext\u0026#34;);  Class\u0026lt;?\u0026gt;[] parameterTypes = new Class[]{String[].class}; Constructor\u0026lt;?\u0026gt; constructor = appCtxClass.getConstructor(parameterTypes); Thread.currentThread().setContextClassLoader(appClassLoader); Object springCtx = getSpringContext(configPath, appClassLoader, constructor); springCtxs.add(springCtx); Method method = appCtxClass.getMethod(\u0026#34;getBeansOfType\u0026#34;, Class.class); Map\u0026lt;String, SoaServiceDefinition\u0026lt;?\u0026gt;\u0026gt; processorMap = (Map\u0026lt;String, SoaServiceDefinition\u0026lt;?\u0026gt;\u0026gt;) method.invoke(springCtx, appClassLoader.loadClass(SoaServiceDefinition.class.getName())); //获取所有实现了lifecycle的bean  LifecycleProcessorFactory.getLifecycleProcessor().addLifecycles(((Map\u0026lt;String, LifeCycleAware\u0026gt;) method.invoke(springCtx, appClassLoader.loadClass(LifeCycleAware.class.getName()))).values()); //TODO: 需要构造Application对象  Map\u0026lt;String, ServiceInfo\u0026gt; appInfos = toServiceInfos(processorMap); Application application = new DapengApplication(new ArrayList\u0026lt;\u0026gt;(appInfos.values()), appClassLoader); //Start spring context  LOGGER.info(\u0026#34; start to boot app\u0026#34;); Method startMethod = appCtxClass.getMethod(\u0026#34;start\u0026#34;); startMethod.invoke(springCtx); // IApplication app = new ...  if (!application.getServiceInfos().isEmpty()) { // fixme only registerApplication  Map\u0026lt;ProcessorKey, SoaServiceDefinition\u0026lt;?\u0026gt;\u0026gt; serviceDefinitionMap = toSoaServiceDefinitionMap(appInfos, processorMap); container.registerAppProcessors(serviceDefinitionMap); container.registerAppMap(toApplicationMap(serviceDefinitionMap, application)); //fire a zk event  container.registerApplication(application); } LOGGER.info(\u0026#34; ------------ SpringClassLoader: \u0026#34; + ContainerFactory.getContainer().getApplications()); } catch (Exception e) { LOGGER.error(e.getMessage(), e); } finally { Thread.currentThread().setContextClassLoader(classLoader); } } } 关键实现点:\n 从\u0026quot;META-INF/spring/services.xml\u0026quot;中读取配置并获得上下文. 通过method.invoke()分别加载dapeng service bean和spring自己的bean. dapeng service bean的class是SoaServiceDefinition(很关键),spring bean的class是LifeCycleAware  这里的Container是dapeng容器的主要结构\n/** * 大鹏容器的主结构，负责管理容器相关的监听器，插件，应用程序。 * * 所有的组件的注册，卸载动作都应该由Container来负责， */ public interface Container { /** * 容器状态 */ int STATUS_UNKNOWN = 0; int STATUS_CREATING = 1; int STATUS_RUNNING = 2; int STATUS_SHUTTING = 3; int STATUS_DOWN = 4; /** * 注册应用程序监听器， * @param listener */ void registerAppListener(AppListener listener); /** * 卸载用用程序监听器 * @param listener */ void unregisterAppListener(AppListener listener); /** * 注册应用程序（保存容器具体的应用信息） * @param app */ void registerApplication(Application app); /** * 卸载应用程序 * @param app */ void unregisterApplication(Application app); /** * 注册插件(like: Zookeeper,netty..etc.) * @param plugin */ void registerPlugin(Plugin plugin); /** * 卸载插件 * @param plugin */ void unregisterPlugin(Plugin plugin); /** * 注册Filter(like: monitor) */ void registerFilter(Filter filter); /** * 卸载Filter * @param filter */ void unregisterFilter(Filter filter); /** * 获取应用程序的相关信息 * @return */ List\u0026lt;Application\u0026gt; getApplications(); List\u0026lt;Plugin\u0026gt; getPlugins(); Map\u0026lt;ProcessorKey, SoaServiceDefinition\u0026lt;?\u0026gt;\u0026gt; getServiceProcessors(); // fixme @Deprecated  void registerAppProcessors(Map\u0026lt;ProcessorKey, SoaServiceDefinition\u0026lt;?\u0026gt;\u0026gt; processors); Application getApplication(ProcessorKey key); // fixme @Deprecated  void registerAppMap(Map\u0026lt;ProcessorKey,Application\u0026gt; applicationMap); Executor getDispatcher(); List\u0026lt;Filter\u0026gt; getFilters(); void startup(); /** * 0:unknow; * 1:creating; * 2:running; * 3:shutting * 4:down * * @return status of container */ int status(); /** * 容器内未完成的请求计数 */ AtomicInteger requestCounter(); } SpringApplicationLoader\n这个类是SpringAppLoader和SpringBootAppLoader的结合,当services.xml存在时,则通过传统的方式加载spring.否则通过spring boot的方式启动spring  这里首先尝试用传统方式,读取services.xml来启动spring.当services.xml不存在时就会抛出异常转而使用spring boot启动.   这里会读取maven命令行配置项springboot.main.class来获得启动基类 获得启动基类后,就像我们以前使用spring boot一样,通过invoke调用spring boot的main方法即可启动spring容器. spring容器启动完之后,新旧版本都会调用afterSpringApplicationStart(),把dapeng service注册到dapeng container里,这里没变.  spring容器启动之后,主要的改动在dapeng-springboot-project下的两个项目.\ndapeng-spring-boot-autoconfigure用于读取dpeng自定义的配置(也就是dapeng.scan.base-packages定义的dapeng-service路径),并把配置传给dapeng-config-spring下的ServiceAnnotationBeanPostProcessor来处理包含@DapengService的服务类\nDapengSpringAutoConfiguration\n这里就是读取dapeng.scan.base-packages的配置(需要扫描的目录),并传到ServiceAnnotationBeanPostProcessor中处理\nServiceAnnotationBeanPostProcessor\n流程大约是:\n 在dapeng.scan.base-packages指定的目录下扫描获取含有@DapengService的类 然后根据这些类生成service bean并注册到spring容器里.  这里的关键是:\n//自定义拦截哪些注解 scanner.addIncludeFilter(new AnnotationTypeFilter(DapengService.class)); public class DapengClassPathBeanDefinitionScanner extends ClassPathBeanDefinitionScanner { public DapengClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, Environment environment, ResourceLoader resourceLoader) { //这个类主要就是为了让useDefaultFilters=false  this(registry, false, environment, resourceLoader); } } 看一下ClassPathBeanDefinitionScanner的源码吧:\n/** * Create a new {@code ClassPathBeanDefinitionScanner} for the given bean factory. * \u0026lt;p\u0026gt;If the passed-in bean factory does not only implement the * {@code BeanDefinitionRegistry} interface but also the {@code ResourceLoader} * interface, it will be used as default {@code ResourceLoader} as well. This will * usually be the case for {@link org.springframework.context.ApplicationContext} * implementations. * \u0026lt;p\u0026gt;If given a plain {@code BeanDefinitionRegistry}, the default {@code ResourceLoader} * will be a {@link org.springframework.core.io.support.PathMatchingResourcePatternResolver}. * \u0026lt;p\u0026gt;If the passed-in bean factory also implements {@link EnvironmentCapable} its * environment will be used by this reader. Otherwise, the reader will initialize and * use a {@link org.springframework.core.env.StandardEnvironment}. All * {@code ApplicationContext} implementations are {@code EnvironmentCapable}, while * normal {@code BeanFactory} implementations are not. * @param registry the {@code BeanFactory} to load bean definitions into, in the form * of a {@code BeanDefinitionRegistry} * @param useDefaultFilters whether to include the default filters for the * {@link org.springframework.stereotype.Component @Component}, * {@link org.springframework.stereotype.Repository @Repository}, * {@link org.springframework.stereotype.Service @Service}, and * {@link org.springframework.stereotype.Controller @Controller} stereotype annotations * @see #setResourceLoader * @see #setEnvironment */ public ClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters, Environment environment, @Nullable ResourceLoader resourceLoader) { Assert.notNull(registry, \u0026#34;BeanDefinitionRegistry must not be null\u0026#34;); this.registry = registry; //useDefaultFilters默认是true \tif (useDefaultFilters) { registerDefaultFilters(); } setEnvironment(environment); setResourceLoader(resourceLoader); } protected void registerDefaultFilters() { this.includeFilters.add(new AnnotationTypeFilter(Component.class)); ClassLoader cl = ClassPathScanningCandidateComponentProvider.class.getClassLoader(); try { this.includeFilters.add(new AnnotationTypeFilter( ((Class\u0026lt;? extends Annotation\u0026gt;) ClassUtils.forName(\u0026#34;javax.annotation.ManagedBean\u0026#34;, cl)), false)); logger.trace(\u0026#34;JSR-250 \u0026#39;javax.annotation.ManagedBean\u0026#39; found and supported for component scanning\u0026#34;); } catch (ClassNotFoundException ex) { // JSR-250 1.1 API (as included in Java EE 6) not available - simply skip. \t} try { this.includeFilters.add(new AnnotationTypeFilter( ((Class\u0026lt;? extends Annotation\u0026gt;) ClassUtils.forName(\u0026#34;javax.inject.Named\u0026#34;, cl)), false)); logger.trace(\u0026#34;JSR-330 \u0026#39;javax.inject.Named\u0026#39; annotation found and supported for component scanning\u0026#34;); } catch (ClassNotFoundException ex) { // JSR-330 API not available - simply skip. \t} }  spring boot默认拦截的是@Component,@Repository,@Service,@Controller,这里改成了只拦截@DapengService.   service bean注册过程:\nprivate void registerServiceBean(BeanDefinitionHolder beanDefinitionHolder, BeanDefinitionRegistry registry, DapengClassPathBeanDefinitionScanner scanner) { //服务实现类名称.  String annotatedServiceBeanName = beanDefinitionHolder.getBeanName(); //根据给定的服务实现类 注册其对应的 Processor Bean -\u0026gt; SoaServiceDefinition.  AbstractBeanDefinition processorDefinition = buildServiceProcessorDefinition(annotatedServiceBeanName); String processorBeanName = annotatedServiceBeanName + SERVICE_PROCESSOR_NAME_SUFFIX; registerBeanDefinition(processorBeanName, processorDefinition, registry, scanner); } private AbstractBeanDefinition buildServiceProcessorDefinition(String serviceBeanName) { BeanDefinitionBuilder builder = rootBeanDefinition(ServiceBeanProcessorFactory.class); // Set 原先的 beanName 到构造函数中去.  builder.addConstructorArgReference(serviceBeanName); // builder.addConstructorArgValue(serviceBeanName);  return builder.getBeanDefinition(); }  HelloService最终注册的bean name = helloServiceImpl_Processor 这里注册使用的工厂类是dapeng自定义的ServiceBeanProcessorFactory,得到的bean type是SoaServiceDefinition  其它注意点 api里面的service.thrift需要额外定义group(这个应该是和nacos的dataId命名空间有关)\n 启动基类使用@PropertySource来指定spring boot的配置文件 此外maven命令行里面的spring.config.location貌似也是相同作用.实测下来两者可以都不配,有默认路径兜底,如果配错则都会报错.  nacos(feign)如何注册和发现服务  调研@EnableFeignClients原理 调研@EnableDiscoveryClient原理 调研@FeignClient原理  ","permalink":"http://euthpic.github.io/tech/dapeng-sc%E8%B0%83%E7%A0%94/","summary":"调研目标 项目结构,项目配置,启动脚本的变化 dapeng-sc的主要修改点 nacos(feign)如何注册和发现服务 项目结构,项目配置,启动脚","title":"dapeng-sc调研"},{"content":"需求背景 有个三级目录(例如国家-省份-城市)的数据要以树的形式展示,由于每一级都是独立的实体,需要单独一张表存储,不同级别的id会重复,于是前端提意见要求整棵树不同级别的id都不能重复,不然他们的组件会出问题.于是只好研究一下多表统一主键的方案.\nSnowflake雪花算法 虽然很自然就想到了分布式id中的雪花算法,不过我们对分布式场景和id严格自增没有要求,这个方案麻烦,没有必要.\nUUID 从唯一性想到uuid也是很自然的. MySQL5.7不支持建表时使用函数,可以用触发器替代.\nCREATE TABLE FOO ( id CHAR(36) PRIMARY KEY ); DELIMITER ;; CREATE TRIGGER `foo_before_insert` BEFORE INSERT ON `foo` FOR EACH ROW BEGIN IF new.id IS NULL THEN SET new.id = uuid(); END IF; END;; DELIMITER ; 不过我们很少让MySQL自己生成uuid,数据库的资源还是很宝贵的,这应该让应用自己生成. 此外UUID当主键的利弊也讨论很充分了,占用空间以及插入慢,适用于数据少的情况.\n序列表 思路是另外专门用一张表存放自增的id,分表新建数据的时候来这里拿号.\n## 创建序列表 DROP TABLE IF EXISTS `sequence`; CREATE TABLE `sequence` ( `tablename` varchar(64) NOT NULL, `nextid` bigint(20) DEFAULT NULL, PRIMARY KEY (`tablename`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; ## 创建主键策略函数 DROP FUNCTION IF EXISTS `auto_seq`; CREATE DEFINER = `root`@`localhost` FUNCTION `auto_seq`(tb_purpose VARCHAR(64)) RETURNS int(11) BEGIN DECLARE reid int; set reid = (select `nextid` from `sequence` where `tablename`=tb_purpose limit 1); update `sequence` set `nextid`=reid + 1; return reid; end; ## 在分表中创建触发器，此触发器为插入前触发，调用主键策略函数返回的结果赋予id DROP TRIGGER `auto_seq`; CREATE TRIGGER `auto_seq` BEFORE INSERT ON `item` FOR EACH ROW begin set new.id = auto_seq(\u0026#39;item\u0026#39;); end; 缺点是多张表同时写入的时候存在竞争.\n间隔插入 上面的做法是插入数据时多张表排同一条队,其实没必要,只要提前约定每张表拿哪个id就可以保证不重复.比如有N张表,那么N个id为一个循环,第一个位置留给表A,第二个位置留给表B\u0026hellip;. 这可以通过设置MySQL生成id的步进和id初始值即可.\n## 设置步长为3 SET @@auto_increment_increment=3; ## 错开分表的初始id CREATE TABLE `A` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=1; CREATE TABLE `B` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=2; CREATE TABLE `C` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=3; 不过这个步长控制的粒度是库级别,会影响别的表.\n分段插入 更进一步,直接划分多个大的区段就可以避免设置步进带来的问题.\n## 错开分表的初始id CREATE TABLE `A` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=1; CREATE TABLE `B` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=100000; CREATE TABLE `C` (ID INT PRIMARY KEY AUTO_INCREMENT) AUTO_INCREMENT=10000000; 表A的id范围是1~99999,表B是100000~9999999,表C是10000000~MAX,只要能保证数据量不是非常大以至于耗尽约定的范围就行,这个方法最简单,我们的各级目录的数量又是很有限的,所以就用了这个方法. 另外用这个方案时要记得写清楚注释,不然别人手抖改了AUTO_INCREMENT就出问题了.\n","permalink":"http://euthpic.github.io/tech/mysql%E5%A4%9A%E8%A1%A8%E7%BB%9F%E4%B8%80%E4%B8%BB%E9%94%AE%E7%AD%96%E7%95%A5/","summary":"需求背景 有个三级目录(例如国家-省份-城市)的数据要以树的形式展示,由于每一级都是独立的实体,需要单独一张表存储,不同级别的id会重复,于是","title":"MySQL多表统一主键策略"},{"content":"看完\u0026lt;快学scala\u0026gt;的一些笔记.\n该书的课后练习参考答案: https://github.com/vybae/scala-hello\n 类型推断错误有时候ide检测不出来 ide联想到的api不能总是及时展示出来,有时候忘记语法api什么的,可能自己写的是对的,但是没写完之前ide判断是错误的 定义变量用val和var,前者不可变量.后者可变量.推荐尽量使用val 若val/var或者表达式未赋值,则默认值为Unit空 操作符其实是方法,比如a+b是a.+(b)的简写 可以使用几乎任何符号来为方法命名, 没有三目运算符,但是可通过if else来替代 可以返回不指定的类型,也就是返回的类型不是通过方法头来确定的,而是根据方法最终的结果决定. 上面这条规定其实是由于val和var的引进,使得变量/常量的类型不需要提前声明,因而可在赋值时才确定. 函数可以不声明返回类型(除了递归),但是函数的所有参数必须声明类型. 引入了一个Unit类,写作(),相当于java中的void,像下面这条语句,没有else语句,如果if条件不成立,需要走else,那么else的返回值默认是Unit.  val a = if(n \u0026gt; 0) \u0026#34;Hello World\u0026#34;   不需要加分号表示语句结束,除非一行上有多条语句.\n  代码块也是一种表达式,有值,值为表达式的最后语句的返回值.\n   for循环分为for to和for until语句,区别在于最后一次是否执行\n  没有break和continue关键字,需要引入util.control.Breaks._包\n  for循环默认返回的是Unit空值,不过可以配合yield使用返回一个集合.yield的作用是把当前的元素记下来,保存在集合中,循环结束后将返回该集合.\n  函数比较灵活,没有参数时不用写括号,不过也看不出来调用的是变量还是方法.\n  //sortWith里面的\u0026#34;_\u0026#34;是参数的简略表示 val r1 = arr1.sortWith(_ \u0026lt; _)   object中有main方法的话就只会执行main方法,否则顺序执行object中全部代码块\n  对list,set,map的操作很灵活,可通过操作符而不是api来操作集合,并且它们之间的转化也很方便.\n  对list,set,map的操作一般都是返回新的集合,不会改变原来的集合.\n  所谓守卫,就是以if开头的Boolean表达式\n  没有返回值(实际返回的是Unit)的函数称为过程\n  元组的访问从1开始而不是0.元组常用于返回值不止一个的情况.\n  yield关键字好像不能放在大括号里面\n  可变的map创建时要注意有new关键字\n  如果没有给某个参数传递值,那么Scala将会传递一个默认值(仅限基本的Int,String等).但如果这个参数是自定义类型(抽象对象),Scala没有它的默认值,此时我们需要借助implicit给它传默认值,也就是隐式参数.在同一个上下文环境中,同一类型的隐式参数只能有一个.\n  有意思的???\noverride def deleteFinalPriceOfferFeedbackById(id: Int): Unit = ??? /** `???` can be used for marking methods that remain to be implemented. * @throws NotImplementedError */ //也就是等待实现的方法,也就是java中的抽象方法?  def ??? : Nothing = throw new NotImplementedError 变长参数\ndef sum(args:Int*)={ var result=0 for (arg \u0026lt;- args ) { result+=arg } result } println(sum(7,2,3)) for循环正常只能顺序遍历(也就是i++),如果要倒叙遍历,需要用reverse函数:\n//实现i--打印 for (i \u0026lt;- (0 to 10).reverse) { println(i) } Chapter5 类   调用无参方法时,可以写上圆括号,也可以不写.推荐对于改值器方法(即改变对象状态的方法)使用括号,对取值器方法去掉括号.\n  无参方法声明时可以不带(),这样调用的时候一定不能带()\n  Scala对于类中的每个字段都会设置成私有,并提供公有的getter和setter方法\n  val的字段不提供getter方法,var的字段setter和getter都提供\n  private的字段,其setter和getter都是private\n  一个类如果没有显式定义主构造器,那么它默认拥有一个无参的主构造器\n  辅助构造器的名称为this\n  val p1= new Person //主构造器 val p2=new Person(\u0026#34;Fred\u0026#34;) //第一个辅助构造器 val p3=new Person(\u0026#34;Fred\u0026#34;,42) //第二个辅助构造器   每个类都有主构造器,它与类定义交织在一起:\n  class Person(val name:String,val age: Int) { // (...)中的内容就是主构造器的参数 }   在Scala中,每个对象都有它自己的内部类,也就是a.Member和b.Member是不同的两个类\n  Chapter6 对象   对象(object)也就是类的单个实例\n  伴生对象也就是和类同名的对象,例如:\n  class Account{ ... } object Account{// 伴生对象  ... }   类和它的伴生对象可以相互访问private的字段/函数/构造方法,它们必须存在与同一个源文件中\n  一般都会定义apply()方法,类和对象都可以\n  假设有个Person类和它的伴生对象,声明了个该类型的person对象:\n  //显式调用apply Person.apply(...) //调用的是伴生对象定义的方法 person.apply(...)\t//调用的是类定义的方法  //上面的可以省去apply,效果是等价的 Person(...) person(...)   Scala的程序从一个对象的main方法开始,或者拓展App特性(extends App),这样就会执行对象内的所有代码块(不包含方法)\n  Chapter7 包和引入   一个文件可定义多个包.同一个包可以定义在多个文件当中(也就是要确认一个包里面有什么东西的话,在java中直接找到对应的目录即可,但是scala中可能得扫描全部的文件才能确认)\n  子包中可以访问父包内容,不需要写完整的包名.\n  在Java中,包名是绝对的;但是在Scala中,包名是相对的,因此引用错误的同名包/类的可能性较大,解决方法是使用绝对包名\n  每个包可以有唯一对应的包对象,可供包内访问调用\n  可以通过private[类名]来限制函数/字段的可见性,例如\n  package com.horstmann.impatient.people class Person{ private[people] def description1 =\u0026#34;....\u0026#34; private[impatient] def description2=\u0026#34;...\u0026#34; }   import语句可以出现在哎任何地方(不限于文件顶部,也包括方法内部),作用域延伸到同一代码块的末尾\n  以下三个包总会被隐式引入:\n  import java.lang._ import scala._ import Predef._ //Predef里面有Map,新建一个map时需要小心你需要的Predef的Map还是immutable/mutable里面的Map   Chapter8 继承   重写方法必须使用override,重写抽象方法除外\n  在Java中,protected修饰的成员对于所在包和子类可见.但是在Scala中只对子类可见,如果需要包可见,可以用包修饰符(见chapter 7)\n  构造器内不应该依赖val的值,因为它使用的是val的初始值,来看个例子:\n  class Creature{ val range: Int = 10 val env : Array[Int] =new Array[Int] (range) } class Ant extends Creature{ override val range = 2 //此时有个隐式的env=new Array[Int] (0)  //在父类中env依赖于range,可是构造器优先于字段的初始化,因此range还未初始化为10,而是有个默认的值0,于是env拿到这个0去为自己初始化了  //解决方法, 不太优雅  class Ant extends{ override val range = 2 } with Creature }   Null类型的唯一实例是null值,可以将null赋值给任何引用,但不能赋值给值类型的变量,比如Int.这决定了我们在使用int,long等基本类型时不可能发生空指针异常\n  Nothing类型没有实例.比如,空列表的类型是List[Nothing],它是List[T]的子类\n  判断两个对象是否相等可以直接使用==操作符,因为它会调用equals()\n  Chapter9 文件和正则表达式  如果字符串中含有\\或者\u0026quot;\u0026ldquo;的话可以使用原始字符串\u0026quot;\u0026ldquo;\u0026ldquo;\u0026ldquo;\u0026ldquo;\u0026ldquo;来定义,这样比转义字符易读些  Chapter10 特质 类可以实现任意数量的特质 特质可以要求实现它们的类具备特定的字段/方法/超类 和Java接口不同,Scala特质可以提供方法和字段的实现 当你将多个特质叠加在一起时,顺序很重要--其方法先被执行的特质排在更后面 特质的关键词是 trait\n 特质中不需要将方法声明为abstract\u0026mdash;因为这些方法默认就是抽象的 特质也可以有构造器.特质构造器的构造顺序从左往右进行(而特质执行顺序则从右往左进行) 构造器的执行顺序: 超类构造器 -\u0026gt; 父特质构造器 -\u0026gt; 从左往右的特质构造器 -\u0026gt;类  Chapter13 集合   +将元素添加到无先后次序的集合中\n  -和\u0026ndash;移除元素\n  +:和:+向前或向后追加到序列\n  ++将两个集合串接到一起\n  list要么是Nil(即空列表),要么由head和tail组成.head是头元素,tail也是一个list,由除了头元素以外的其它元素组成\n  注意map的声明,是(k-\u0026gt;v)而不是(k,v)\n  注意区分map取值的两种方式,map.get(\u0026ldquo;key\u0026rdquo;)返回的是Some类型,map(\u0026ldquo;key\u0026rdquo;)返回的是单纯的value.\n  map(\u0026ldquo;key\u0026rdquo;)如果找不到对应的元素,就会报NoSuchElementException的异常,所以推荐用**map.get()或者map.getOrElse()**比较安全.\n  list转成map: list.groupby(_.key) 转换后的map使用key做键,值是一个list\n  zipWithIndex()方法是为集合的每个元素创建一个下标/索引:\n  val days = Array(\u0026#34;Sunday\u0026#34;, \u0026#34;Monday\u0026#34;, \u0026#34;Tuesday\u0026#34;, \u0026#34;Wednesday\u0026#34;,\u0026#34;Thursday\u0026#34;, \u0026#34;Friday\u0026#34;, \u0026#34;Saturday\u0026#34;) days.zipWithIndex.foreach(println(_)) //(Sunday,0),(Monday,1),(Tuesday,2),(Wednesday,3),(Thursday,4),(Friday,5),(Saturday,6)   zipWithIndex主要的作用在于对list使用map()遍历时,可以获取元素的下标(在java中,fori循环可以获取下标,但在scala中可能是觉得这种循环写法太不优雅,所以用这个方法来替代)\n  request.priceOfferItems.zipWithIndex.map(x =\u0026gt; FinancePriceRequestItem(x._1.price, logisticsFeeList(x._2).price)) //request.priceOfferItems是个list,这段的作用是实际跟fori是一样的.   flatMap = map + flatten 即先对集合中的每个元素进行map，再对map后的每个元素（map后的每个元素必须还是集合）中的每个元素进行flatten\n  Chapter14 模式匹配   模式匹配发生在运行期,此时泛型已经被擦除,所以不能用模式匹配来匹配特定类型的Map(如果匹配不上就会报错)\n  但是数组中的类型是支持匹配的\n  val n=1 //如果声明时ar的类型是具体确定的,那么根据类型匹配时就会报错  //val ar=1 \t//所以得用下面这种,声明为Any类型再赋值  val ar:Any = if(n\u0026gt; 0) 1 else \u0026#34;Hello\u0026#34; val s = ar match { case i:Int =\u0026gt; \u0026#34;张三\u0026#34; case s:String =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34;   在模式匹配里,如果case都没有匹配成功就会报错,所以最后都要用 case _来兜底\n  模式匹配列表和元组时,变量可以绑定到它们的不同部分,比如(0,\u0026hellip;)以0开头的结构,或者(x,y)只包含x和y的结构等.这是通过提取器机制实现的.\n  BitInt和BigDecimal可以同时进行除法操作和取模操作,操作符是/%,但只有这种情况(可能是这两种运算的关联性强),其它的什么同时加减啊都是不支持的\n  在模式匹配中,匹配数组/列表/元组时, _* 操作符可以匹配剩余的全部元素\n  样例类是一种适合用于模式匹配的特殊类\n  样例class必须带括号,样例object必须不带括号\n  样例类好处:\n 创建实例时不需要new 免费得到toString,equals,hashCode和copy方法(浅克隆)    密封类通过关键字sealed声明,密封类的所有子类都必须在与该密封类相同的文件中定义.\n  ","permalink":"http://euthpic.github.io/tech/scala%E7%AE%80%E6%98%93%E7%AC%94%E8%AE%B0/","summary":"看完\u0026lt;快学scala\u0026gt;的一些笔记. 该书的课后练习参考答案: https://github.com/vybae/scala-hello 类型推断错误有时候ide检测不出来 ide联想到的api不能总是及时展","title":"Scala简易笔记"},{"content":"import java.io.{PrintWriter} import scala.collection.mutable import scala.io.{Source, StdIn} import scala.reflect.io.File case class Person(id: Int, name: String, age: Int) /** * Scala 基本笔记 */ object ScalaLesson extends App { /** * 基础-算术 */ def lesson1() = { println(1.+(2)) println(2.-(2)) println(2.*(2)) println(2./(2)) println(10 max 2) } /** * 基础-重载 */ def lesson2() = { val str = \u0026#34;Hello\u0026#34;(2) //等同于java中的”Hello“.charAt(2)  println(str) //实现原理为StringOps包中的一个apply方法：def apply(n:Int): Char  val s = \u0026#34;Hello\u0026#34;.apply(2) println(s) } /** * 基础-字符串操作 */ def lesson3() = { val str = \u0026#34;Hello World\u0026#34; //获取字符串的第一个字符  println(str.head) //获取字符串的最后一个字符  println(str.last) //获取字符串前3个字符  println(str.take(3)) //获取字符串后3个字符  println(str.takeRight(3)) //删除字符串前3个字符  println(str.drop(3)) //删除字符串后3个字符  println(str.dropRight(3)) } /** * 结构与函数-条件判断 */ def lesson4(n: Int) = { //if/else语法结构与java结构一致，在scala中表达式是有值的，这个值就是跟在if/else之后的表达式的值  val s = if (n \u0026gt; 1) 1 else 0 //等同于以下方式,不过在scala中推荐使用val而尽量不使用var可变量  //注：scala中不支持三目运算，scala把java的三目运算结合在了if/else中  var a = 0 if (n \u0026gt; 1) a = 1 else a = 0 println(s) println(a) } /** * 结构与函数-公共超类 */ def lesson5(n: Int) = { //在scala中存在一个公共超类Any，java中只能返回指定类型，scala可以返回不指定的类型,支持混合型表达式  val s = if (n \u0026gt; 0) \u0026#34;Hello World\u0026#34; else 1 println(s) //在scala中如果if/else中缺失了else后部分，如：  val a = if (n \u0026gt; 0) \u0026#34;Hello World\u0026#34; //在scala中每个表达式都是有值的，如果缺失了else后部分，为了解决这个问题，在java的基础上引入了一个Unit类，写作()，这样就等同于if(n\u0026gt;0) \u0026#34;Hello World\u0026#34; else ()  //这个()相当于一个无用值，Unit与java中的void相当，区别在于void没有值，Unit有一个“无用”的值。  println(a) } /** * 结构与函数-终止语句 */ def lesson6(n: Int) = { //scala中不需要加分号结束,如果是多行写成1行则需要分号结束，但不建议这样写,如：  val s = if (n \u0026gt; 0) { var r = 1 * n r -= 1 } else 1 //  } /** * 结构与函数-块表达式 */ def lesson7(n: Int) = { //与java中的{}一致，但scala的块是一种表达式，所以有值，值为表达式的最后语句返回值  val s = { val a = 5 a * n } println(s) } /** * 结构与函数-输入与输出 */ def lesson8() = { //不换行打印  //print(\u0026#34;Scala\u0026#34;)  //换行打印  //println(\u0026#34;你好\u0026#34;)  //上面2个打印等同于下面语句  //println(\u0026#34;Scala\u0026#34; + \u0026#34;你好\u0026#34;)  //带C风格格式化字符串的函数  //printf(\u0026#34;Hello,%s! 你是一款很好的语言,你存在有 %y 年了吗？\u0026#34;,\u0026#34;Scala\u0026#34;,10)  //读取控制台一行输入(2.11.0以后的版本使用StdIn)  val name = StdIn.readLine() println(\u0026#34;你输入的姓名是:\u0026#34; + name) val age = StdIn.readInt() println(\u0026#34;你输入的年龄是:\u0026#34; + age) } /** * 结构与函数-while循环 */ def lesson9(n: Int) = { //与java中的while和do循环一致  var i = n while (i \u0026gt; 0) { println(i * 3) i -= 1 } } /** * 结构与函数-for循环 */ def lesson10(n: Int) = { //scala中的for循环与java的结构不一样,scala的结构为 for(i \u0026lt;- 表达式)  for (i \u0026lt;- 0 to n) println(i * 3) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //在RichInt类中存在 to 这个方法。0 to n 代表的是 0到n的区间（包含n），如果只是0到n-1的话这采用until方法，如下:  for (i \u0026lt;- 0 until n) println(i * 3) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) val str = \u0026#34;Hello\u0026#34; var sum = 0 //在循环中不仅可以对数字区间进行遍历，也可以对字符串进行遍历  for (ch \u0026lt;- str) sum += ch println(sum) } /** * 结构与函数-退出循环 */ def lesson11(n: Int) = { //scala中的并没有提供break或者continue语句来退出循环，如果需要break该怎么做，如下：  //1：需要引入import util.control.Breaks._包  import util.control.Breaks._ //break例子  breakable( for (i \u0026lt;- 0 to 5) { println(i) if (i == n) break } ) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //continue例子,需要注意判断要放在最前面  for (i \u0026lt;- 0 to 5) { breakable { if (i == n) break println(i) } } println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //中断嵌套循环  breakable( for (i \u0026lt;- 0 to 5) { println(i) breakable( for (a \u0026lt;- 1 to 3) { println(\u0026#34;i * a = \u0026#34; + i * a) if (i * a == 1) break } ) } ) } /** * 结构与函数-高级循环 */ def lesson12(n: Int) = { //scala支持多个生成器,之间用分号隔开,(等同于多个嵌套for循环)如下:  for (i \u0026lt;- 0 to 3; j \u0026lt;- 1 to 4) println(i * j) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //scala支持守卫,以if开头的Boolean表达式(注：if之前没有分号):  for (i \u0026lt;- 0 to 3; j \u0026lt;- 1 to 4 if i \u0026lt; j) println(i * j) //scala还支持引入循环中的变量：  for (i \u0026lt;- 1 to 3; j \u0026lt;- n to 3) println(i * j) //for推导式，以yield开始:  val list = for (i \u0026lt;- 1 to 10) yield i % 3 println(list) } /** * 结构与函数-默认参数与带名参数 */ def lesson13(name: String, n: Int = 18) = { println(s\u0026#34;你的姓名是：$name,年龄是:$n\u0026#34;) } /** * 懒值，只需要在需要懒加载的值或函数前面加 lazy关键字即可,如： * 只有在调用的时候才执行，不调用的时候不执行 */ lazy val m = System.currentTimeMillis() /** * 结构与函数-异常处理 */ def lesson14(n: Int): Unit = { //scala异常的工作原理与java的一样，区别在于scala没有受体异常--不需要声明函数或者方法可能会抛出某种异常  // throw new XXException(xxx)  //捕抓异常采用模式匹配  val a = try { 10 / n } catch { case ex: Exception =\u0026gt; println(ex.getMessage) } } /** * 数组操作 */ def lesson15() = { import scala.collection.mutable.ArrayBuffer //定长数组  val arr1 = new Array[Int](5) println(arr1.toString, arr1.length) //推导数组，类型根据推导出来，提供初始值的时候不需要new  val arr2 = Array(\u0026#34;Hello\u0026#34;, \u0026#34;world\u0026#34;) println(arr2.toString, arr2.length) //根据角标获取值时采用()而不是java的[]  println(arr2(0)) //变长数组：数组缓冲  //创建空的数组缓冲，准备存放整数  val arr3 = ArrayBuffer[Int]() //用 += 在尾端添加元素  arr3 += 1 //得到ArrayBuffer(1)  arr3 += (1, 3, 4, 8, 2) //得到ArrayBuffer(1,1,3,4,8,2)  arr3 ++= Array(9, 3, 5) // 可以用 ++= 操作符追加任何的集合  //得到ArrayBuffer(1,1,3,4,8,2,9,3,5)  arr3.trimEnd(2) //移除最后2个元素  arr3.trimStart(2) //移除最前2个元素  //可以在任意位置插入元素(低效)  arr3.insert(1, 3) //可以在任意地方移除多少个元素(低效)  arr3.remove(1, 2) //数组缓冲转成Array  arr3.toArray //数组转数组缓冲  arr1.toBuffer } /** * 数组操作-遍历 */ def lesson16() = { val arr1 = Array(1, 2, 6, 3, 8, 4) //需要下标时  for (i \u0026lt;- 0 until arr1.length) println(i + \u0026#34; : \u0026#34; + arr1(i)) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //不需要下标时  for (e \u0026lt;- arr1) println(e) println(\u0026#34;-----------------这是分割线-------------------\u0026#34;) //数组转换,采用yield关键字  val r = for (e \u0026lt;- arr1) yield 2 * e println(r.toList) } /** * 数组操作-常用算法 */ def lesson17() = { val arr1 = Array(1, 2, 6, 3, 8, 4) //求和  println(arr1.sum) //获取最大值  println(arr1.max) //获取最小值  println(arr1.min) //获取平均值  println(if (arr1.nonEmpty) arr1.sum / arr1.length else 0) //从小到大排序(注:排序得到的是一个新数组缓冲，原数组不会改变)  val r = arr1.sorted //指定排序  val r1 = arr1.sortWith(_ \u0026lt; _) println(r.toList) println(r1.toList) //数组拼接成字符串  println(r1.toString) println(r1.mkString) println(r1.mkString(\u0026#34;,\u0026#34;)) } /** * 数组操作-多维数组 */ def lesson18(): Unit = { //与java一样，多维数组是通过数组的数组来实现的。Double的二维数组类型为Array[Array[Double]]  //要构造这样函数可以采用ofDim方法：  val matrix = Array.ofDim[Double](3, 4) //三行四列  //访问这样的数组，使用两对圆括号(row)(column):  matrix(1)(2) } /** * 数组操作-与java的相互操作 */ def lesson19(): Unit = { //引入相应的包  import scala.collection.JavaConversions.bufferAsJavaList import scala.collection.mutable.ArrayBuffer val command = ArrayBuffer(\u0026#34;ls\u0026#34;, \u0026#34;-al\u0026#34;, \u0026#34;/home/cay\u0026#34;) val pb = new ProcessBuilder(command) //Scala转java  import scala.collection.JavaConversions.asScalaBuffer import scala.collection.mutable.Buffer val cmd: Buffer[String] = pb.command() //java转Scala  } /** * 映射与元组-映射 */ def lesson20(): Unit = { //构建一个不可变的映射(建议)  val map = Map(\u0026#34;张三\u0026#34; -\u0026gt; 98, \u0026#34;李四\u0026#34; -\u0026gt; 83, \u0026#34;王五\u0026#34; -\u0026gt; 100) //构建一个可变的映射(不建议)  val map1 = mutable.Map(\u0026#34;张三\u0026#34; -\u0026gt; 98, \u0026#34;李四\u0026#34; -\u0026gt; 83, \u0026#34;王五\u0026#34; -\u0026gt; 100) //构建一个空的映射，需要指定类型参数  val map2 = new mutable.HashMap[String, Int]() // -\u0026gt; 操作符用来创建对偶  // (\u0026#34;张三\u0026#34; -\u0026gt; 98) 等同于 (\u0026#34;张三\u0026#34; , 98)，所以也可以用以下方式定义映射:  val map3 = Map((\u0026#34;张三\u0026#34;, 98), (\u0026#34;李四\u0026#34;, 83), (\u0026#34;王五\u0026#34;, 100)) //获取映射中的值，直接使用(),相当于java中的map.get(\u0026#34;张三\u0026#34;)  println(map(\u0026#34;张三\u0026#34;)) //如果key不存在，则会抛异常，检查映射中是否存在某个key，可以使用contains方法:  val value = if (map.contains(\u0026#34;张三\u0026#34;)) map(\u0026#34;张三\u0026#34;) else 0 //可以简写成getOrElse(key,result):  println(map.getOrElse(\u0026#34;张三\u0026#34;, 0)) //更新可变映射中的值  map1(\u0026#34;张三\u0026#34;) = 67 //增加可变映射中的值  map1(\u0026#34;赵六\u0026#34;) = 74 //可以使用 += 增加多个关系  map1 += (\u0026#34;王七\u0026#34; -\u0026gt; 35, \u0026#34;蔡八\u0026#34; -\u0026gt; 86) //可以使用 -= 移除对应的键值  map1 -= \u0026#34;王五\u0026#34; //可以使用 + 操作符生成新的映射  val map4 = map + (\u0026#34;王七\u0026#34; -\u0026gt; 35, \u0026#34;蔡八\u0026#34; -\u0026gt; 86) //映射遍历 for((k,v) \u0026lt;- map)  } /** * 映射与元组-元组 */ def lesson21() = { //映射是键值对的集合，对偶则是元组（tuple）的最简单形态--元组是不同类型的值的聚集，元组的值是通过将单个的值包含在()构成的,如：  val t = (2, \u0026#34;张三\u0026#34;, 4d) //类型分别为 Tuple[Int, String, Double]  //元组的访问可以使用_1、_2、_3这样  println(t._1) println(t._2) println(t._3) //也可以在返回值的时候就定义下来，通常使用这种,这样更为直观的表达出返回的结果分别代表什么  val (first, second, third) = t println(first) println(second) println(third) } /** * 映射与元组-拉链操作 */ def lesson22(): Unit = { //使用元组的原因之一是吧多个值绑定在一起，以方便它们能够被一起处理，这个通常可以使用zip方法来完成，如下：  val a1 = Array(\u0026#34;张三\u0026#34;, \u0026#34;李四\u0026#34;, \u0026#34;王五\u0026#34;) val a2 = Array(55, 26, 96) val p = a1.zip(a2) //得到的对偶数组为Array((\u0026#34;张三\u0026#34;,55),(\u0026#34;李四\u0026#34;,26),(\u0026#34;王五\u0026#34;,96))  //还可以把对偶数组转成映射  println(p.toMap) //得到结果 Map(张三 -\u0026gt; 55, 李四 -\u0026gt; 26, 王五 -\u0026gt; 96)  } /** * 对象-单例对象 * 在Scala中没有静态方法或者静态字段，可以通过object语法结构来定义 * */ object Accounts { //伴生对象  private var lastNumber = 0 def newUniqueNumber() = { lastNumber += 1; lastNumber } } /** * 对象-伴生对象 * 在Scala中没有静态方法或者静态字段，可以通过object语法结构来定义 * */ class Accounts { val id = Accounts.newUniqueNumber() } /** * 对象-扩展类或特质对象 */ abstract class UndoableAction(val desc: String) { def undo(): Unit def redo(): Unit } object DoNothingAction extends UndoableAction(\u0026#34;Do nothing\u0026#34;) { override def undo(): Unit = {} override def redo(): Unit = {} } //DoNothingAction可以被所有所需要这个缺省行为的地方共用  val actions = Map(\u0026#34;open\u0026#34; -\u0026gt; DoNothingAction, \u0026#34;save\u0026#34; -\u0026gt; DoNothingAction) /** * 对象-应用程序对象 * 每个Scala程序都必须从一个对象main方法开始，也可以扩展App特质 * object Hello{ * def main(args:Array[String]) { * println(\u0026#34;Hello World\u0026#34;) * } * } */ /** * 对象-枚举 * scala中并没有枚举类型，不过提供了一个Enumeration助手类，可以用于产出枚举 */ object TrafficLightColor extends Enumeration { val Red = Value(0, \u0026#34;Stop\u0026#34;) val Yellow = Value(10) val Green = Value(\u0026#34;Go\u0026#34;) } def lesson23(): Unit = { println(TrafficLightColor.Red) println(TrafficLightColor.Yellow) println(TrafficLightColor.Green) } /** * 文件操作-读取文件 */ def lesson24(): Unit = { //以指定的GBK字符集读取文件，第一个参数可以是字符串或者是java.io.File  val source = Source.fromFile(\u0026#34;贪腐词库.txt\u0026#34;, \u0026#34;GBK\u0026#34;) //获取所有行  val lines = source.getLines() //将所有行放到list中  val list = lines.toList //将文件用逗号串起来(注：旧版采用source.mkString,新版中获取不到值)  val str = list.mkString //println(\u0026#34;------------\u0026#34;+str)  //迭代打印集合  //list.foreach(it=\u0026gt;println(it))  //关闭流  source.close() } /** * 文件操作-从URL读取文件 */ def lesson25(): Unit = { val source = Source.fromURL(\u0026#34;https://www.baidu.com\u0026#34;, \u0026#34;UTF-8\u0026#34;) //获取所有行  val lines = source.getLines() //将所有行放到list中  val list = lines.toList //将文件用逗号串起来(注：旧版采用source.mkString,新版中获取不到值)  val str = list.mkString //println(\u0026#34;------------\u0026#34;+str)  //迭代打印集合  list.foreach(it =\u0026gt; println(it)) //关闭流  source.close() } /** * 文件操作-读取二进制文件 */ def lesson26(): Unit = { val file = File(\u0026#34;其他词库.txt\u0026#34;) val in = file.inputStream() val bytes = new Array[Byte](file.length.toInt) in.read(bytes) println(bytes.mkString(\u0026#34;,\u0026#34;)) in.close() } /** * 文件操作-写入文件 */ def lesson27() = { //scala中没有内建对文件的支持，所以使用java.io.PrintWriter  val out = new PrintWriter(\u0026#34;test.txt\u0026#34;) for (i \u0026lt;- 1 to 100) out.println(i) } /** * 正则表达式 */ def lesson28() = { //采用String中的r方法  val pattern = \u0026#34;[0-9]+\u0026#34;.r println(pattern findAllIn \u0026#34;02,2\u0026#34;) } /** * 操作符 */ def lesson29() = { //在scala中可以使用任意序列的操作字符作为定义,如:  val * = \u0026#34;ere\u0026#34; val \u0026amp; = \u0026#34;ere\u0026#34; val ! = \u0026#34;ere\u0026#34; //一旦遇到命名定义是scala关键字的还可以采用反引号来拯救(当然平时还是注意少用scala的关键字来命名)  val `type` = 354 //还可以使用 a 标识符 b 这样写  //1 to 10 //实际上是调用了 1.to(10) 的方法  //1-\u0026gt; 10 //等同调用 1.—\u0026gt;(10) 的方法  //1 toString //等同于 1.toString ，以上均为一元操作符，点号可以省略  //赋值操作符 a 操作符= b 等同于 a = a + b  //1 += 2 //等同于 1 = 1 + 2  //结合性,已冒号结束，操作符是右结合  //1::2::Nil //等同于 1::(2::Nil)  } import math._ /** * 高阶函数-值函数 */ def lesson30() = { val num = 3.14 //把 ceil函数赋值给fun  val fun = ceil _ val a = Array(2.14, 1.42, 2.0).map(fun) println(a.mkString(\u0026#34;,\u0026#34;)) } /** * 高阶函数-匿名函数 */ def lesson31() = { val s = (x: Double) =\u0026gt; 3 * x //等同于 def s(x:Double) = 3 * x  println(s(3)) } /** * 高阶函数-带函数参数的函数 */ def lesson32(f: (Double) =\u0026gt; Double) = { f(0.25) } //对于只出现一次的参数，可以使用_替代  //lesson32(ceil _)  /** * 高阶函数-一些有用的高阶函数 */ def lesson33() = { val list = List(\u0026#34;张三\u0026#34;, \u0026#34;李四\u0026#34;, \u0026#34;王五\u0026#34;) //map方法，遍历应用到集合中的所有元素，并返回全新的结果集合  println(list.map(it =\u0026gt; it -\u0026gt; \u0026#34;你好\u0026#34;)) //foreach方法,遍历集合的所有元素，但不返回结果(对于只出现一次的参数，可以使用_替代)  list.foreach(println(_)) //filter方法,根据条件过滤生成全新的集合  val f = list.filter(_ != \u0026#34;张三\u0026#34;) println(f) //sortWith方法,指定参数排序，返回全新的集合  println(list.sortWith(_ \u0026gt; _)) //sorted方法,从低到高排序，返回全新的集合  println(list.sortWith(_ \u0026gt; _).sorted) //groupBy方法,根据指定参数进行合并分组，返回map[object,List[object]]集合  val userList = List(Person(1, \u0026#34;张三\u0026#34;, 23), Person(2, \u0026#34;李四\u0026#34;, 45), Person(1, \u0026#34;王五\u0026#34;, 30)) val map: Map[Int, List[Person]] = userList.groupBy(_.id) //Map遍历,_1代表key,_2代表value  map.foreach(it =\u0026gt; { println(it._1) println(it._2) }) //exists方法,遍历判断条件是否成立  println(list.exists(_ == \u0026#34;张三\u0026#34;)) //distinct方法,去除重复的元素  println(list.distinct) } /** * 集合之间的操作 */ def lesson34() = { //注：在scala中不可变集合进行操作生成的都为新的集合，原有集合不发生改变  val list = List(1, 2, 3) val map = Map(1 -\u0026gt; \u0026#34;java\u0026#34;, 2 -\u0026gt; \u0026#34;scala\u0026#34;, 3 -\u0026gt; \u0026#34;php\u0026#34;) //List追加元素, 后面追加采用 :+ 前面追加采用 +:  println(list :+ 2) //List移除元素  println(list.toSet - 2) //List第一位置增加元素  println(5 +: list) //2个List合并，采用:::或者 ++ 或者 | 或者++:  println(list ++ List(6)) println(list ::: List(6)) println(list ++: List(6)) println(list.toSet | List(7).toSet) //list中移除另外一个List中所有包含的元素,采用 -- 或者 \u0026amp;~  println(list.toSet -- List(2)) println(list.toSet \u0026amp;~ List(2).toSet) //俩个list的交集  println(list.toSet \u0026amp; List(2).toSet) } /** * 模式匹配-更好的switch */ def lesson35(n: Int) = { //scala的switch更优雅的处理方式，在c和类C语言中，模式匹配必须在末尾显式的使用break语句进行退出switch，scala不会有这样的问题  //match与if类似，都是使用表达式。在很多时候match的使用更优雅与if/else if，这样不会出现多层嵌套的情况  val result = n match { case 1 =\u0026gt; \u0026#34;张三\u0026#34; case 2 =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(result) //模式用还可以使用守卫，如下：  val r = n match { case 1 =\u0026gt; \u0026#34;张三\u0026#34; case a if (a \u0026gt; 5) =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(r) //类型匹配，如下：  val ar: Any = if (n \u0026gt; 0) 1 else \u0026#34;Hello\u0026#34; val s = ar match { case i: Int =\u0026gt; \u0026#34;张三\u0026#34; case s: String =\u0026gt; \u0026#34;李四\u0026#34; case _ =\u0026gt; \u0026#34;王五\u0026#34; } println(s) } /** * 模式匹配-Option类型 */ def lesson36() = { //生成Option带值的元素  val o: Option[String] = Some(\u0026#34;张三\u0026#34;) println(o) //生成不带值的Option元素  val o1: Option[Nothing] = None println(o1) //Option元素获取值，采用get方法  println(o.get) //但是直接采用get方法很容易出现 java.util.NoSuchElementException: None.get异常，scala提供了组合方法getOrElse  println(o1.getOrElse(\u0026#34;\u0026#34;)) } /** * 模式匹配-断言 */ def lesson37(n: Int) = { //在scala中与java一样使用断言，格式 assert(条件,提示语)  assert(n \u0026gt; 1, \u0026#34;参数不能小于2\u0026#34;) } def lesson38() = { (0 to 10).map(println(_)) } lesson38() } ","permalink":"http://euthpic.github.io/tech/scala%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A0/","summary":"import java.io.{PrintWriter} import scala.collection.mutable import scala.io.{Source, StdIn} import scala.reflect.io.File case class Person(id: Int, name: String, age: Int) /** * Scala 基本笔记 */ object ScalaLesson extends App { /** * 基础-算术 */ def lesson1() = { println(1.+(2)) println(2.-(2)) println(2.*(2)) println(2./(2)) println(10 max 2) } /** * 基础-重载 */ def lesson2() = { val str = \u0026#34;Hello\u0026#34;(2) //等同","title":"Scala基础练习"},{"content":"前面我们提到的持久化,内存淘汰,管道等机制都是Redis在单个实例上的功能,但是随着系统并发量的不断升高,单个实例的redis就很难继续满足需求了.\n为了满足高并发需求,Redis提供了主从(Redis Replication),哨兵(Redis Sentinel),集群(Redis Cluster)三种架构,分别应用于不同情景.\n本文要介绍的就是主从架构,可以说,很多项目不一定会用到哨兵和集群,但是使用了redis的几乎都会用到主从.\n主从的作用 读写分离!\n读写分离!!\n还是读写分离!!!\n这个设计思路在前面体现过很多次了,可见其广泛的适用性.\n 在主从架构中,master(主节点)负责写,slave(从节点)负责读. 每当master中的数据被修改,它就会把相同的命令同步(传播)到各个slave,以实现整个集群的数据一致性. 这样master的读任务就会被各个slave分摊,写的效率就会提高,而读的效率可随着slave节点的增加而提高.  思考一个问题 : 为什么只能在master里写呢?多增加几个写节点(master)不就也提高了写的效率吗?\n不行,因为redis并不支持主主架构.这个问题得从线程安全的角度去思考了.\n我们把redis的每个节点都看成一个线程的话,原先单节点操作,也就是单线程,是不会有线程安全的隐患.但是拓展到了多节点,多个节点同时读写就需要考虑并发的问题了.\n如果是一主多从,读与读操作不会有问题.但是多主的情况下,写与写就会有问题了,假设A,B两节点同时写同一个key,此时客户端访问不同的master,就会得到不一致的数据.其次A和B还得相互通知对方最新的数据,那么怎么才能确认谁的版本才是最新的呢?为了避免这种情况,就得加锁,写数据的时候双方就得频繁通信,以此确认对方没有在修改同一份数据,这样显然会消耗大量的CPU资源.\n所以主主架构不仅没能提高性能,甚至还会带来数据不一致的问题.\n主从的使用 可以通过slaveof命令来实现主从 : slaveof \u0026lt;host\u0026gt; \u0026lt;port\u0026gt;\n比如有A,B两个节点,我们向A发送命令 : slave B 6379\n那么B就会变成A的master,A就会完全复制B的数据.\n我们对master的任何写操作都会同步到slave,在没有异常的情况下,主从保存的数据是一致的.\n主从复制 先简要概括下复制的过程:\n slave刚加入集群的时候,数据是空白的,因此要完全复制master的数据. 第一次复制完成后,slave和master的数据就暂时一致了.此后,每当master有数据变动时,为了保持一致,也得把相应的改变同步到slave,这时只需要增量复制.  Redis的复制功能在2.8版本之后改进了一些地方,旧版本的命令是sync,新版本是psync.\n这里我们先介绍旧版的,然后对比新版来体会下改进的地方.\n旧版的复制功能分为同步(sync)和命令传播(command propagate)两个阶段.就是我们上面说的两步.\n同步(全复制) 同步操作是由sync命令开启的:\n slave向master发送sync命令. master开始执行bgsave命令,生成一个rdb文件.与此同时还会使用一个缓冲区记录从现在开始执行的所有写命令(类似于aof). 当master执行完bgsave后,就会将生成的rdb文件发给slave. slave接收并载入这个rdb文件,从而将自己的数据库状态更新到与master执行bgsave之前的状态一致. 然后master继续把缓冲区里记录的写命令发送给slave,slave执行这些命令,此时slave的状态就和master完全一致了.  命令传播(增量复制) 在同步操作完成之后,master和slave就暂时达到了一致.不过master的数据会随时被修改,因此slave就得持续地与master进行同步.\n所谓命令传播,就是 : master会将自己执行的每条写命令,发送给各个slave,这样它们执行完命令后,master与slave又会再次回到一致状态.\nSYNC的缺陷 旧版本的问题在于没有考虑过由于网络抖动造成的断线重连问题.\n由于网络原因造成master与slave暂时无法通信,那么master发送给slave的命令将会丢失.\n此后,当slave重新连接master之后,由于断线期间的命令已经丢失,通过命令传播无法再次达到主从一致,所以slave只能再次执行sync,对master进行全复制才能再次达到一致.\nsync需要生成RDB,是非常消耗CPU的.而且此前slave与master已经全复制过一次,数据大部分相同,断线的时间可能并不长,丢失的数据可能并不多,为了这少量丢失的数据而再次全复制,这显然不太合理.\n新版PSYNC的实现 PSYNC分为完整重同步(full resynchronization)和部分重同步(partial resynchronization)两种模式.\n其中完整重同步与sync的同步的步骤是一样的.\n而部分重同步则是新增,用于处理断线重连的情况 : 当slave重连时,如果条件允许,master只需把掉线期间的命令发给slave即可,不需要再次全复制.\n注意,这里需要强调的是\u0026quot;条件允许\u0026quot;,并不是所有重连上的slave都可以跳过全复制.\n部分重同步的实现 先介绍三个名词:\n   名词 释义     offset 复制偏移量   backlog 复制积压缓冲区   run ID 服务器运行ID    offset master和slave都会持有一个offset,作用相当于TCP中滑动窗口的sync,用来表示复制到了哪个位置.\n假设master和slave目前的offset都是1000,然后master写入了一个33字节的数据,那么master的offset就会变成1033;然后master向slave发送这条写命令,slave的offset也会变成1033.\n如果master和slave是一致的,那么它们的offset就会相同.\n如果slave断线后重连,把offset发给master.master看见slave的offset小于自己的,就能明白slave是刚重连上来的.\nbacklog 前面我们说\u0026quot;并不是所有重连上的slave都可以跳过全复制.\u0026quot;,就是和这个缓冲区backlog有关.\nbacklog由master维护,是个固定长度的队列(默认1mb),存储的是master最近执行过的写命令及其对应的offset.\n为什么是固定长度呢?因为master会一直执行写命令,如果队列是无界的,那么最终会把redis的内存都占满了.\n如果slave掉线重连,向master发送offset,然后到backlog里一查,发现offset还在队列里,那么master就能把队列里这个offset之后的所有命令发给slave以实现部分重同步.\n但是由于队列的长度是固定的,如果slave掉线的时间太长,导致重连上来后无法在队列里找到对应的offset,那么它就只能进行完整重同步了.\nrun ID 主从服务器都拥有自己的,独特的run ID.\n当slave连接上master时(不管是首次,还是重连),需要向master发送自己之前保存的run ID(也就是上次连接的master的ID),然后master拿这个和自己的比较.\n如果相同,那么就说明slave是断线重连的,那么master就准备给它尝试部分重同步.\n如果不相同,那么就说明slave是首次连接本服务器,就需要给它发送自己的run ID,然后进行完整重同步.\n概念介绍完毕,接下来完整地看一遍psync的操作过程:\n 如果slave此前未曾复制过任何服务器,或者之前执行过slaveof no one(从slave转变为master),当它开始新一次的复制时,会向master发送psync ? -1命令,主动请求进行完整重同步. 如果slave已经复制过了某个服务器,那么它开始新一次的复制时,需要向master发送psync \u0026lt;runid\u0026gt; \u0026lt;offset\u0026gt;命令,master则根据两个参数来判断进行完整重同步还是部分重同步. 如果master返回的是+fullresync \u0026lt;runid\u0026gt; \u0026lt;offset\u0026gt;,那么就进行完整重同步. 如果master返回的是+continue,那么就进行部分重同步. 如果master返回的是-err,就代表master的redis版本低于2.8,无法识别psync命令,那么slave只能向master发送sync来进行全复制.  ","permalink":"http://euthpic.github.io/tech/redis%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/","summary":"前面我们提到的持久化,内存淘汰,管道等机制都是Redis在单个实例上的功能,但是随着系统并发量的不断升高,单个实例的redis就很难继续满足","title":"Redis主从同步原理"},{"content":"过期key的删除策略 Redis所有的键都可以设置过期时间.\n思考一个问题,所有到期的键都一定会被删除吗?\n假设同一时间内redis上有大量的key一起过期,那么这段时间redis用于删除key的时间就会太长,会严重影响到对客户端的请求处理.\n所以答案是 : 不会.Redis对于过期的key有两种删除策略,定时扫描(主动)和惰性删除(被动)\n而对于外部而言,redis过期的键是否删除是不确定的,但如果外部主动去观察(查询)的话,那么过期的键一定会被删除,是不是很像薛定谔的猫呢?\n过期的key集合 设置了过期时间的key,redis会把它们放到一个独立的字典里去,方便删除时的遍历.\n两种策略 定时扫描删除(主动) Redis默认会每秒进行十次过期扫描,考虑到效率的问题,并不会扫描过期字典里全部的key,而是采用了一种简单的贪心策略:\n 从过期字典中随机20个key. 删除这20个key中已经过期的. 如果过期的key比率超过25%,那就重复第一步. 如果本次扫描用时超过25ms,则停止扫描.  可以看到,Redis在过期扫描这里,为了不过分阻塞客户端的请求,在回收过期key的量和时间上都做了限制.有没有觉得,这里和JVM的G1收集器设计很像呢?\n不过即便如此,如果每次扫描都达到了时间上限,那么每秒就会有250ms,也就是25%的时间用于回收key,还是会有很大的性能影响.因此要注意合理设置key的过期时间,防止同时过期.\n惰性删除(被动) 在客户端访问key的时候,redis也会检查它的过期时间,如果过期了就立即删除.\n定时删除是集中,主动处理,而惰性删除是零散,被动处理.\nAOF与RDB对过期key的处理 RDB\n 创建RDB文件时,会对数据库的过期键检查,过期的key不会写入RDB文件里(保留一个疑问,还不确定检查的同时会不会一起把数据库的过期key也删了). 而载入RDB文件时,也同样会检查,过期的key会被忽略.也就是说,无论是写入还是读取RDB文件,都不会存在过期的key.  AOF\n 写入AOF文件时,由于AOF保留的是命令集,键被删除会有del命令,所以不需要额外检查key是否过期,会保留所有过期但未被删除的key. 载入aof文件和aof重写时也会像RDB一样去检查过期的key.  从库的过期策略 从库不会进行过期扫描,从库对过期key的处理是被动的,由主库统一控制.\n主库在key到期时,除了向aof里增加一条del指令,也会同步到所有的从库,从库通过执行这条del指令删除过期的key.\n内存淘汰机制 如果有大量过期的key既没有被定时扫描选中,也没有任何客户端去访问它们,那么它们一直待在内存里,会使得内存爆炸.\n为了解决这个问题,redis设计了内存淘汰机制. 可以通过配置项maxmemory来设置Redis存储数据时限制的内存大小.例如:\nmaxmemory 100mb 设置maxmemory为0代表没有内存限制。对于64位的系统这是个默认值，对于32位的系统默认内存限制为3GB。\n当达到指定的内存限制大小时,Redis有六种内存淘汰策略:\n   noeviction 返回错误（大部分的写入指令，但DEL和几个例外）     allkeys-lru 尝试回收最少使用的键（LRU）   volatile-lru 尝试回收最少使用的键（LRU），但仅限于在过期集合的键   allkeys-random 回收随机的键   volatile-random 回收随机的键，但仅限于在过期集合的键。   volatile-ttl 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键    如果没有键符合回收条件,那么volatile-xxx就和noeviction的效果一样了.而且为key设置过期时间也会消耗内存,当redis的内存有压力时,这显然不是什么好的选择.\n所以一般情况下,allkeys-lru会是更加高效的选择.\ndel指令 由于过期的key与del指令息息相关,这里也顺便提一下.\n删除指令del会直接释放对象的内存,大部分情况下,这个指令非常快,但如果删除的key是非常大的对象,那么实际操作起来可能导致单线程卡顿.\n因此,Redis在4.0后引入了unlink指令,它能对删除操作进行懒处理,丢给后台线程来异步回收内存.\n而且这并不会引发线程安全问题,一旦unlink指令发出,主线程的其它指令就无法访问这个key了.\n可以理解为要删除key的时候,主线程先标记,被标记的key此后无法再被主线程访问,而是交由后台线程去处理.\n通信协议 RESP是Redis序列化协议的简写,是文本协议.\nRedis协议在以下几点之间做出了折衷：\n 简单的实现 快速地被计算机解析 简单得可以能被人工解析  作者认为数据库系统的瓶颈一般不在于网络流量,而是数据库自身内部逻辑处理上.所以即使Redis使用了浪费流量的文本协议,依然可以取得极高的访问性.\n而且解析性能非常好,我们不需要特殊的redis客户端,仅靠telnet或者是文本流就可以跟redis进行通讯.\n客户端的命令格式：\n 简单字符串 Simple Strings, 以 \u0026ldquo;+\u0026ldquo;加号 开头 错误 Errors, 以\u0026rdquo;-\u0026ldquo;减号 开头 整数型 Integer， 以 \u0026ldquo;:\u0026rdquo; 冒号开头 多行字符串类型 Bulk Strings, 以 \u0026ldquo;$\u0026ldquo;美元符号开头 数组类型 Arrays，以 \u0026ldquo;*\u0026ldquo;星号开头  例如我们想给服务器发送一个set hello abc,使用以下代码即可实现:\nI/O多路复用 前面讲单线程模型的时候提到过, I/O多路复用是提高Redis性能的关键.这部分就来详细介绍一下.\n五种I/O模型 在最原始的I/O(BIO,阻塞IO)里,写过socket相关应用的人都知道,调用read()函数时线程是会被阻塞的.\n假如客户端开始向服务器发送指令,这期间客户端指令没发完,去做别的准备工作了,服务器没收到结束的信号,就只能傻傻等着,白白浪费CPU的资源.\n后来人们为了提高I/O的性能,又相继发明了非阻塞I/O,多路复用I/O,信号驱动I/O和异步I/O等模型.\nRedis使用的就是多路复用I/O.\n文件事件处理器 Redis里的事件分为文件事件和时间事件.\n文件事件是对套接字操作的抽象,也就是Redis服务器和客户端通信所产生的事件,比如我们常见的客户端向服务器发送get请求,或者服务器向客户端发送它请求的结果.也可以把文件事件理解为网络事件.\n而文件事件处理器就专门处理文件事件,它包括四个部分:\n 多个套接字(socket) I/O多路复用程序 文件事件分派器 事件处理器  其工作原理:\n 事件包括accept(连接应答),write(写入),read(读取),close(关闭)等,由socket产生. 由于socket一般会有多个,因此多个事件可能会并发地出现. I/O多路复用程序负责监听多个socket(事件轮询),并向文件事件分派器传送那些产生了事件的socket. I/O多路复用程序维护了一个事件队列,产生了事件的socket会进入此队列,每次只会从该队列里传输一个socket给分派器.通过此队列,I/O多路复用程序保证了到达分派器的socket是有序,同步的. 当上一个socket产生的事件被处理完毕后,I/O多路复用程序才会继续传输下一个socket,保证了事件的处理是串行的,避免了线程安全以及竞态问题. 分派器负责把产生了事件的socket分派到对应的事件处理器.  注意,由于事件是串行处理的,如果某些事件的处理时间过长, 那么就会阻塞到后面的事件,那么Redis就不快了.所以要避免执行一些像keys这样的指令.\nI/O多路复用的实现 前面说过,Redis的I/O多路复用是通过epoll实现的,这其实并不准确.\n事实上,Redis包装了常见的select,epoll,evport和kqueue这些I/O多路复用函数库来实现多路复用.\n由于操作系统各异,不是每个函数都能在所有系统上执行,所以程序会自动选择在系统中性能最高的函数库来作为底层实现.一般选择的就是epoll了.\n关于epoll在性能上领先的地方,这将在讲到网络编程时再详细展开.\n事务 Redis也有事务,不过按照传统的事务定义ACID来看,redis的事务并不具备ACID的全部特性.\n事务相关命令    命令 作用     multi 事务开启的标志,后续的一系列指令都为事务指令,配合exec使用   exec 事务提交/执行的标志,服务器在收到multi命令后,会把后续的命令先入队,但直到收到exec才会一起执行   watch 乐观锁,监视某个key是否在事务提交前被修改了   discard 在exec之前发送,丢弃本次事务.    事务的实现 Redis的事务从开始到结束通常会经历三个阶段:\n 事务开始 \u0026mdash; multi 命令入队 事务执行 \u0026mdash; exec  事务开始\nmulti命令标志着事务的开始.\n命令入队\n当客户端处于非事务状态时,它发送的命令会被服务器立即执行.\n但当客户端处于事务状态时,服务器会根据命令的不同而执行不同的操作:\n 如果命令为exec,discard,watch,multi中的一个,那么服务器立即执行该命令. 如果命令为其它命令,服务器不会立即执行,而是把命令放入一个事务队列里面,然后向客户端返回queued回复.  事务执行\n当服务器收到exec命令时,就会遍历执行这个事务队列的所有命令,最后将执行命令所得结果全部返回给客户端.\n来看一个事务的例子:\nclient \u0026gt; multi redis \u0026gt; ok client \u0026gt; set a 1234 redis \u0026gt; queued client \u0026gt; get \u0026#34;a\u0026#34; redis \u0026gt; queued client \u0026gt; exec redis \u0026gt; ok redis \u0026gt; \u0026#34;1234\u0026#34; 乐观锁WATCH watch命令是一个乐观锁,它可以在exec命令执行之前,监视数据库的任意key.\n如果在exec执行时,被watch监视的key至少有一个被修改了,那么服务器将拒绝执行事务,并向客户端返回代表事务执行失败的空回复(nil或null).一般客户端失败后都会选择重试.\n下面就是个失败的例子:\n监视机制的触发\n那么,watch是如何知道哪些键被修改的呢? \u0026mdash;改动方主动通知\n所有对数据库进行修改的命令,比如set,lpush,sadd等,在执行完后都会去查看被修改的key是否被某些客户端监视.若有,则会将监视被修改键的客户端的标志(redis_dirty_cas)打开.\n当服务器执行exec时,就会根据该客户端的标志是否打开来决定是否执行事务.\n注意 : Redis禁止在multi和exec之间执行watch指令,否则会报错.\n事务的ACID性质 前面我们提到,redis并不具备ACID的全部特性,接下来就逐一分析下.\n先上结论,根据\u0026lt;Redis设计与实现\u0026gt;的说法:\nredis的事务总能保证原子性,一致性和隔离性,且在aof持久化模式下,并且appendfsync选项的值为always时,事务也具备持久性 实际上,如果你用mysql的事务去看这段话,你会发现除了隔离性和持久性,其余的都是在扯犊子.\n原子性 事务具备原子性指的是，数据库将事务中多个操作当作一个整体来执行，服务要么执行事务中所有的操作，要不一个操作也不会执行。 实现原子性的重要前提是能否保证事务可以回滚.\n显然redis不具备回滚事务的能力,因为它是先执行,保存完数据才会去写aof日志.如果执行到后面发现有指令执行错误,前面的指令已经持久化了,根本无法回滚.\n而mysql是先写日志,边写边执行,结果暂不提交.如果写到后面发现有指令执行错误,就可以根据日志内容进行回滚.\n顺便一提,redis之所以不支持事务回滚也是为了性能考虑,者与redis追求简单高效的设计主旨不符.而且作者认为回滚的情况很少会在实际的生产环境中出现,所以也没有去设计的必要.\n那为什么书中会认为redis具有原子性呢?\n因为redis把指令的错误分为两种:\n 入队错误 : 指令语法的错误,例如不存在的指令,或者缺失必要参数的指令.在入队的时候就可以检查出来. 执行错误 : 一般是数据类型的错误,例如对list的键执行了hash的操作.入队的时候无法判断,错误只能在执行时才能暴露出来.  如果是入队错误:\n例如提交了一个不存在的\u0026quot;YAHOOOO\u0026quot;指令,那么redis在入队的时候能检查出来,然后整个事务都会被拒绝执行.\n作者认为这种情况体现了\u0026quot;原子性\u0026rdquo;.\n但如果错误到了执行阶段才暴露出来,redis无法回滚已经执行的命令,并且会继续执行后续可执行的命令.也就是到了执行阶段,所有命令都会被执行,执行失败的命令不回影响到其他命令的正常执行.\n一致性 对于事务的理解不同,一致性的定义也有两种版本.\n版本1\n如果数据库在事务开始之前是完整的,那么事务结束以后数据库的完整性也没有被破坏。 版本2\n事务使得系统从一个正确的状态到另一个正确的状态 这里的完整性具体是指,数据库是否存在会持续报错的缺陷,例如,存在两个同名的表,又或者类型是int的list却存进了几个字符串.\n书中作者采用了第一种定义,只关注数据库的状态.\n作者认为对于redis的错误操作都会在入队阶段或者执行阶段被检查出来,从而被拒绝执行;\n而当发生宕机的情况后,redis也能通过aof或者rdb文件进行还原,因此事务的一致性可以得到保证.\n但是在传统的ACID概念中,一致性不单单要保证数据库的正确状态,而且在逻辑上也是要正确的.\n来看著名的转账问题:\nA转账给B分为两步:\n A减少10块钱 B增加10块钱.  因为redis不支持回滚,那么当A的钱减少了之后,系统错误,B的钱没有变化,那么A的钱就白白减少了.这在逻辑上是不正确的.\n隔离性和持久性就不单独讨论了,没啥问题.\n从本质上来说,ACID中的AID都是手段/过程,C才是事务的最终目的.也就是说我们是通过事务的原子性,隔离性,持久性来保证一致性.\nredis无法保证原子性,自然也无法保证一致性.\n结论 说了一大堆,并不是想批判redis不遵守事务的规范,从redis的设计思想就可以看出它无意实现传统意义上的ACID,只是想提醒各位使用redis的小伙伴 : redis中的事务和传统意义上的事务还是有很大区别的,使用的时候要注意这点,不要到时候因为无法回滚事务而搞出大问题.\nPipeline pipeline也就是管道,或者说流水线,使用了这条命令后,客户端可以把一连串指令打包,然后一起发送给服务器.\n这可以提高客户端与服务器之间通信的效率.原本需要多次通信,使用管道后只需通信一次.\n使用pipeline之前 :\n使用pipeline之后:\n而且如果这些命令之间执行结果不会相互影响,那么pipeline还可以通过改变指令的执行顺序来合并相关指令,进而进一步提升效率.\n例如原先的指令顺序:\n经过优化合并后,可以视作只有一个write和一个read :\n实现 pipeline有两种实现方法:\n 用multi和exec包裹起这些命令,即使它们没有事务的需求. 像事务一样收集起这些命令,等到命令收集完毕后再一起发送给服务器.这种方法效率更高,可以合并指令.  ","permalink":"http://euthpic.github.io/tech/redis%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6%E4%B8%8Eio%E6%A8%A1%E5%9E%8B/","summary":"过期key的删除策略 Redis所有的键都可以设置过期时间. 思考一个问题,所有到期的键都一定会被删除吗? 假设同一时间内redis上有大量的ke","title":"Redis内存淘汰机制与IO模型"},{"content":"上一次我们介绍了Redis的数据类型,应用场景,单线程模型以及与Memcached的比较.今天继续介绍它的持久化.\n持久化 Redis的数据是存在内存里的,如果不采取特殊措施,一旦redis重启/退出/故障,里面的数据将全部丢失.\n即便它很快重启也只是个空的redis,这时大量的请求将会直接打到DB去.\n为了避免这种情况,Redis提供了RDB和AOF两种持久化方式.\nRDB  RDB也就是快照,持久化时是把当前内存中的数据集快照写入磁盘(名为dump.rdb的二进制文件),恢复时则把快照文件直接读到内存中.\nRDB文件的创建\n两条命令:\n SAVE会阻塞Redis服务器进程，服务器不能接收任何请求，直到RDB文件创建完毕为止。 BGSAVE创建出一个子进程，由子进程来负责创建RDB文件，服务器进程可以继续接收请求。bgsave期间服务器会拒绝任何save命令(防止竞态).  这两条命令既可以由客户端来请求执行,也可以在满足下列条件时自动触发:\n 用户设置了save配置选项,比如save 60 10000,那么从最近一次创建快照之后开始算起,当\u0026quot;60秒内有10000次写入\u0026quot;这个条件满足时,redis就会自动触发bgsave命令.如果设置了多条,满足其中一条就会触发.如果用户没有主动设置,redis也有自己的默认配置 收到shutdown(关闭服务器)或者标准term信号(linux下终止进程)后,redis会执行save命令,阻塞所有客户端,不再执行客户端发送的任何命令,并在save命令执行完毕后关闭服务器. 主从复制时,主服务器向从服务器发送一条sync命令,如果此时主服务器没在执行bgsave,也不是刚刚执行完bgsave,就会执行bgsave命令.  save命令的操作比较简单,但是它在执行期间会阻塞线上的业务,所以一般我们想手动备份的时候都是用的bgsave.要了解bgsave,必须先清楚什么是COW,什么是fork.\ncow也就是copy on write,写时复制.这是一种读写分离的设计思想.\n假设有一块内存空间,我们要对它进行读写,那么当只有读的需求时,不需要额外操作.如果有写的需求时,另外开一块新的内存空间,把需要写的那页复制到这里,然后就在新的空间进行修改,而写的同时仍旧可以读旧的空间.写完之后把指针指向新的空间,旧的空间抛弃.\n这样设计的好处是读写分离,不需要加锁来造成阻塞,写的同时也可以读,提高了读的效率.(联想到CopyOnWriteList没有?)\n而fork是linux系统的一个进程机制,应用到了cow.当父进程fork出一个子进程时,两者共享内存里面的代码段和数据段.\nRDB的具体执行过程如下:\n redis调用forks创建子进程来进行rdb操作. 子进程将数据集写入到一个临时rdb文件中,父进程继续提供读写服务.此时父进程和子进程共享数据段,可以用到copy-on-write机制. 不必担心父进程的读写会改变子进程持久化的数据,因为此时子进程已经把内存的数据固定下来了.父进程读的时候,依然如旧.父进程写的时候,子进程会把脏页(即将被写入的那页)复制一份出来让父进程去写.子进程写入硬盘的还是旧的那页. 当子进程完成对rdb文件的写入时,redis用新的rdb文件替换原来的rdb文件.而fork时产生的那些脏页也会替换原来未修改的那些页.旧页被抛弃.  需要注意的是,在bgsave这个过程中,服务器执行的写命令并没有一起同步到rdb文件中,也就是说,RDB文件在bgsave之后并不与当前的数据库状态一致.\nRDB文件的载入\nrdb文件是在服务器启动的时候自动载入的,redis没有提供任何主动载入rdb文件的命令.\n只要redis在启动的时候检测到rdb文件的存在,就会主动载入它.\n但是AOF文件的载入优先级高于RDB.只有AOF功能关闭时,redis才会用rdb文件来恢复数据.\nAOF  aof(append only file)与rdb文件存储数据集不同,aof文件存储的是redis所有执行过的命令,类似于mysql的binlog.\nAOF的实现\n分为三步:\n 命令追加 文件写入 文件同步  这里要先介绍现代操作系统的文件写入机制:\n为了提高效率,在现代操作系统中,当用户试图写入文件时,这些数据会先写入到操作系统的内存缓冲区里,这就是上面说的文件写入.\n等到内存缓冲区被填满了,或者超过一定时限后,操作系统才真正把内存缓冲区里的数据写入到硬盘的文件中去.这就是文件同步.\n服务器在执行完一个写命令后,就会把这条命令追加到redis的aof缓冲区的末尾.\n服务器每次在结束一个事件循环之前,都会根据配置项appendfsync来判断是否要把缓冲区的命令写入到aof文件.\nappendfsync有三种选项:\n always : 将aof缓冲区的数据写入并同步到aof文件中. everysec(默认) : 写入文件,如果距离上次同步aof文件的时间超过了1秒,那就再同步. no : 只写入文件,不同步.同步时间由操作系统决定.  使用always,当redis故障时数据丢失最少,但写入硬盘的操作频繁,但效率最慢.\n使用no,不仅故障时丢失数据最多,而且当缓冲区等待写入硬盘的数据填满时,redis的写操作将被阻塞,所以平均效率与always相当.\n使用everysec不仅安全,而且实际效率与不进行持久化时相差无几.推荐使用everysec.\nAOF的载入\n由于aof里包含了redis执行过的所有写命令,当服务器重启时,只需读取并执行aof文件里的所有命令即可还原数据库状态.\n但是由于redis的命令只能在客户端上下文中执行,因此这个还原的过程实际上是借助于伪客户端来进行的.\nAOF重写(bgrewriteaof)\nRDB文件和AOF文件随着服务器的运行,数据越来越多,文件的体积也会膨胀起来.RDB文件没有办法优化,但是AOF可以通过AOF重写来减少体积.\nAOF存储的是命令集,有时候对于一条数据反复设置可以产生多条命令(过程).由于还原数据库只需要最终的数据(结果),因此可以想办法把这多条命令合并到一条命令上.比如aof记录了set a 1, set a 2,set a 3三条命令,那么我们最终只需要set a 3这条等效命令.\n实际上,AOF重写并不依赖AOF文件,而是从数据库一个个读取key现在的值,然后把一条条set命令写入到新aof文件去.写完之后用新aof文件替代旧aof文件,那么重写就完成了.\nAOF后台重写\naof重写需要读取全部的key,如果只通过一个线程去操作的话,无疑会造成严重阻塞,在此期间无法继续处理客户端的请求(有没有想到keys命令?)\n所以Redis将重写放入到子进程去进行,让父进程继续处理客户端的读写请求.(和bgsave相似).\n同样地,在重写期间,如果父进程继续修改数据库的数据,那么重写后的aof文件与当前数据库的状态并不一致.\n为了解决这种数据不一致的问题,redis设置了一个aof重写缓冲区(与aof缓冲区区分开),在fork子进程之后使用.所以在重写期间,服务器进程会执行以下三个工作:\n 执行客户端发来的命令. 将执行后的写命令追加到aof缓冲区末尾. 将执行后的写命令追加到aof重写缓冲区末尾.(新增)  当子进程重写完成后,会把aof重写缓冲区里的命令(未优化过)写入到新的aof文件里,这样就可以保证新aof文件与数据库状态的一致性了.\n所以要注意的是,即便aof刚刚重写完毕,它也是还可以再压缩的.\n顺便一提,主从复制时也会用到这个机制.\nRDB与AOF对比  RDB存储数据集,每次持久化是全量存储.AOF存储命令集,每次持久化只存储增量. RDB持久化时间更长,可能丢失的数据也更多. RDB文件一般会小于AOF文件(未重写),用于还原数据库状态时的速度也会快于AOF 但是由于aof文件的更新频率更高,丢失数据的数量也更小,因此服务器启动时默认载入aof文件. 由于RDB文件紧凑性,便于复制数据到一个远端数据中心，非常适用于灾难恢复,适合冷备. AOF更新频率快,可能丢失的数据少,适合热备.  为了防止服务器突然宕机的情况,Redis在运行期间一定要开启aof进行备份.\n但是如果Redis是正常退出,它会自动生成RDB文件,这个RDB文件保存的是最新的数据集.那么重启时,可以把aof文件删掉,载入这个RDB文件以快速恢复.\nRDB和AOF混合搭配模式 在对redis进行恢复的时候，如果我们采用了RDB的方式，因为bgsave的策略，可能会导致我们丢失大量的数据。如果我们采用了AOF的模式，通过AOF操作日志重放恢复，重放AOF日志比RDB要长久很多。\nredis4.0之后，为了解决这个问题，引入了新的持久化模式，混合持久化，将rdb的文件和局部增量的AOF文件相结合，rdb可以使用相隔较长的时间保存策略，aof不需要是全量日志，只需要保存前一次rdb存储开始到这段时间增量aof日志即可，一般来说，这个日志量是非常小的。\n","permalink":"http://euthpic.github.io/tech/redis%E6%8C%81%E4%B9%85%E5%8C%96%E5%8E%9F%E7%90%86/","summary":"上一次我们介绍了Redis的数据类型,应用场景,单线程模型以及与Memcached的比较.今天继续介绍它的持久化. 持久化 Redis的数据是存","title":"Redis持久化原理"},{"content":"在分布式系统中,为了应对高并发的情况,有3种主要的手段 : 缓存,异步,分流\n今天,我们要讲的就是在缓存中被最广泛使用的中间件 : Redis.\n思维导图 Memcached vs. Redis? 为什么会有这个问题?\n在大名鼎鼎的stackoverflow上搜索\u0026rsquo;redis',可以看到排名第一,最高赞的问答就是这个\n这已经是2012年的问题了,虽然时过境迁,但这几年我在很多文章中还会看到对这个问题的讨论.\n与Memcached的比较对于我们认识redis还是大有裨益的,可以更加了解redis的特色.\n在这里就直接把一问一答都贴出来了.\n名为Sagiv Ofek的用户需要在Memcached和Redis中做出抉择,他希望了解的是:\n哪一个拥有更好的性能?Memcached和Redis彼此都有哪些优点,哪些缺点?比较的范围可以包括:\n 读写的速率 内存的使用 持久化 可拓展性  最高赞Carl Zulauf的回答(最后更新于2017年)很全面了:\n与memcached相比，Redis功能更强大，更受欢迎并且得到更好的支持. Memcached只能做Redis可以做的一小部分。即使在两者功能重叠的部分，Redis也表现得更好. 对于任何新内容，请使用Redis。  读/写速度：两者都非常快。虽然基准测试因工作负载，版本和许多其他因素而异，但通常显示redis与memcached一样快或几乎一样快。我建议使用redis，但不是因为memcached速度慢。不是。 内存使用 : Redis更好。  memcached：缓存大小由你决定，并且在插入数据时，守护程序会迅速增长到略大于该大小。除了重新启动memcached之外，从来没有真正的方法可以回收任何空间。您所有的密钥都可能过期，您可以刷新数据库，并且该数据库仍将使用您为其配置的全部RAM。 redis：设置最大大小由您决定。Redis将永远不会使用过多的内存，并会适时回收不再使用的内存。 我将100,000个〜2KB字符串（〜200MB）的随机句子存储到了两者中。Memcached RAM使用量增加到约225MB。Redis RAM使用量增加到〜228MB。刷新两次后，redis降至〜29MB，memcached保持在〜225MB。它们在存储数据方面同样有效，但是只有一个能够回收数据。   持久化：Redis显然是赢家，因为它默认情况下会这样做(自动持久化)，并且具有很多配置项可选。而如果不借助第3方工具，Memcached没有任何机制可以转储到磁盘。 可扩展性：在需要多个实例作为缓存之前，两者都为您提供了大量的空间。Redis包含的工具可帮助您超越此范围，而memcached则不会。(我补充一下,这里应该指的是Redis Replication,Redis Sentinel和Redis Cluster)  最后,他的结论是:\n毫无疑问，对于任何新项目或尚未使用memcached的现有项目，我建议使用redis而非memcached。\n以上听起来像我不喜欢memcached。相反：它是一个功能强大，简单，稳定，成熟和强化的工具。甚至在某些用例中，它比redis快一点。我喜欢memcached,我只是认为这对未来的发展没有多大意义。\n在2020年的今天看来,二者不仅从理论上,而且从实践上也已经分出了高下.如今使用memcached的公司是少之又少了.\n如果你的新项目考虑缓存,请毫不犹豫使用Redis.\n应用场景 热点数据的缓存 由于redis访问速度快,支持的数据类型比较丰富，所以redis很适合用来存储热点数据,缓解DB的压力.\n另外结合expire.我们可以设置过期时间然后再进行缓存更新操作.\n限时业务 redis中可以使用expire命令设置一个键的生存时间,到时间后redis会删除它.利用这一特性可以运用在限时的优惠活动信息、手机验证码等业务场景。\n计数器 redis由于incrby命令可以实现原子性的递增，所以可以运用于高并发的秒杀活动,分布式序列号的生成.具体业务还体现在比如限制一个手机号发多少条短信、一个接口一分钟限制多少请求、一个接口一天限制调用多少次等等。\n排行榜相关问题 榜单也属于热点数据,借助zset(sorted set)可以很轻松在redis上实现排行榜.\n以用户的openid作为的username,以用户的点赞数作为的score, 然后针对每个用户做一个hash,通过zrangebyscore就可以按照点赞数获取排行榜，然后再根据username获取用户的hash信息.\n分布式锁 先用setnx来争抢锁,抢到以后用expire给锁加个过期时间,防止忘记释放锁而导致死锁.\n由于setnx和expire两条命令的操作并非原子性,如果setnx之后执行expire之前进程挂掉,则可能造成死锁.\nredis在2.6.12版本过后提供了解决方案:\n命令:\nSET key value [EX seconds] [PX milliseconds] [NX|XX]\n例子:\nSET key value ex 10 nx\n过期时间是10,单位是秒(px则是毫秒).这样上面我们就用一条set指令原子性地替代了上面两条指令.\n延时操作 这里同样借助zset来实现,但要注意,这里的\u0026rsquo;延时'不是指redis延迟发送数据(并不是mq),而是说客户端主动查询N秒前的数据.\n拿时间戳作为score,消息内容作为key调用zadd生产消息,消费者用zrangebyscore命令获取N秒前的数据来轮询处理.\n异步队列 看到有些文章提到有这个做法,个人觉得可以是可以(只要有双端list都能做),但是这个问题最好交给mq解决.\n社交关系 set的特点是去重,因此用来表示交并集十分适合.\n例如微博上的共同关注,共同好友,公众号上\u0026rsquo;xx位朋友也在看\u0026rsquo;等.\n基本数据类型 Redis有5个基本的数据类型:string,hash,list,set,zset(sorted set).\n当然,为了在内存上\u0026rsquo;精打细算',redis在它们之下还有ziplist,skiplist等数据结构,不过这部分内容会在另一篇文章中再详细研究.\n  String\n实用最广泛的数据类型,用途包括:\n 缓存 计数器,限速器 共享session服务器也是基于该数据类型  常用命令有:\n  set/get :set name时如果name重复,那么设置的值会进行覆盖\n  setnx : 如果这个name已经存在,不会进行覆盖而是直接返回0.可用于分布式锁,返回值1表示获取锁,返回值0表示获取失败,当发生死锁时可使用del命令将锁数据删除(需要检测是否超时).\n  setex : 设置该key在缓存中的过期时间,过了这个时间后返回nil\n  mset/mget : 批量设置与获取\n  incr/incrby/decr/decrby : 数值加减,用作计数器\n    Hash\n一个hash表下可包含多个field,常用于缓存结构化数据(如果用string的话需要序列化和反序列化等额外开销),例如数据库中的整张表(一个field对应表中的一个字段).\nhset user_1 id 1 name 张三 age 16 sex 1\nhset user_2 id 2 name 李四 age 16 sex 1\n这样如果需要修改用户的某个属性值,可以单个修改,例如:\nhset user_1 age 30\n  List\n特点是有序,用途主要包括:\n 消息队列: LPUSH+BRPOP(阻塞特征) 缓存: 用户记录等各种记录,最大特点是可支持分页 栈: LPUSH+LPOP 队列: LPUSH+RPOP    Set\n无序性,主要用于去重,交集可用于表示社交上的共同关注,共同爱好\n  Zset\n适合用于存储排行榜,例如消费排行,设置执行任务的权重\n  单线程模型 Redis是单线程的,为什么它会这么快? 另一个经典的问题了.\n这可以分解成两个问题:\n 为什么redis是单线程的?  其实redis并非真正意义上的单线程服务器,之所以大家都这么认为可能是由于官方文档里的一句话:\n\u0026ldquo;Redis is, mostly, a single-threaded server from the POV of commands execution (actually modern versions of Redis use threads for different things).\u0026quot;\n\u0026ldquo;redis绝大部分的命令都是通过单线程执行,事实上现代版本的redis将多线程用于其他不同的事情上.\u0026rdquo;\nRedis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器。它的组成结构为4部分：\n  多个套接字\n  IO多路复用程序\n  文件事件分派器\n  事件处理器\n  因为文件事件分派器队列的消费是单线程的(主要就是读写内存中的数据)，所以Redis才叫单线程模型。\n至于redis为什么是单线程的,官方的FAQ(http://www.redis.cn/topics/faq.html)是 :\n因为CPU不是Redis的瓶颈。Redis的瓶颈最有可能是机器内存或者网络带宽.\n既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。\nRedis客户端对服务端的每次调用都经历了发送命令，执行命令，返回结果三个过程。\n其中执行命令阶段，由于Redis是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个队列中，然后逐个被执行。并且多个客户端发送的命令的执行顺序是不确定的。但是可以确定的是不会有两条命令被同时执行，不会产生并发问题，这就是Redis的单线程基本模型。\n如果CPU成为你的Redis瓶颈了，或者，你就是不想让服务器其他核闲置，那怎么办？ 这很简单，单机多起几个Redis进程就好了。Redis是key-value数据库，不是关系数据库，数据之间没有约束。只要客户端分清哪些key放在哪个Redis进程上就可以了。redis-cluster可以帮你做的更好。\n为什么redis这么快?  Redis有多快可以参考官方的基准测试 : https://redis.io/topics/benchmarks\n量级10W ~ 100W QPS.\n主要的原因如下:\n 基于内存,所以读写速度都很快. 基于单线程,避免了上下文切换和竞态产生的开销. 使用epoll实现了I/O多路复用,不会因等待客户端发送数据而造成阻塞.  ","permalink":"http://euthpic.github.io/tech/redis%E6%A6%82%E8%BF%B0/","summary":"在分布式系统中,为了应对高并发的情况,有3种主要的手段 : 缓存,异步,分流 今天,我们要讲的就是在缓存中被最广泛使用的中间件 : Redis. 思维导图 Memcached vs. Redis? 为","title":"Redis概述"},{"content":" group by可以走索引,但是所有group by列需要引用同一索引的属性,并且顺序正确. order by可以走索引,但是order by的字段必须包含在where子句使用的索引中. group by与order by走索引的机制是相同的.where走索引group by才能走索引,并且走的同一个索引. like可以走索引,但是模糊查询不能以%开头 in与exist: in是子查询,当子查询表较大时效率会低,子查询返回的是数据集;可以用exists替代  exists返回的是一个布尔值,当外层表较大时效率会低; not exists永远比not in效率高   exists和in都走索引,主查询和子查询都可以走索引,而且相互独立 b+索引使用不等于(!=或者\u0026lt;\u0026gt;)时无法使用索引(误 is null,is not null无法使用索引(误) 上述两条是否走索引由mysql优化器自己判断,如果数据比较均匀的话有可能索引失效 字符串不加单引号,该字段以后的索引会失效 or会使索引失效  ","permalink":"http://euthpic.github.io/tech/%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96/","summary":"group by可以走索引,但是所有group by列需要引用同一索引的属性,并且顺序正确. order by可以走索引,但是order by的字段必须包含在wher","title":"索引优化技巧"},{"content":"索引为什么能加快检索的速度 很多人回答这个问题时,喜欢用目录作比喻.\n的确,当我们查找书中的某些内容时,会先翻目录页,得到目标所在的页数后再翻到该页查看详细内容.索引也是,内节点就是我们的目录页,叶子节点就是详细内容.我们在内节点中获取叶子节点的指针,然后才去查看叶子节点的行数据.\n不过,我认为这样的解释还是没有到位的.来看两个例子.\n假设我们现在是要查的是历史书,比如说明朝那些事儿,我们想看看崇祯皇帝朱由检的故事,而我们又知道他是明代最后一个皇帝,所以直接从目录的最后往前翻,这样很快就能找到.\n但如果现在我们要在一篇诗集里找一首诗呢?比如说,云雀叫了一整天里的一首很出名的从前慢.这时你并不知道它大概会出现在书中的哪个位置,只能从头到尾地去查目录.在数据库里,这个操作叫做全表扫描.\n这两种情况有什么区别呢?历史书是按照历史事件的发生顺序,也就是时间顺序来排列的.而诗集大部分的情况下是无序的.\n所以,仅有目录是不够的,目录中的内容得按照某种顺序排列,目录才能更好地起作用.\n索引的有序性,正是索引能高效运作的重要原因.\n针对索引的有序性,还可以做进一步的思考与总结.比如,为什么一张表需要用到多个索引?主键索引和普通索引有什么区别?还有联合索引呢?为什么索引会失效?\n通过分析我们可以发现,上述的问题其实都是相通的,其关键就在于有序性.\n是否有序,按什么排序,分析了这两点,几乎等于回答了所有关于索引的问题.\n为什么一张表需要用到多个索引?因为查询时有多个维度,所以索引也得按多个维度排序.\n主键索引?普通索引?联合索引?简单,它们排序的标准不一样呗.\n为什么索引会失效?因为既有索引的排序(a,b,c)和查询的排序(a,c)不一样.\n索引是否生效?哪个索引起的作用?这类问题会有很多种情况,如果分别去记忆每种情况的现象,很容易混淆开来,因此我个人认为总结一些提纲挈领的规律/原则,会让分析变得更加简单可靠.\n叶子节点上存了哪些东西 在介绍B+树索引的结构时,我们提到过这么一句\u0026quot;内节点存的是叶子节点的指针,叶子节点存的是具体数据\u0026quot;,后来又补充道\u0026quot;主键索引(的叶子节点)存的是整行数据,普通索引(的叶子节点)仅存索引列和主键\u0026quot;.\n不过后来我发现,这个介绍还是比较含糊的.某次和别人讨论MVCC时,对方抛出了一个问题:MVCC下的B+树如何实现?追问之下才发现对方以为叶子节点里存的是多版本的数据.\n在一棵B+树上同时存储多个版本的数据,这的确是个技术活.\n它的难点在哪里呢?假设我们有两个叶子节点,第一个存的是100~200的数据,第二个存的是200~400的数据,其中有一条数据的当前版本值是100,上次更新的值是400;\n当我们想查一条400的记录,那么是应该去第一个节点还是第二个节点找呢?显然是第二个.\n但是,如果我们想查的是上个版本值为400的记录呢?如果innodb不存储每个版本数据在每个页上的分布情况,那么是无法第一时间就确定该去第一个节点找的.\n如何存储这些信息又是个麻烦的问题,因为某一个叶子节点上存的当前版本数据是连续有序的,但是历史版本是离散无序的.在内节点上可以简单用1~200表示某个叶子节点的当前版本数据存储范围,但是要表示历史版本的话,就得全部列举了(1,5,8,33\u0026hellip;.),这样内节点原先的精简性就不复存在,它实际上已经和叶子节点没有区别(甚至还是多余的),B+树退化成了B-树,并且索引的内容随着数据版本迭代成倍增长,白白消耗许多空间.\n所以,如果要在物理上实现多版本数据并存,与其把它们全部挤在一棵树上,倒不如一个版本就新建一个快照?两种方案占用的空间是一样的.\n上面所说的显然是笨方法了,前面我们提过了,历史版本的数据是通过undo log计算出来的,叶子节点里只含最新版本的数据.\n如果对叶子节点的具体结构不了解的话,结合其MVCC等内容时的确可能会有这种误解.\n来看下叶子节点的结构:\n主键索引 ","permalink":"http://euthpic.github.io/tech/mysql%E7%B4%A2%E5%BC%95%E7%AF%87/","summary":"索引为什么能加快检索的速度 很多人回答这个问题时,喜欢用目录作比喻. 的确,当我们查找书中的某些内容时,会先翻目录页,得到目标所在的页数后再翻到","title":"Mysql索引篇"},{"content":"前言 InnoDB引擎中的事务完全符合ACID的特性.也许ACID的定义很多人倒背如流,可是在我看来,不仅定义要熟悉,而且其各自的意义以及实现方法也要有所了解.\nACID中,一致性是事务的最终目的,事务中的操作,都是为了让系统从一个正确的状态变为另一个正确的状态.而原子性,持久性和隔离性则是一致性的实现手段.\n原子性的实现关键在于是否支持事务回滚.在InnoDB中,事务的回滚是由回滚日志undo log来保证的.\n持久性的实现关键在于系统崩溃前能否及时地保存已提交的事务,这个能力称为crash-safe.在InnoDB中,这一点是由redo log来保证的.\n而隔离性,我们上一篇就提到过,在InnoDB中是由锁机制来保证的.\n由于原子性和持久性都与日志有关,所以这篇重点讨论InnoDB的隔离性,讨论一下在不同隔离级别下事务的执行可能会发生哪些问题,而InnoDB又是如何解决的.\n事务的启动方式 MySQL的事务启动方式有以下几种：\n 显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行commit 或 rollback 语句，或者断开连接。  有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。\n长事务不仅占用锁资源,还会导致回滚日志无法被清理.\n因此，我会建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。(Navicat默认方式)\n但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用commit work and chain语法。\n在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。\n事务的隐式提交 以下SQL语句会产生一个隐式的提交操作,即执行完这些语句后,会有一个隐式的commit操作.\n DDL语句 : alter database,alter table,create databse\u0026hellip;. 修改mysql架构的操作 : create user,drop user, grant,rename user\u0026hellip;. 管理语句 : analyze table , cache index,check table\u0026hellip;.  很多人往往会忽视对于DDL语句的隐式操作,这有时候可能会引起一些误解.\n隔离级别 首先我们要知道，隔离得越严实，效率就会越低.因此很多时候，我们都要在二者之间寻找一个平衡点.\nSQL标准的事务隔离级别包括：\n 读未提交（read uncommitted）: 一个事务还没提交时，它做的变更就能被别的事务看到。 读提交（read committed）: 一个事务提交之后，它做的变更才会被其他事务看到。 可重复读（repeatable read）: 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 串行化（serializable ）: 串行化会在读取的每一行数据上都加锁(行锁),当其他事务想去update该行时会被阻塞。  不过在实践中,读未提交和串行化几乎不怎么使用.为了完整性,这里也会稍微讨论一下.\nMySQL的默认隔离级别是可重复读,简称是RR,所以这篇文章重点讨论的是RR级别.\n不过据我所知,阿里云RDS的默认级别是读提交,简称RC,他们觉得在大部分的业务上并没有可重复读的需求,为了提高并发性,所以默认使用RC,若有问题应由研发在代码中做好控制.\n假设在表中只有一列,其中一行的初始值为1,来看看在不同隔离级别下,图里面V1、V2、V3的返回值分别是什么:\n 读未提交 : V1=2,V2=2,V3=2 读提交 : V1=1,V2=2,V3=2 可重复读 : V1=1,V2=1,V3=2 串行化 : V1=1,V2=1,V3=2  如果不清楚结果为什么是这样,可以往下看完后再回来验证一遍.\nMVCC 上一篇我们提到过,修改数据会加上一个X锁,并且直到事务提交之后才会释放锁.可是在上述例子的RC以及RR级别下,你实验后可以发现事务A在查询V1的时候并没有被阻塞,这难道不是和锁的机制相矛盾了吗?\n其实并不是,X锁仍然存在,只不过表中某一列的某一行上\u0026quot;并非只有一个数据\u0026quot;,事务A查询的数据并不是被X锁锁住的那个数据.\n我们可能会被Nacivat等可视化客户端展示的视图所蒙骗,以为InnoDB的表结构就是行与列的二维结构.\n其实它是三维的,在行与列之外还有历史版本数据,这就是MVCC.\nMulti-Version Concurrency Control 多版本并发控制,简称MVCC,是InnoDB用于解决不可重复读,幻读的重要机制.\nInnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。\n而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。\n下图就是一个记录被连续更新后的状态.\n引入了MVCC后,在一行上有最新的数据,也有历史的数据,根据读哪个版本的数据也就区分出了快照读和当前读:\n 快照读 : 事务读到的是在事务开启之前就已经提交,或者在本事务中被修改的数据. 当前读 : 事务读到的是最新提交的数据.  在RC级别下,任何读写都是当前读(update之前也得先查到对应的数据)\n在RR级别下,只有普通查询是快照读,任何加锁操作都是当前读,例如select \u0026hellip; for update(X锁),select \u0026hellip; lock in share mode(S锁),update/insert/delete(X锁).\nMVCC的多版本数据实际上并不是并行存在的,而是根据undo log计算出来的.例如一个数据update后从V2变为V1,然后记录在undo log中.此后如果想查询V1版本的数据,就根据undo log对V2进行一个反操作即可得到V1.\n在MySQL中,有很多概念看似易懂,实际上很容易犯迷糊,比如MVCC与锁的结合.也许你会和我一样,知道了锁的基本概念后就以为全都懂了,然而下一秒就陷于\u0026quot;为啥这一行加了锁,别的事务也能访问呢\u0026quot;的困惑中.\n学习了MVCC后,对于加锁需要有这样的概念 : 锁不是锁住某一行,而是锁住某一行的最新版本数据.\n此外,也许你之前会记住一个概念,类似\u0026quot;由于InnoDB有锁,所以读操作和写操作是相互阻塞的\u0026quot;.\n有了当前读和快照读之后,这个表述就容易让人误会了.\n实际上,快照读并不会被X锁或者S锁阻塞.也就是快照读和当前读是兼容共存的.\n为什么呢?回忆下X锁以及S锁的定义,X锁阻塞的是S锁或者另一个X锁,S锁阻塞的是X锁.\n那么快照读是什么锁呢?快照读其实没有锁.所以它和X锁,S锁都没有冲突.\n假设表t中有id,c两列,有(0,0),(5,5),(10,10)等行,那么下面这个例子中,session A更改了id=5这一行后,session B使用快照读并不会被阻塞.\n    session A session B     T1 begin;    T2 update t set c=5 where id=5;    T3  select * from t where id=5;(query ok)   T4 commit;     在上面的例子中,由于session A修改了数据,session B读到的不是最新的,这还不能证明快照读与X锁相互兼容.\n再来看个例子,在这个例子中,没有数据更改,两个事务读到的都是同一版本的数据.\n    session A session B     T1 begin;    T2 select * from t where id=5 for update;    T3  select * from t where id=5;(query ok)   T4 commit;     那么,快照读与MDL写锁兼容吗?答案是它们会相互阻塞.\n    session A session B     T1 begin;    T2 select * from t where id=5;    T3  ALTER TABLE  t MODIFY COLUMN d int(13) NULL DEFAULT NULL AFTER c;(blocked)   T4 commit;     可以看到当前读阻塞了alter.\n可是如果操作顺序颠倒,你可能会看到不一样的结果.\n    session A session B     T1 begin;    T2 ALTER TABLE  t MODIFY COLUMN d int(13) NULL DEFAULT NULL AFTER c;    T3  select * from t where id=5;(query ok)   T4 commit;     可以看到快照读并没有被阻塞.如果你用这个去验证快照读与MDL写锁是否兼容,那么你就会被误导了.\n为什么会出现这个结果呢?原因就在于前面我们提到的DDL隐式提交的特点.alter完会自动提交事务,也就把MDL写锁给释放了,这样自然不会阻塞到后面的快照读.\n此外,lock table \u0026hellip; write会阻塞快照读,但是read不会.这里就不举例了.\n读未提交 在这一级别下,读取到的都是最新的数据,而且不存在锁,所以可以立即看得到其他事务所做的修改.\n这样做会有什么问题呢?\n    session A session B     T1 begin;    T2  begin;   T3  update t set c=10 where id =5;   T4 select * from t where id =5;    T5 \u0026hellip; rollback;   T6 \u0026hellip;    T7 commit;     T4时session A拿到了session B修改了但未提交的数据,结果T5时session B回滚了,这样session A就很尴尬了.\n这个问题叫做脏读.\n读未提交的特点:\n 无锁 没有MVCC 读到的都是最新数据.  读提交 读提交与读未提交的区别,就在于引入了锁.在这一级别下,事务更新数据时会加上X锁,并且在事务提交时才释放.\n而且在这一级别下是当前读模式,所以,别的事务试图读取未提交的记录会被阻塞,相当于只能读到已提交的数据,所以解决了脏读问题.\n可是它也仍然存在问题.\n    session A session B     T1 begin;    T2 select * from t where id =5;(c=5) begin;   T3  update t set c=10 where id =5;(c=10)   T4  commit;   T5 select * from t where id =5;(c=10)    T6 commit;    T7      可以看到T2和T6时刻,session A两次相同的查询却得到不一样的结果.这个问题叫做不可重复读.\n不过不可重复读也不是非要解决不可的问题,因为读到的数据都是已经提交了的,像我们前面所说的,阿里云RDS就不把这个当问题.\n我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。\n假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。\n这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。\n读提交的特点:\n 所有查询都是当前读. 没有MVCC. 引入了锁,试图读取未提交的数据时会被阻塞,因此只能读取已提交的数据.  可重复读 可重复读与读提交的区别,在于它引入了MVCC多版本控制以及间隙锁,读数据被分为快照读以及当前读两种模式.\n来看个例子吧.下面是一个只有两行的表的初始化语句。\nmysql\u0026gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 这里，我们需要注意的是事务的启动时机。\nbegin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。\n事务B先启动事务.\n事务C启动比B晚,不过更新数据在B之前.\n事务B在更新了行之后查询;\n事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。\n这时，如果我告诉你事务B查到的k的值是3，而事务A查到的k的值是1，你是不是感觉有点晕呢？\n这是因为事务A是快照读模式,由于事务B和事务C都是在它启动之后才提交了,所以它看不见B和C的更新.\n事务B使用了update当前读,所以读到了事务C提交的修改,所以它实际上的操作是把2变成3,由于这个操作是本事务内进行的,所以它后来的select可以看得到这个结果.\n数据可见性 关于数据的可见性,在mysql45讲中使用了\u0026quot;快照\u0026quot;来进行比喻.\n 在RC级别下,快照是在sql语句执行的时候生成.事务只能看到sql语句执行前提交的数据. 在RR级别下,快照是在事务开启的时候生成.事务只能看到事务开启前提交的数据.  这个快照,InnoDB称其为一致性视图(consistent read view),但它并不是像Redis里面的快照一样,将这一时刻的数据拷贝一份出来,因为如果数据量太大,这么做就太慢了.\n一致性视图在实现上的算法和数据版本的row trx_id有关,不过人肉用这个算法去判断可见性很不直观,至少我是这么认为的,本来觉得已经理解了可见性,但是接触这个算法之后又晕了会.\n在面试时如果需要判断数据的可见性,由于题目中都会像我们的例子那样可以看出sql语句的执行顺序,所以就可以无视row trx_id直接判断了.\n一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：\n 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。  幻读 除了不可重复读之外,InnoDB的RR级别还解决了另一个重要的问题 : 幻读.\n在sql标准里面,其实RR级别并不需要解决幻读,这本该由串行化解决,所以InnoDB的RR级别实际上达到了sql标准里串行化的标准.怎么样,可以看出InnoDB多厉害了吧.\n幻读的意思是:\n第一次查询时本来没有的数据,第二次查询时却冒了出来,感觉像看到\u0026quot;幻象\u0026quot;一样. 不可重复读侧重点在于同一行数据在两次查询时的表现不同.\n而幻读的侧重点在于两次查询得到不同的数据量.\n所以大部分人会把不可重复读归因到update,而幻读则归因到insert/delete.这不太准确.\n在mysql技术内幕中,甚至还把不可重复读直接称作\u0026quot;幻象问题\u0026quot;.\n来看一个例子.我们还是假设表中只有id和c两列,数据行有(0,0),(5,5),(10,10),(15,15)\n    session A session B     T1 begin;    T2  begin;   T3 select * from t where id \u0026gt;=5 and id \u0026lt;=10    T4  update t set id=7 where id =15;   T5  commit;   T6 select * from t where id \u0026gt;=5 and id \u0026lt;=10    T7 commit;     在T3时刻,session A查到的数据有(5,5),(10,10)两行,但是在T6时刻查询的数据就多出了(7,15)这一行.\n这就是一个典型的由update导致的幻读问题.\n所谓的insert或者delete是针对全表的数据而言的,而我们的select可能只是表中的一个范围,所以update可以把select范围内的一条数据变得不符合where的筛选条件,也可以把select范围外的一条数据变得符合where的筛选条件.用绝对的insert或者delete去定义幻读是不妥的.\n这里绝不是教大家去做一个杠精,去杠什么是幻读,什么不是幻读.\n事实上,定义是为了我们更好地描述一样事物,所谓名可名,非常名,同一样事物,可能你和我对它的叫法不一样,但只要双方理解讨论的具体是什么东西就行了.讨论幻读时,我们只要清楚讨论的是哪些情况,有哪些注意问题,又有哪些解决方案就足矣.\nInnoDB是通过MVCC+间隙锁解决幻读的.被间隙锁🔒住的区间无法插入数据.\n间隙锁之间是相互兼容的,因为它们共同的目的在于阻止插入.\n可重复读的特点:\n 在锁的基础上还引入了MVCC和间隙锁. 查询数据可分为快照读和当前读. 只能读到事务开启前就已经提交的数据.  串行化 串行化的目的在于使得两个冲突的事务串行执行,安全性最高,效率也最低.\n在实现上,串行化下没有快照读,它的每个普通select语句后面都会自动加上lock in share mode,使之持有S锁,阻塞其他访问相同资源的事务.\n这里顺便说下,MySQL的分布式事务方案使用的是XA事务,也就是二阶段提交,对于隔离性的要求比较高,所以使用的是串行化级别.\n不过在微服务概述那篇文章里面有提到,分布式事务一般不会使用mysql,因为这样太慢了.\n","permalink":"http://euthpic.github.io/tech/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/","summary":"前言 InnoDB引擎中的事务完全符合ACID的特性.也许ACID的定义很多人倒背如流,可是在我看来,不仅定义要熟悉,而且其各自的意义以及实现","title":"MySQL事务篇"},{"content":"思维导图 前言 我们都知道,事务有ACID四大特性.前面讲Redis时提到过,Redis的隔离性是由单线程模型来实现的,每条命令都是串行化执行,保证彼此不相互干扰.\nRedis运行在内存里,内存速度快,所以它可以这么玩,可是MySQL不行,硬盘的速度可比内存慢太多了.所以MySQL使用锁来实现隔离性.有了锁,一方面不仅能最大程度地利用数据库的并发访问,另外一方面还能确保每个用户能以一致的方式读取和修改数据.\n事务与锁是MySQL中的重要内容,两者密切联系,互为因果.事务的实现离不开锁,锁的目的是保证事务.今天就来谈一谈MySQL的锁,同时也为下一篇事务专题做准备.\n全局锁 根据锁的粒度不同,MySQL可以分为全局锁,表级锁和行级锁.\n全局锁是粒度最粗的锁,顾名思义,就是对整个数据库实例加锁.\nMySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL).使用了这个命令后,整个库都处于只读状态, 任何试图修改数据库的命令都会被阻止.\n全局锁的典型使用场景是，做全库逻辑备份.也就是把整库每个表都select出来存成文本.这听上去很危险,如果是主库,那么业务基本就停摆了;如果是从库,那么主从就暂时无法同步.\n不过这基本是运维专属的活了,我们研发知道它的存在就行.\n表级锁 MySQL的表级锁分为两类:\n lock tables … read/write MDL(metadata lock)元数据锁  lock tables … read,锁定整张表,只允许读,就连拿到这个锁的线程在unlock tables之前都只能读,不能写.\nlock tables … write,锁定整张表,不允许其它线程访问.\nMDL  MDL分为MDL读锁和MDL写锁. MDL读锁之间不会相互阻塞. MDL写锁与写锁,写锁与读锁之间相互阻塞. MDL不需要显式使用,在访问一个表时mysql会自动帮我们加锁. DML -\u0026gt; MDL读锁. 对一个表进行增删查改等DML操作时,mysql自动帮我们加上MDL读锁. DDL -\u0026gt; MDL写锁. 对一个表进行结构性更改,例如增加字段等DDL操作时,mysql自动帮我们加上MDL写锁.  可以看到,MDL锁的设计是为了隔离开DML和DDL操作,但平时我们讨论mysql的锁时,注意力都在CURD的情景中,很容易就忽略了MDL元数据锁的存在.\n忽视MDL也可能导致严重后果.来看一张图\nsession A和B拿到了MDL读锁,正常运行,而C拿到了写锁,因此被阻塞了.\nC自己被阻塞了还不要紧,但它的加入会阻塞了原本不会阻塞的session D以及其它想读改表的事务\u0026ndash;这就是锁表了.\n如果这张表存的都是读的热点数据,那么由于这个阻塞可能都导致这个库的线程爆满.\n如果这个C是个长事务,事务不提交，就会一直占着MDL锁.\n所以在进行DDL的情况下,一定要注意MDL锁的存在.\n 尽量避免在长事务中使用. 如果一时半会也拿不到MDL锁,就考虑先kill掉这条命令,等表空闲的时候再来操作  除了以上两种,MySQL名义上的表级锁还包括两种意向锁IS和IX,这个放到下面与X/S锁一起介绍.\n行级锁 行锁是InnoDB引擎的特点.也许你曾被问到InnoDB和MyISAM之间有何不同,在这里你就会明白锁粒度的不同(是否支持行锁)是它们最鲜明的差别.\nInnoDB的行级锁包括:\n S锁 : 共享锁,也叫读锁.语法是SELECT … LOCK IN SHARE MODE. X锁 : 排他锁,也叫写锁.语法是SELECT … FOR UPDATE. 间隙锁 : Gap Lock,与S/X锁不同,锁定的是一个范围,开区间. Next-Key Lock : 行锁(X/S锁)与间隙锁的组合,左开右闭.  S锁 一般我们讲行锁,指的都是X锁或者S锁,它们锁定的是具体的某一行.\nS锁的作用是锁定数据,只允许读,不允许修改.\nmysql\u0026gt; select c from t where c =5 lock in share mode; 输入了上述的语句后,假如id=5的行存在,那么就会在这一行上加上S锁.\n为什么说假如呢,因为如果没有符合条件的行,那么innodb加的不是S锁,而是间隙锁.\ninnodb加锁的逻辑是有点复杂的,这篇文章不会牵涉那么深,在介绍完事务和和索引之后,会专门去分析加锁的逻辑.\nX锁 X锁的作用是锁定数据,只允许自己读写,不允许其他事务访问.\n当执行update,insert,delete或者select \u0026hellip; for update时,就会自动获取X锁.\nS锁之间兼容,X锁阻塞别的X锁或者S锁.\n间隙锁 间隙锁和Next-Key Lock锁定的都是一个范围,主要作用是锁定行与行之间的间隙,防止插入.\n间隙锁是个开区间,Next-Key Lock左开右闭.它们可以是共享锁,也可以是排他锁.\n也许你会问,怎么手动加上间隙锁啊?\n其实,innodb在行上的加锁单位是Next-Key Lock,不过根据情况的不同,Next-Key Lock会退化成间隙锁,X锁或者S锁.\n上面我们说select \u0026hellip; lock in share mode加的是S锁,其实不详细.一开始加的是Next-Key Lock,在上述这个等值查询的例子中,刚好存在这么一条数据,所以它退化成了S锁.\n假设表中只有id=5和id=10两条数据.\nmysql\u0026gt; select * from t where id =7 for update; 上述sql的目的是想选择id=7的数据,可是没有这么一条数据,那么mysql就会把这个区间锁🔒住.\n对于间隙锁,锁住的就是(5,10);对于Next-Key Lock,锁住的就是(5,10].它们的区别就在于右边界,间隙锁不会锁住id=10这一行,但是Next-Key Lock会.\n不过,间隙锁并不是静态的,它会动态生长.在上述例子中,假如我们把id=10这一行删掉,那么间隙锁将从(5,10)变为(5,+∞]. +∞是个开区间.\n你可以验证一下,在删掉id=10这一行后,试试能不能插入id=10或者更大的数据.\n也许这里你还会有个疑问,为什么id=7没有对应的行也要上锁呢?而且还是把整个区间都锁上了.\n这就涉及到不可重复读和幻读的问题了,得等到了事务篇再详细解释.\n这里先记住,间隙锁最大的意义在于解决幻读的问题.\n意向锁 InnoDB支持两种意向锁:\n 意向共享锁 : IS Lock.事务想要获得一个表中某几行的共享锁. 意向排他锁 : IX Lock.事务想要获得一个表中某几行的排他锁.  关于它们有以下协议:\n 在事务能够获取表中的行上的共享锁之前，它必须首先获取表上的IS锁或更强的锁。 在事务能够获取表中的行上的独占锁之前，它必须首先获取表上的IX锁。  你可以这么理解,获得了X锁,也一定同时持有IX锁;获得了S锁,也一定同时持有了IS锁.\nIS锁和IX锁彼此兼容,而且和X/S锁也兼容,这体现了innodb支持不同粒度的锁共存的特性.\n意向锁属于表级别的锁,但是实际上它们仅表意向,并没有阻塞功能.\n也许你会有疑问,怎么会有不阻塞的锁?那么意向锁有什么作用呢?\n根据《MySQL技术内幕:InnoDB引擎》的说法,意向锁只会阻塞全表扫描的请求.\n那么我们就可以这样理解 : 意向锁的目的其实是加快能否扫全表时的判断效率.\n全表扫描时,需要获得表中所有的S锁或者X锁,只要有一行获取失败,那么就无法进行扫描.\n既然如此,与其试图获取全部全部的S锁或者X锁(一行一行扫描,不累吗),不如直接通过意向锁来进行判断.\n如果你想读全表,本来你得一行行判断没有X锁,现在只用判断有没有IX锁即可.\n如果你想写全表,本来得一行行判断有没有S或者X锁,现在只用判断有没有IX或者IS锁即可.\n与其说它们是锁,倒不如说是个信号.\n这样设计的好处不仅在于加快效率,而且也可以减少阻塞甚至是死锁的情况.假如我们全表扫之前不先做判断拿表锁,而是进去一行一行的获取行锁,如果我们在最后一行被阻塞了,那么我们前面获取的锁就暂时不会得到释放,同样会阻塞其他访问该表的请求,更糟糕的是如果拿到最后一行锁的事务又要拿前面的行锁,这样就会造成死锁了.\n二阶段锁协议 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议.\n两段锁协议是指每个事务的执行可以分为两个阶段：生长阶段（加锁阶段）和衰退阶段（解锁阶段）。\n 加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得S锁，在进行写操作之前要申请并获得X锁。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。  MySQL的两段封锁法是这样实现的：\n 事务开始后就处于加锁阶段，一直到执行ROLLBACK和COMMIT之前都是加锁阶段。 ROLLBACK和COMMIT使事务进入解锁阶段，即在ROLLBACK和COMMIT模块中DBMS释放所有封锁。  锁升级 锁升级是指将当前锁的粒度降低,也就是粒行锁变表锁,表锁变全局锁.\n在某些数据库中,为了降低锁的开销,会在满足条件后,将多个锁合并升级到一个更粗粒度的锁.\n可是在InnoDB中,1个锁的开销和1000000个锁是一样的,都没有开销.\n所以InnoDB没有锁升级.\n","permalink":"http://euthpic.github.io/tech/mysql%E9%94%81%E7%AF%87/","summary":"思维导图 前言 我们都知道,事务有ACID四大特性.前面讲Redis时提到过,Redis的隔离性是由单线程模型来实现的,每条命令都是串行化执行,","title":"MySQL锁篇"},{"content":"前言 MySQL的主要索引是B+树,为了之后能更好地专注MySQL本身的内容,所以把底层存储结构B+树的内容抽出来先单独分析.\n为什么MySQL选择了B+树而不是B-树? 这是MySQL面试很常见的问题,想回答清楚就得对两种树的原理有一定了解.\n这里的B-树,好像会有部分人念成\u0026quot;B减树\u0026quot;.emm\u0026hellip;其实B-Tree中间是连字符(hyphen)，不是减号(minus).每个程序员心中都该有点B树.\n好了,这里先上结论.\n B+树的平均高度更低,平均磁盘读写次数更少,而读磁盘,尤其是机械硬盘,是很消耗时间的. B+树的数据都在叶子节点上,在需要全盘扫描的时候,B+树的遍历更加方便,只需遍历叶子节点. B+树的叶子节点和相邻的节点使用链表相连,在需要范围查询的时候,B+树更为方便.  最主要的原因是第一点,其余两点是B+树的锦上添花.\n接下来就具体分析,看看B+树是怎么实现这些优势的.\nB-树 B-树本质上是一种多路搜索树.假设树的高度是M,那么一棵M阶的B-树有以下特点:\n 根节点至少有两个子节点 每个节点最多有M-1个key，并且以升序排列 位于M-1和M key的子节点的值位于M-1 和M key对应的Value之间 其它节点至少有M/2个子节点  不负责任地声明一下,由于我并不打算亲自实现B树,所以没有验证过上面这些数字是否准确(关乎到树的平衡).不过本文用于理解B树和B+树的大概原理是足够的了.\n可以把B树每个节点看成(key,data)的结构,key就是该节点的索引,data就是该节点存储的数据指针.\n平衡过程\nB+树 B+树是B树的一种变形树,其与B树的主要差异在于:\n 内节点只存key,不存data,数据只存在于叶子节点. 每个叶子节点会通过链表与相邻的叶子节点相连.  平衡过程\nB-树和B+树的区别 在MySQL中(innodb引擎),数据文件就是索引文件,以数据页为单位组织起来的,也就是说,MySQL的数据全部存在一棵B+树上.\nB+树的每个节点就是一个数据页,而这些数据页存储的介质是磁盘.\n读磁盘是很慢的操作,因此提高MySQL数据存取性能的关键点就在于减少磁盘的读写次数.也就是减少B+树节点的访问次数.\nB+树的优点  磁盘读写次数更少 : 在B+树里面,由于内节点不存储数据,因此每个内节点中能存放更多的key,在相同的数据量下,内节点数量更少,树的高度更低,磁盘的读写时间开销更低. 遍历更加方便 : 由于B+树的数据都存储在叶子结点中,内结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 范围查找更加方便 : B+树的叶子节点通过链表与相邻的叶子节点相连,更适合于范围查找,而在数据库中基于范围的查询是非常频繁的.  B-树也不是完全没有优点,如果存储的数据就在根节点或者比较靠近根节点,那么查询的性能可能就接近O(1),而B+树每次都得访问到叶子节点才能找到目标.\n不过这毕竟是少数情况,在数据库的应用中,B+树的性能还是远远优于B-树的,\n","permalink":"http://euthpic.github.io/tech/b+%E6%A0%91-vs.-b-%E6%A0%91/","summary":"前言 MySQL的主要索引是B+树,为了之后能更好地专注MySQL本身的内容,所以把底层存储结构B+树的内容抽出来先单独分析. 为什么MySQL","title":"B+树 vs. B-树"},{"content":"思维导图 前言 整理了MySQL的笔记后,我做了上面这张图.不过看了林晓斌老师的专栏后,我在这篇文章里并不打算直接陷入这些细节中了,而是从各个常用sql出发,拆分一下它们执行的流程.专栏里介绍了select和update的过程,我这里再稍微补充下order by和group by的原理.\nSELECT是如何执行的 select查询是MySQL里最常使用的操作了,我们常会输入下面的语句:\nmysql\u0026gt; select * from T where ID=10； 然后,mysql就会返回T表中ID为10的结果.下面来拆解一下其中执行的过程.\nMySQL的逻辑架构 上面这张图,可以很直观地看见一条sql从被提交后都经过哪些组件做了哪些处理.\n大体来说，MySQL可以分为Server层和存储引擎层两部分。\nServer层包括:\n 连接器 查询缓存 分析器 优化器 执行器  涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎.\nInnoDB支持事务,MyISAM不支持.你可以这么理解,业务需要并发的,选择InnoDB,不需要并发的,选择MyISAM.而现在做Web业务,并发是少不了的,所以现在工业界基本都只用InnoDB.在我的mysql文章里,默认只讨论InnoDB.\n连接器 当我们往mysql服务器发送一条sql时,需要先通过连接器建立连接.\n连接器的作用包括管理连接和身份认证.\n当身份认证通过后,这条连接会一直保持着\u0026quot;验证通过\u0026quot;的状态,直到连接断开,即便中途修改了用户密码.\n客户端如果太长时间没动静，连接器就会自动将它断开.这个时间是由参数wait_timeout控制的，默认值是8小时.\n查询缓存 连接建立完成后,就可以执行select语句了.\n在真正执行之前,mysql还会检查一下这条语句有没有对应的缓存,若有就直接把缓存的结果返回.\n上一篇文章提过,mysql的数据是存在硬盘里的,对于近期大量执行的一些select语句,mysql肯定不想每次都去读磁盘,所以它把它们都存了起来复用.\n但是呢,在实践中才发现查询缓存失效非常频繁,只要某个表更新了一点东西,这个表上所有查询缓存都会被清空.\n费了好大劲才存起来的结果,然后没等到使用就被清除了.所以MySQL在8.0以后直接把查询缓存给删了.\n不过即便在以前的版本,除了不太会更新的配置表,都不建议使用查询缓存.\n分析器 查询缓存结束了,就开始执行sql语句了吗?\n不是的,在此之前mysql还需要通过分析器检查一下你的sql有没有毛病.\n主要的工作是:\n 词法分析 : 你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。比如要把\u0026quot;select\u0026quot;这个关键字识别出来,这是一个查询语句.把\u0026quot;T\u0026quot;识别成表名T,把\u0026quot;ID\u0026quot;识别成列ID. 语法分析 : 根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。比如select有没有拼成selct  优化器 分析器处理完之后,该开始执行了吧?\n很抱歉,还是不行.即便你的语句没有问题,为了提升执行的效率,还得经过优化器的优化.\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。\n也许你接触过执行计划explain这个概念,没有也不要紧,之后我们也会专门讲到.工欲善其事,必先利其器,在真正开始工作之前,研究制定出一个高效的执行计划,这就是优化器的任务.\n执行器 上述的步骤都是在MySQL Server里面进行的,而MySQL的数据保存在存储引擎中,所以执行器得使用存储引擎提供的接口来获得我们的数据.\n 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。  至此，这个语句就执行完成了。\nUPDATE是如何执行的 update同样需要经过以上步骤,不过在执行时有所不同.\nselect时有缓存,其实update也有.试想,如果某一时刻mysql接到大量的写请求,对于每个请求mysql都得去写一次磁盘,那用于磁盘I/O的时间就会严重影响mysql的性能,导致qps上不去.\nmysql的策略是先写日志(内存缓存中),等到不忙的时候再去写磁盘.\n这个技术叫做WAL,Write-Ahead Logging.和Redis刚好是反过来的,redis先保存数据,再写aof.\n这里的日志指的就是redo log,重做日志.有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\nInnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么redo log总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。\nwrite_pos是记录点,每写一份数据,它就往后挪一挪.\ncheck_point是擦除点,每把一份数据写回磁盘,它就往后挪一挪.\n可以这么理解,write_pos在前面写(记录数据),check_point在后面追(把这些数据写回磁盘).当check_point追上write_pos,就说明全部数据都写回磁盘了.当write_pos追上check_point,说明缓存区写满了,暂时无法处理新的写请求.\n除了redo log,更新数据时也需要写binlog.它们的不同在于:\n redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。 redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。  所以redo log用于数据库宕机时保证持久性,而binlog用于还原数据版本.可以这么理解,redo log是主,binlog是备.\n对redo log和binlog的操作需要是原子性的.\n 如果一个事务先写redo log,写完以后数据库重启binlog丢失了,那么以后利用binlog恢复的话就会丢失这个事务. 如果一个事务先写binlog,写完以后数据库重启redo log丢失了,那么重启后redo log写回磁盘的数据就丢失了这个事务,可是以后利用binlog恢复的话这个事务又会回来.  mysql使用二阶段提交来保证redo log和binlog的操作是原子性的,要么全部保存,要么全部丢失.\n以下是它们的二阶段提交流程:\nORDER BY是如何执行的 在select 阶段,如果有where条件,那么使用索引的逻辑和没有order by是相同的.\n如果没有where条件,order by的字段被索引覆盖,且select的字段刚好都在这个索引上(不需要回表的情况),那么就会使用这个索引.否则就全表扫描.\n在select完之后,如果数据的排序已经满足要求了,就不需要排序.否则就要.\nmysql会在内存中开辟一块空间用于排序,这个叫内排序.\n如果数据量太大,缓冲区放不下,就得利用硬盘来协助排序了,这个叫外排序.\nGROUP BY是如何执行的 在select阶段,如果有where条件,那么where能走索引就肯定走索引,不过还会判断group by能不能走索引,如果group的字段也能被索引覆盖,那么就会使用联合索引,先取出where的索引列,再到group by的索引中对where的索引列进行分组计算,最后取数据.\n如果没有where条件,就直接判断group by能不能走索引,能的话就利用对应的索引进行分组.\n不过这里要强调,group by的字段必须完全包含在索引中,而不仅仅只是满足最左前缀原则.假设索引列为(x,y)那么仅当group by的字段为(x)或者(x,y)时才能使用索引,(x,y,z)这样多出了z这列的情况会使索引失效.\ngroup by的索引逻辑和回表与否没有关系,所以select的字段随意.\n如果无法利用索引,mysql就得分配一个临时表(temporary)用于分组\n此外group by还涉及松散索引和紧凑索引等概念,不过这些会留到索引优化的文章里再详细展开了.\n","permalink":"http://euthpic.github.io/tech/sql%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","summary":"思维导图 前言 整理了MySQL的笔记后,我做了上面这张图.不过看了林晓斌老师的专栏后,我在这篇文章里并不打算直接陷入这些细节中了,而是从各个常","title":"SQL语句是如何执行的"},{"content":"概念: 队列同步器AQS是AbstractQueuedSynchronizer的简称,是JUC的核心类.AQS使用了一个int类型的变量表示同步状态,通过内置的FIFO队列来完成线程获取资源的排队工作. AQS是实现锁的关键.简单来说,它为并发包的各个组件加锁解锁提供了底层支持.  AQS原理面试题的核心回答要点  state状态的维护 CLH队列 ConditionObject通知 模板方法设计模式 独占与共享模式 自定义同步器 AQS全家桶,如ReentrantLock等   AQS的类图结构 AQS主要维护的数据结构包括一个state状态量以及一个FIFO(先进先出)队列CLH(即Craig, Landin, and Hagersten).CLH队列里的元素是Node节点,里面封装了线程.\nstate源码设计的回答要点:\n state用volatile修饰,保证多线程中的可见性 对state的get,set方法都用final修饰,限制子类重写它们 compareAndSetState()方法调用UnSafe类的CAS方法,也是采用final修饰. state\u0026gt;0表示已经获取了锁,state=0表示无锁  CLH队列要点:\n 先进先出的双向队列,队列的元素是Node节点,当一个线程获取锁失败时,同步器会将该线程以及等待状态等信息构造成一个Node并放入CLH队列中.Node节点也可以理解为正在排队获取锁的线程. 首节点是成功获取锁的Node,获取锁失败的线程会进入队列成为尾节点. Node入队需要保证线程安全,因此设置尾节点时使用了CAS操作,需要比较当前线程\u0026quot;认为\u0026quot;的尾节点与当前节点.如果此处由于并发问题导致CAS操作失败,则调用enq()方法,自旋加上CAS保证一定入队成功. 队列中由于只有首节点才会出队,无竞争因素,因此出队不需考虑线程安全. 首节点出队后,会将state设置为0解锁. 节点进入同步队列之后就进入了一个自旋的过程,判断其前驱节点是否首节点,若是则尝试获取锁.这样就保证了只有头节点才能获取到锁,保证了FIFO的规则.  共享模式的实现:\n 当节点获取锁之后,不会改变state,只会唤醒后继节点然后出队,每个被唤醒的节点在被唤醒的同时也会唤醒下一个节点,因此让可能成功获取锁的节点都来尝试获取锁,以实现共享状态的\u0026quot;向后传播\u0026quot;  ConditionObject队列与CLH队列\n ConditionObject的作用是让线程进入等待通知状态,在不满足某条件时挂起线程,直到被另一线程唤醒. 为单向队列 当CLH的头节点的线程调用了await()方法后会出队,释放锁并加入conditionObject等待队列中. 当某线程调用了该conditionObject的signal()方法后,等待队列的firstWaiter(头节点)会被唤醒进入CLH队列.   CountDownLatch 该类使一个线程等待其余多个线程各自执行完毕后再执行.\nA线程调用await()方法进入阻塞态,计数为N,此后当其他线程共计调用了N次countDown()方法后才唤醒A线程.\nCountDownLatch属于共享锁,维护了一个AQS的子类Sync,创建一个CountDownLatch对象时，所传入的整数n就会赋值给state属性，当countDown()方法调用时，该线程就会尝试对state减一，而调用await()方法时，当前线程就会判断state属性是否为0，如果为0，则继续往下执行，如果不为0，则使当前线程进入等待状态，直到某个线程将state属性置为0，其就会唤醒在await()方法中等待的线程\nCountDownLatch和CyclicBarrier区别:\n countDownLatch是一个计数器，线程完成一个记录一个，计数器递减，只能只用一次,不能reset.强调一等多.(班长收作业,等大家都交完了才能交给老师) CyclicBarrier的计数器更像一个阀门，需要所有线程都到达，然后继续执行，计数器递增，提供reset功能，可以多次使用.强调多等一(等待最后一个) (田径比赛,所有选手都到达终点之后才能进行颁奖)   Semaphore 用来控制可同时访问临界区的线程数.调用tryAcquire()获取信号量,release()方法释放资源.\n共享锁\n CyclicBarrier 其主要逻辑：若有线程未到达栅栏位置，到达栅栏位置的线程一直等待状态，直至发生以下场景： ①. 所有线程都到达栅栏位置 ②. 有线程被中断 ③. 线程等待超时 ④. 有线程调用reset()方法，断开当前栅栏，将栅栏重置为初始状态\n在线程池中使用CyclicBarrier时一定要注意线程的数量要多于CyclicBarrier实例中设置的阻塞线程的数量就会发生死锁。 调用await()方法的次数一定要等于屏障中设置的阻塞线程的数量，否则也会死锁。\n ReentrantLock   默认非公平锁,效率会比公平锁高很多\n  可重入锁,外层方法获取锁不会阻塞内层方法(同一个线程可以重复获取锁)\n  公平锁加锁的过程:\n 首先判断AQS中的state是否为0,为0表示锁是闲置的. 然后调用hasQueuedPredecessors()方法判断CLH队列中是否有其他线程,若有则不会尝试获取锁(非公平锁没有此步,),若无则用CAS将state置1来获取锁,并将当前线程设置为获取锁的独占线程setExclusiveOwnerThread(current). 如果state不为0,则判断当前线程是否为独占线程if (current == getExclusiveOwnerThread()),若是则获取锁(可重入性) 若state既不为0,当前线程也非独占线程,则无法获取锁,依次调用addWaiter()加入CLH队列中,acquireQueued()方法来自旋获取锁.  节点进入队列后的操作:\n 死循环进行以下两步操作 首先根据node.predecessor()来判断上一个节点是否头节点,若是则尝试获取锁. 若上一个节点并非头节点或者获取锁失败,则调用shouldParkAfterFailedAcquire(),根据上个节点的waitStatus来判断是否需要挂起当前线程. 如果等待过程中出现了异常,并且还没有获取锁,则会取消这次获取锁的请求.  解锁过程:\n 首先判断当前线程是否独占线程,若不是则抛出异常. 然后重置独占线程为null 然后state-1后若为0则表示锁可以释放了,唤醒被挂起的线程.若state不为0则说明当前线程仍未完全释放锁,不唤醒其他线程而是直接返回   FutureTask 主要功能:\n 异步执行任务,可以通过get()获取任务的结果 可以开始,取消以及查看任务是否完成 如果任务没有执行完,get方法会导致调用的线程阻塞 可以作为Runnable被线程执行,也可以作为Callable获取任务结果. 一旦一个执行任务已经完成就不能再次开始和结束(除非执行时调用runAndReset( ) 方法) 配合线程池的submit()使用,该方法的传参可以是FutureTask类型或者Runnable类型,不过Runnable类型会被适配器转换为FutureTask   BlockingQueue ","permalink":"http://euthpic.github.io/tech/%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8aqs/","summary":"概念: 队列同步器AQS是AbstractQueuedSynchronizer的简称,是JUC的核心类.AQS使用了一个int类型的变量表示同","title":"队列同步器AQS"},{"content":"含义: JUC是java.util.concurrent工具包的简称,专门负责处理线程  ArrayList -\u0026gt; CopyOnWriteArrayList  内部持有一个可重入锁ReentrantLock,增删改(add,remove,set)等操作加锁保证线程安全,读(get)不必. 通过读写分离保证了并发效率,写时复制(联想到redis的rdb父子进程)出一个新的数组,写完再把新数组赋值给array. 由于读写分离需要复制新数组,因此适合操作较小的数组,若数组较大则占用内存多. 适合读操作更多或数组元素较小的情景 由于直接复制,不存在扩容操作   HashSet -\u0026gt; CopyOnWriteArraySet  CopyOnWriteArraySet内置了一个CopyOnWriteArrayList,所有操作都依赖于CopyOnWriteArrayList,由于set是去重的,因此list里面新增了addIfAbsent等方法来去重. CopyOnWriteArraySet的removeAll等批处理方法实际是调用基础的增删查改等方法,虽然get,set等方法是原子性的,但迭代调用它们却不是,因此调用批处理方法时需要另外加锁才能保证线程安全. CopyOnWriteArraySet的迭代器支持next(),hasNext()等不可变操作,但不支持remove()等可变操作.  TreeSet -\u0026gt; ConcurrentSkipSet  都支持排序.ConcurrentSkipSet依赖于ConcurrentSkipListMap实现,底层是跳表,TreeSet底层是红黑树. 它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整；而对跳表的插入和删除，只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，需要一个全局锁，来保证整个平衡树的线程安全；而对于跳表，则只需要部分锁即可。 ConcurrentSkipSet的线程安全不是通过加锁实现的,原理有些复杂,可以理解为线程冲突后再重新遍历.日后再细细研究  跳表的本质，是同时维护了多个链表，并且链表是分层的.\n HashMap -\u0026gt; ConcurrentHashMap  默认数组大小16,扩容因子0.75. HashMap的底层数据结构是一个hash桶,桶内每个元素又是一个链表,jdk1.8之后,若链表过长会转变为红黑树(查询效率提高至O(log n) ) ConcurrentHashMap在HashMap的基础上新增了一个Segment数组.Segment是ReentrantLock的子类,用于实现线程安全.Segment内置了一个HashEntry数组,可以理解成每个segment都是一个hashmap,通过对每个segment分开加锁,避免全局加锁,从而支持了相当于Segment 数组数量的并发量.(分段锁,可以联想到LongAdder对cells数组的CAS操作) HashEntry中的value使用了volatile修饰,不能保证原子性,因此增删改时仍要加锁(tryLock) put操作是两层hash,先是对key进行hash计算定位到相应的segment,然后再hash定位到对应的entry.如果获取锁失败,会自旋获取锁,自旋次数达到上限则改为阻塞锁获取,保证获取成功. 由于entry中的value使用了volatile修饰,保证了可见性,因此get操作不需加锁,,每次获取时都是最新值.  重点讲下1.8下concurrenthashmap的机制:\n 在jdk1.8中,抛弃了原先的Segment分段锁,转而采用CAS+synchronized,对node数组中单个链表加锁. 构造方法中没有初始化node数组,而是在首次put的时候进行判断是否初始化,属于懒汉式,同时通过一个互斥信号量sizeCtl和CAS来保证初始化或者扩容时的线程安全. put的时候,若node数组未初始化则初始化,若正在扩容则协助扩容,然后若对应的数组位置为空,则通过CAS来写入.若不为空,则对首节点加锁,之后的操作与hashmap一致. get操作遇到扩容时,需要调用正在扩容节点的find方法去nextTable里找,若匹配则返回   TreeMap -\u0026gt; ConcurrentSkipListMap 原理见上.\n","permalink":"http://euthpic.github.io/tech/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8collections/","summary":"含义: JUC是java.util.concurrent工具包的简称,专门负责处理线程 ArrayList -\u0026gt; CopyOnWriteArrayList 内部持有一个可重入锁ReentrantLock,增","title":"并发容器Collections"},{"content":"前面我们介绍map和list的时候说过,它们是线程不安全的.\n在并发环境下,线程安全是必须要关注的问题,所以今天这篇文章就围绕线程安全这个话题来展开.\n不过由于这部分的内容比较多且抽象,如果直接展开太多细节可能很难消化,所以今天只是简单谈谈并发中比较基础的概念,常见的线程安全问题以及解决方案,原理先不深究,好从整体上掌握这个方向.\n思维导图 线程安全 何谓线程安全?\n当多个线程访问某个类,或调用某个方法时，不管运行环境采用何种调度方式或者这些进程将如何交替执行，并且在主调代码中不需要任何额外的协同或者同步，这个类或方法都能表现出正确的行为，那么就可以说这个类或方法是线程安全的。 那为什么会出现线程不安全的情况呢?\n这得从JMM(Java内存模型)说起了\n在JMM中,每个线程不会直接去操作主存,而是先操作线程内的副本.\n举个栗子🌰,当线程A和B同时对变量进行喜**+1**操作,假设变量原先的值是10.\n那么单线程下它们是顺序执行的,A把10变为11,然后B再把11变为12.\n可是在并发环境下,它们也许是\u0026rsquo;同时\u0026rsquo;进行的.\nA,B同时把副本的10变成11.\n然后A把11写回主存,主存变为11,然后A继续运行.\n而B暂时并不知道A做了什么,它以为主存中的变量还是10,所以它仍然把副本里的11写回了主存.主存从11变为11(等于没变).\n所以B实际上相当于没有操作过,它的操作由于\u0026rsquo;误会\u0026rsquo;被抵消了.这就导致线程不安全了.\n线程安全性体现在3个方面:\n 原子性：提供互斥访问，同一时刻只能有一个线程对数据进行操作（Atomic、CAS算法、synchronized、Lock） 可见性：一个主内存的线程如果进行了修改，可以及时被其他线程观察到（synchronized、volatile） 有序性：如果两个线程不能从 happens-before原则 观察出来，那么就不能保证他们的有序性，虚拟机可以随意的对他们进行重排序，导致其观察观察结果杂乱无序（happens-before原则）  锁 为了保障线程安全,其中一项最主要的手段是给临界资源加锁🔒\nJava中的锁从各种性质上可分为多对含义相对的锁,比如:\n 乐观锁/悲观锁 共享锁/互斥锁 公平锁/非公平锁 可重入锁/不可重入锁  此外,根据synchronized的锁状态又可以分出无锁/偏向锁/轻量级锁/重量级锁.\n不过,从具体形式上来看,最主要的就是这两个: synchronized和Lock.\nsynchronized和Lock 它们的区别包括:\n 前者是关键字,后者为接口 synchronized在发生异常时,会自动释放锁,不会有死锁问题,而Lock不会释放,建议在finally中释放锁 Lock的子类包括可重入锁ReentrantLock,读写锁ReentrantReadWriteLock等,可通过lockInterruptibly()方法实现可中断性,通过构造方法来实现公平性等. Lock能起到的作用更多,例如中断等待锁的线程,获知是否成功获取锁. ReentrantReadWriteLock可通过读写分离提高并发效率,而synchronized在高并发下效率较低.  不过synchronized经过1.6的优化后(也就是上面说的锁状态)效率提高了.\n举个🌰,ConcurrentHashmap原先使用了CAS+分段锁,1.8之后改回了synchronized,这说明官方对于synchronized的效率是有信心的.\n乐观锁/悲观锁 悲观锁\n悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改\n例子 : synchronized,Lock\n乐观锁\n乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。\n例子 : CAS\n共享锁/互斥锁 互斥锁\n也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。\n例子 : synchronized,ReentrantLock,ReentrantReadWriterLock中的写锁\n共享锁\n是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。\n例子 : ReentrantReadWriterLock中的读锁\n公平锁/非公平锁 公平锁\n多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。\n公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。\n非公平锁\n多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。\n非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。\n例子 : ReentrantLock可通过构造函数里的fair参数来决定是否公平,默认是非公平锁\n可重入锁/不可重入锁 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞,不会自己阻塞自己导致死锁。\n例子 : synchronized,ReentrantLock\nps : 特地备注在这里,经过1.6优化后synchronized复杂了很多,根据锁的状态不同,它既可以是乐观锁也可以是悲观锁,既可以是共享锁也可以是互斥锁,不需要特地去记它的分类,研究完它的原理之后自然了然于心.\nvolatile volatile的作用\n 保证数据的可见性. 禁止指令重排序 不能保证原子性  volatile做了哪些工作?\n  当写一个volatile变量时，写完后JMM会把该线程对应的本地内存中的共享变量立即刷新到主内存\n  当读一个volatile变量时，读之前JMM会把该线程对应的本地内存置为无效，线程接下来将从主内存中读取共享变量\n  ThreadLocal ThreadLocal类用来提供线程内部的局部变量。\n这种变量在多线程环境下访问（通过get和set方法访问）时能保证各个线程的变量相对独立于其他线程内的变量。\nThreadLocal实例通常来说都是private static类型的，用于关联线程和线程上下文。\nThreadLocal变量在线程的生命周期内起作用，减少同一个线程内多个函数或组件之间一些公共变量的传递的复杂度。\n上述可以概述为：ThreadLocal提供线程内部的局部变量，在本线程内随时随地可取，隔离其他线程。\n使用场景\n数据库连接、Session管理等\n线程池 线程池好处  重用存在的线程,减少对象创建,销毁的开销 可有效控制最大并发线程数 提供定时执行,定期执行等功能.  ThreadPoolExecutor 线程池可以通过ThreadPoolExecutor生成,以下是它的构造函数:\npublic TheadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) Executors Executors类为我们提供了几种常用的线程池,可以跳过参数设置直接使用.\n  newFixedThreadPool (固定数目线程的线程池) :\n  newCachedThreadPool(线程无限的线程池)\n  newSingleThreadExecutor(单线程的线程池)\n  newScheduledThreadPool(定时或周期执行的线程池)\n  但是,对于线程的使用必须要慎重,如果不清楚线程池的各种特点就滥用很可能导致严重的线上故障.\n因此,Executors这种\u0026rsquo;傻瓜一键式\u0026rsquo;创建线程池的做法在阿里开发手册里是被禁止的.\nJ.U.C J.U.C是java.util.concurrent工具包的简称,专门负责处理线程与并发相关的问题.\nJ.U.C的内容包括原子类,并发容器,并发工具以及我们上面讲到的线程池.\n原子类 原子类也就是Atomic包下的类.\n在并发环境中,共享资源不能使用int,long double等基本类型,而是使用AtomicXXX等类,例如AtomicInteger,AtomicLong等的incrementAndGet ()方法.\n最常用到的类包括:\n AtomicBoolean：以原子更新的方式更新 boolean. AtomicInteger：以原子更新的方式更新 Integer. AtomicLong：以原子更新的方式更新 Long.  并发容器 前面我们提到过Hashmap,ArrayList是线程不安全的,可以在JUC里面找到它们对应的线程安全类.\n它们对应的替代关系:\n ArrayList -\u0026gt; CopyOnWriteArrayList HashSet -\u0026gt; CopyOnWriteArraySet TreeSet -\u0026gt; ConcurrentSkipSet HashMap -\u0026gt; ConcurrentHashMap TreeMap -\u0026gt; ConcurrentSkipListMap  并发工具 前面我们提到的原子类以及并发容器都是为了防止线程安全问题,数据不一致问题的发生而设计.\n在并发环境下,完成一个任务可能需要线程之间相互协作,这时候就需要用到并发工具了.\nJava的并发工具包括:\n CountDownLatch : 倒数器(计数器).举个栗子🌰,赛跑比赛中,裁判得等到选手们都抵达终点,他的任务才算完成.假设裁判和选手都是线程,每一个选手线程抵达终点,裁判线程的倒数器就-1,减到0之后裁判在这阶段的工作才算完成,才可以进行下一阶段的工作. Semaphore : 信号量.用于控制资源能够被并发访问的线程数量，以保证多个线程能够合理的使用特定资源.这个大家应该不陌生了,生产者消费者模型就是最典型的应用之一. CyclicBarrier : 栅栏.用于控制线程之间的同步,需要所有线程都到达,然后才能继续执行.赛跑比赛中,所有选手都抵达起跑线之后,比赛才能开始,选手才能开始跑. Exchanger : 用于两个线程在同步点交换资源.举个栗子🌰,无间道看过吧,卧底和阿sir到指定地点交换情报或者证据.为什么要强调同步点呢,因为在不同的时候,线程的资源值是不同的,必须要在正确的时间,才能交换到正确的资源,如果卧底提前把情报放在那里,可能会被掉包.  AQS 队列同步器AQS是AbstractQueuedSynchronizer的简称,是JUC的核心类.\nAQS使用了一个int类型的变量表示同步状态,通过内置的FIFO队列来完成线程获取资源的排队工作. AQS是实现锁的关键.简单来说,它为并发包的各个组件加锁解锁提供了底层支持.\n基本上,JDK的锁(除了关键字synchronized)都有一个内部类Sync,这个Sync一定是继承AQS的.\nCAS CAS = compare and swap 先比较后交换\nCAS是JUC并发包的核心实现,本质是一种乐观锁,自旋锁.\n当多线程同时对某资源进行操作时,只能有一个线程操作成功,但不会阻塞其他线程而是通知它们操作失败,最底层调用的是native方法.\n该算法有三个操作数:\n  内存值V,\n  旧的预期值A,\n  要修改的新值B\n  当且仅当A和V的值相同时,才将内存值V更新为新值B,否则什么都不做.\n一般情况下，“更新”是一个不断重试的操作(死循环)。\nCAS虽然很高效，但是它也存在三大问题，这里也简单说一下：\nABA问题\nCAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。\nJDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。\n循环时间长开销大\nCAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。\n只能保证一个共享变量的原子操作\n对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。\nJava从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。\n安全发布对象 发布对象 使一个对象能够被当前范围之外的代码所使用.\n相对的概念 : 对象逸出 一种错误的发布.当一个对象还没有构造完成时,就使它被其它线程所见.\n四种方法  在静态初始化函数中初始化一个对象引用. 将对象的引用保存到volatile类型域或者AtomicReference对象中. 将对象的引用保存到某个正确构造对象的final类型域中. 将对象的引用保存到一个由锁保护的域中.  线程不安全类与写法  StringBuilder -\u0026gt; StringBuffer SimpleDateFormat -\u0026gt; joda-time or java8的日期api ArrayList,HashSet,HashMap等 非原子性操作,例如先检查再执行: if( condition(a) ) { handle(a); }  后言 本文带大家走马观花地看了一遍并发中比较基础的概念,常见的线程安全问题以及解决方案,意在对于并发编程的全貌有个大致的框图.\n当然了,并发是一个很复杂也很庞大的问题,一篇文章是不支持深入到各个细节去研究的.\n对于本文所提到的各个组件,各种原理,后续会由一系列文章来展开.\n","permalink":"http://euthpic.github.io/tech/%E5%85%B3%E4%BA%8E%E5%B9%B6%E5%8F%91/","summary":"前面我们介绍map和list的时候说过,它们是线程不安全的. 在并发环境下,线程安全是必须要关注的问题,所以今天这篇文章就围绕线程安全这个话题","title":"关于并发"},{"content":"线程池好处  重用存在的线程,减少对象创建,销毁的开销 可有效控制最大并发线程数 提供定时执行,定期执行等功能.   ThreadPoolExecutor 线程池可以通过ThreadPoolExecutor生成,以下是它的构造参数:\npublic TheadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 几个核心参数的作用:\n corePoolSize: 核心线程数量 maximumPoolSize: 最大线程数 workQueue: 任务队列,存储等待执行的任务(有界阻塞) keepAliveTime: 线程没有任务执行时最多保持多久时间 unit: 上面的时间单位 threadFactory: 线程工厂,用于创建线程 rejectHandler: 当队列已满时如何处理任务的策略   任务执行 线程池执行过程,亦即execute()方法的调用过程:\n 提交一个任务,若线程池里的线程数小于corePoolSize时,创建一个新线程去处理提交任务. 如果线程数已经大于等于corePoolSize,新提交的任务会被放入任务队列workQueue排队等待执行. 如果线程数已经大于等于corePoolSize,并且任务队列也已经满了,会判断线程数是否达到maximumPoolSize,若未达到,则创建一个非核心线程来执行提交的任务. 如果线程数已经达到maximumPoolSize,新任务到来之时会采用拒绝策略处理.   四种拒绝策略  AbortPolicy(默认,抛出一个异常) DiscardPolicy(直接丢弃新任务) DiscardOldestPolicy(丢弃队列里最老的任务,将当前任务继续提交给线程池) CallerRunsPolicy(交给线程池调用所在的线程进行处理,即此任务会从异步执行变为同步执行)   线程池异常处理   异常需要主动在任务中用try catch捕获,不会抛出到主线程.\n  如果调用的是submit()方法,异常也可以在FutureTask的get()方法处抛出.\n  也可以为工作线程设置UncaughtExceptionHandler来处理异常.\n  ExecutorService threadPool = Executors.newFixedThreadPool(1, r -\u0026gt; { Thread t = new Thread(r); t.setUncaughtExceptionHandler( (t1, e) -\u0026gt; { System.out.println(t1.getName() + \u0026#34;线程抛出的异常\u0026#34;+e); }); return t; }); threadPool.execute(()-\u0026gt;{ Object object = null; System.out.print(\u0026#34;result## \u0026#34; + object.toString()); });  工作队列 种类有:\n ArrayBlockingQueue LinkedBlockingQueue DelayQueue PriorityBlockingQueue SynchronousQueue  ArrayBlockingQueue: 用数组实现的有界阻塞队列,FIFO\nLinkedBlockingQueue: 用链表实现的阻塞队列,FIFO,容量可以进行设置,若不设置的话默认无边界.newFixedThreadPool,newSingleThreadExecutor线程池使用了这个队列\nDelayedWorkQueue: 一个任务定期执行的延迟队列,根据指定的执行时间从小到大排序,否则根据入队的先后排序.newScheduledThreadPool线程池使用了这个队列\nPriorityBlockingQueue: 具有优先级的无界阻塞队列\nSynchronousQueue: 一个不存储元素的阻塞队列,只负责移交数据,每个插入操作必须等待另一个线程取出,否则一直处于阻塞状态.newCachedThreadPool线程池使用了这个队列。\n Executors创建的几种常用的线程池   newFixedThreadPool (固定数目线程的线程池) :\n  newCachedThreadPool(线程无限的线程池)\n  newSingleThreadExecutor(单线程的线程池)\n  newScheduledThreadPool(定时或周期执行的线程池)\n  newFixedThreadPool特点 :\n 核心线程数和最大线程数大小一样(数量由用户指定,必须) 没有所谓的非空闲时间,即keepAliveTime为0 阻塞队列为无界队列LinkedBlockingQueue fixed的意思即队列大小适应任务数量,来多少处理多少 不会丢弃任务,但是任务积压过多会导致内存溢出. 适用于CPU密集任务,不适用IO密集任务  newCachedThreadPool特点:\n 核心线程数为0,即所有线程都是非核心线程 最大线程数为Integer.MAX_VALUE 阻塞队列是SynchronousQueue 非核心线程空闲存活为60s 适用并发执行大量短期的小任务  newSingleThreadExecutor特点:\n 核心线程和最大线程都是1 阻塞队列是LinkedBlockingQueue keepAliveTime为0 适用于串行执行任务 串行任务的数量一般不是很大,因此不用担心内存爆炸的问题  newScheduledThreadPool特点:\n 最大线程数是Integer.MAX_VALUE 阻塞队列是DelayedWorkQueue keepAliveTime为0 可选延迟策略有两种:  scheduleAtFixedRate() ：按某种速率周期执行 scheduleWithFixedDelay()：在某个延迟后执行     线程池状态  running: 运行态 shutdown: 调用shutdown()方法后进入此阶段,不再处理新任务,任务队列的任务处理完后会进入tidying stop: 调用shutdownNow()方法后进入此阶段,新任务与任务队列的任务都不处理,当各线程完成后进入tidying tidying: 该状态表明所有任务都运行完毕,可以准备进行销毁 terminated: tidying阶段调用terminated()后进入此阶段(该方法什么也不做),表示线程池彻底终止.   一些思考题   为什么newCachedThreadPool要使用SynchronousQueue队列?\n原因有两个:\n SynchronousQueue结构简单,消费速度比较快,高并发下效率更高些. SynchronousQueue适用于生产与消费速率一致的情况,而当队列中有新任务之后,newCachedThreadPool能马上新建线程去消费任务,因此能发挥SynchronousQueue的高性能优势    ","permalink":"http://euthpic.github.io/tech/%E7%BA%BF%E7%A8%8B%E6%B1%A0executors/","summary":"线程池好处 重用存在的线程,减少对象创建,销毁的开销 可有效控制最大并发线程数 提供定时执行,定期执行等功能. ThreadPoolExecutor 线程池可以通过ThreadPool","title":"线程池Executors"},{"content":"线程安全性包括以下三点:\n 原子性：提供互斥访问，同一时刻只能有一个线程对数据进行操作（Atomic、CAS算法、synchronized、Lock） 可见性：一个主内存的线程如果进行了修改，可以及时被其他线程观察到（synchronized、volatile） 有序性：如果两个线程不能从 happens-before原则 观察出来，那么就不能保证他们的有序性，虚拟机可以随意的对他们进行重排序，导致其观察观察结果杂乱无序（happens-before原则）   CAS (compare and swap) CAS是JUC并发包的核心实现,本质是一种乐观锁,自旋锁,当多线程同时对某资源进行操作时,只能有一个线程操作成功,但不会阻塞其他线程而是通知它们操作失败,最底层调用的是native方法.\n该算法有三个操作数,内存值V,旧的预期值A,要修改的新值B,当且仅当A和V的值相同时,才将内存值V修改为B,否则什么都不做.\n相关应用: **自旋锁,**CAS操作失败时一直循环,直到成功为止.\n在JAVA9之前,JUC的CAS操作是由Unsafe这个类来完成的,但是在java9之后变成了VarHandle这个类,主要是出于安全性和可移植性考虑.  Atomic包 在并发环境中,共享资源不能使用int,long double等基本类型,而是使用AtomicXXX等类,例如AtomicInteger,AtomicLong等的incrementAndGet ()方法.\nAtomicXXX -\u0026gt; Unsafe类的compareAndSwapInt.\n AtomicLong与LongAdder Atomic包的CAS操作在竞争激烈的环境对于CPU的消耗过大,因此java8中新增了LongAdder,核心实现思想是热点分离.\n原理: 高并发时将对单一变量的CAS操作分散为对cells数组中多个元素的CAS操作(通过hash判定操作哪个元素),取值时进行求和;低并发时仅对base变量(初始值V)进行CAS操作.\n优缺点:\n AtomicLong操作简单,适合并发度低或者对结果严谨的环境. LongAdder操作复杂,适合高并发的环境,但是可能会出现统计有偏差的情况.   AtomicReference 作用是原子性更新对象引用.\nAtomicIntegerFieldUpdater 作用是原子性更新该类的某个指定字段的值,该字段需要用volatile修饰且提供get方法.\n CAS的ABA问题 某资源的初始值的A,一个线程把它修改为B,另一个线程又把它修改回A,这样该资源会被CAS判定为未修改过,但实际上却被修改过了.解决方法是AtomicStamperReference类,为资源增加版本标记,每次操作将版本号加1,进行CAS比较时不仅比较数值,也比较版本号.\n 两种锁:synchronized和Lock 区别在于:\n 前者是关键字,后者为接口 synchronized在发生异常时,会自动释放锁,不会又死锁问题,而Lock不会释放,建议在finally中释放锁 Lock的子类包括可重入锁ReentrantLock,读写锁ReentrantReadWriteLock等,可通过lockInterruptibly()方法实现可中断性,通过构造方法来实现公平性等. Lock能起到的作用更多,例如中断等待锁的线程,获知是否成功获取锁. ReentrantReadWriteLock可通过读写分离提高并发效率,而synchronized在高并发下效率较低.   可重入锁 在Java中,synchronized和ReentrantLock属于可重入锁,即同一个线程在外层方法获取锁的时候,在进入内层方法会自动获取该锁,不会造成死锁.\n 偏向锁/轻量级锁/重量级锁 这几个是描述锁的状态,是JVM底层与synchronized的实现有关的概念,从1.6开始优化synchronized而引入的锁机制,原理比较复杂,目前也不太需要关心,使用JVM默认的策略即可.\n 可见性-volatile 通过加入内存屏障和禁止重排序优化来实现\n 对volatile变量写操作时,会在写操作后加入一条store屏障指令,将本地内存中的共享变量值刷新到主内存中.(写完后及时保存) 对volatile变量读操作时,会在读操 作前加入一条load屏障指令,从主内存中读取共享变量.(读之前先更新)   有序性 - happens-before原则 共8条,意思大概是A操作必须先于B操作,太长不想记.\n 线程封闭  ad-hoc 线程封闭 : 程序控制实现,最糟糕,忽略 堆栈封闭 : 局部变量,无并发问题 ThreadLocal : 如果多线程需要操作的是全局变量而非线程内部的变量,可以用threadlocal进行封闭.应用有数据库连接池,获取connection   StringBuffer与StringBuilder与String string为final类,绝对的线程安全.但是每个string都是一个新的对象,内存消耗大.\nstring重写了equals和hashcode方法,两个字符串比较内容而不是地址.\n单线程下的效率builder\u0026gt;buffer\u0026gt;string,多线程下修改字符串首选buffer.\nStringBuffer如何实现线程安全?\n在append等方法上使用synchronized修饰.\n 线程不安全类与写法  StringBuilder -\u0026gt; StringBuffer SimpleDateFormat -\u0026gt; joda-time or java8的日期api ArrayList,HashSet,HashMap等 非原子性操作,例如先检查再执行: if( condition(a) ) { handle(a); }   同步容器与并发容器 同步容器包括HashTable,Vector等,是java早期对于线程安全的解决方案,能保证线程安全,但不保证并发效率.如今不推荐使用\n并发容器包括JUC包下的ConcurrentXXX等工具类\n","permalink":"http://euthpic.github.io/tech/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8/","summary":"线程安全性包括以下三点: 原子性：提供互斥访问，同一时刻只能有一个线程对数据进行操作（Atomic、CAS算法、synchronized、Lo","title":"线程安全"},{"content":"何时该用缓存  某些应用消耗大量CPU去计算,例如正则表达式,就可以考虑将结果缓存下来 数据库连接池比较繁忙,经常报出连接不够的报警,也该考虑缓存  缓存的选择 ​\t单机缓存用guava或者caffeine,分布式用redis.\n​\t本地缓存用guava或者caffeine,远程用redis.\n​\t数据量不是很大,数据更新频率较低,可以使用本地缓存guava;如果数据量更新频繁,也想使用进程缓存的话,可以将其过期时间设置为较短,或者设置较短的自动刷新时间.\n二级缓存方案 ​\t利用 Caffeine 做一级缓存，Redis 作为二级缓存，步骤如下：\n 首先去 Caffeine 中查询数据，如果有直接返回。如果没有则进行第 2 步。 再去 Redis 中查询，如果查询到了返回数据并在 Caffeine 中填充此数据。如果没有查到则进行第 3 步。 最后去 MySQL 中查询，如果查询到了返回数据并在 Redis，Caffeine 中依次填充此数据。  对于 Caffeine 的缓存，如果有数据更新，只能删除更新数据的那台机器上的缓存，其他机器只能通过超时来过期缓存，超时设定可以有两种策略：\n 设置成写入后多少时间后过期。 设置成写入后多少时间刷新。  对于 Redis 的缓存更新，其他机器立刻可见，但是也必须要设置超时时间，其时间比 Caffeine 的过期长。\n缓存更新 ​\t一般来说缓存的更新有两种情况：\n 先删除缓存，再更新数据库。 先更新数据库，再删除缓存。  1.先删除缓存再更新DB\n结论：产生脏数据的概率较大（若出现脏数据，则意味着再不更新的情况下，查询得到的数据均为旧的数据）(这种情况问题可能出在写数据库期间)\n比如：两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。\n2.先更新DB再删除缓存（使用场景多）\n结论：产生脏数据的概率较小，但是会出现一致性的问题；若更新操作的时候，同时进行查询操作，若hit，则查询得到的数据是旧的数据。但是不会影响后面的查询。（代价较小）(这种情况问题可能出现在删除缓存期间)\n（ 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。）\n该设计模式产生脏数据的可能情况：\n一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效;然后,之前的那个读操作再把老的数据放进去，所以，会造成脏数据。\n该情况出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。\n缓存挖坑三剑客 缓存穿透  定义:\t查询的数据在数据库不存在,所以缓存中也不会有,永远都无法命中而会去数据库查询,给数据库增加了无用的请求 解决方案: 对于返回为NULL的数据依然缓存一个自定义的值,并且设置较短的超时时间来减少缓存的维护成本.对于抛出异常的返回不进行缓存 优化思路: 制定一些规则过滤一些不可能存在的数据，小数据用 BitMap，大数据可以用布隆过滤器。比如你的订单 ID 明显是在一个范围 1-1000，如果不是 1-1000 之内的数据那其实可以直接给过滤掉。  缓存雪崩  定义:\t是指缓存不可用或者大量缓存由于超时时间相同在同一时间段失效,大量请求直接访问数据库,数据库压力过大导致系统雪崩 解决方案:  采用多级缓存,不同级别缓存设置的超时时间不同 缓存根据类别来设置过期时间,例如热门数据过期时间长(或永不过期),冷门数据过期时间短   优化思路:  缓存击穿   定义:\t跟缓存雪崩很像,都是由于缓存过期导致大量请求直接访问数据库,不同的是雪崩是一大片同时失效(面),击穿是某个热点数据失效(点).\n  解决方案:\n 加分布式锁:加载数据的时候可以利用分布式锁锁住这个数据的key,在redis中直接使用setNX操作即可,对于获取到这个锁的线程,查询数据库更新缓存,其他线程采取重试策略,这样数据库同一时间只能有一个线程来访问. 异步加载:对热点数据采取到期自动刷新的策略,而不是到期自动淘汰.    优化思路:\n  缓存的监控 ​\t缓存上线之后统计获取缓存的情况(成功,不存在,过期还是失败等),继而可有效对于统计情况来对缓存的参数进行优化\n一般避免以上情况发生我们从三个时间段去分析下：\n 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + Hystrix 限流+降级，避免MySQL 被打死。 事后：Redis 持久化 RDB+AOF，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。   一致性哈希 简单哈希可用于数据分区,但机器变动的时候可能会发生数据迁移.而一致性哈希算法可减少因机器变动而带来的数据迁移量)仅会影响相邻的机器.\n ","permalink":"http://euthpic.github.io/tech/%E7%BC%93%E5%AD%98/","summary":"何时该用缓存 某些应用消耗大量CPU去计算,例如正则表达式,就可以考虑将结果缓存下来 数据库连接池比较繁忙,经常报出连接不够的报警,也该考虑缓存","title":"关于缓存"},{"content":"思维导图 前排友情提示 : 周志明的那本深入理解JVM去年已经出第3版了,主要更新了GC工作的详细过程,包括\u0026quot;三色标记法\u0026quot;等.\n什么是JVM JVM是Java Virtual Machine（Java虚拟机）的缩写，包括一套字节码指令集、一组寄存器、一个栈、一个垃圾回收堆和一个存储方法域。 JVM屏蔽了与具体操作系统平台相关的信息，使Java程序只需生成在Java虚拟机上运行的目标代码（字节码）,就可以在多种平台上不加修改地运行。JVM在执行字节码时，实际上最终还是把字节码解释成具体平台上的机器指令执行。\nJRE/JDK/JVM是什么关系 JRE(JavaRuntimeEnvironment，Java运行环境)，也就是Java平台。所有的Java 程序都要在JRE下才能运行。普通用户只需要运行已开发好的java程序，安装JRE即可。\n**JDK(Java Development Kit)**是程序开发者用来来编译、调试java程序用的开发工具包。JDK的工具也是Java程序，也需要JRE才能运行。为了保持JDK的独立性和完整性，在JDK的安装过程中，JRE也是 安装的一部分。所以，在JDK的安装目录下有一个名为jre的目录，用于存放JRE文件.Oracle并不是唯一提供JDK的公司,此外流行的还有开源的OpenJDK.\n在上面这张图中,java文件经过JDK里的编译器编译成class文件,然后JIT将字节码翻译成不同平台的机器码来运行.\n为什么要了解JVM 学习JVM主要关注点在于解决两个问题:\n StackOverflowError OutOfMemoryError  也就是我们常讲的栈溢出和内存溢出.\n初学者往往会忽略程序使用的内存空间是有限的,因为入门时写的demo程序并不会引发这种问题.\n不过在真正的线上环境下,如果你的项目访问量大,需要处理的数据多,假如程序编写得不够好,或者JVM参数不够合理,很可能会频繁出现OOM引起的full gc,这时候排查问题就得依靠我们对JVM的理解了.\n内存模型 ","permalink":"http://euthpic.github.io/tech/jvm%E5%8E%9F%E7%90%86/","summary":"思维导图 前排友情提示 : 周志明的那本深入理解JVM去年已经出第3版了,主要更新了GC工作的详细过程,包括\u0026quot;三色标记法\u0026quot;等.","title":"JVM原理"},{"content":"思维导图 集合框架 Java集合是Java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。\nJava集合工具包的位置是Java.util.*。\n类图 可以看到,Java集合主要可以划分为四个部分：List列表、Set集合、Map映射、工具类（Iterator迭代器、Enumeration枚举类、Arrays和Collects）。\n 虽然涉及的类和接口很多,但其中一部分像Hashtable或Vector是历史遗留或者不常用到的类,没有过分深究的必要. List是一个有序的队列，每一个元素都有它的索引。第一个元素的索引值是0.List的实现类有LinkedList, ArrayList等. Set是一个不允许有重复元素的集合。 Set的实现类有HastSet和TreeSet。HashSet依赖于HashMap，它实际上是通过HashMap实现的；TreeSet依赖于TreeMap，它实际上是通过TreeMap实现的.因此掌握了Map也就掌握了Set Map是一个映射接口，即key-value键值对。Map中的每一个元素包含“一个key”和“key对应的value”.AbstractMap是个抽象类，它实现了Map接口中的大部分API Iterator是遍历集合的工具，我们通常通过iterator()函数来遍历集合.  Fail-fast机制 当多线程同时操作同一个Collection时可能会抛出一个ConcurrentModificationException异常,这就是一个fail-fast事件.\n来看一段代码:\nfinal void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } modCount这个变量是用来记录集合被修改的次数,每次修改后modCount计数+1.\npublic void remove() { if (lastRet \u0026lt; 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } 每次要对集合进行操作前,都要调用checkForComodification()来检查预期值expectedModCount和集合当前的modCount是否一致.\n如果此时也有另一个线程在修改集合的内容,那么就会不一致,就会立刻抛出异常.\nFast-fail解决办法:\n通过并发工具J.U.C下的相应包来进行处理,例如CopyOnWriteArrayList,ConcurrentHashmap等,这将在之后讲并发的时候再详细介绍.\n接下来就开始介绍hashmap了.作为Java集合最重要的类之一,同时也是面试的高频问题,hashmap可以说必须要方方面面都研究透彻才行.\n数据结构 hashmap由链表和数组组成.这个数组1.8之前叫做Entry,后来改名Node,实现了**Map.Entry\u0026lt;K,V\u0026gt;**这个接口.\nstatic class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; final K key; V value; Node\u0026lt;K,V\u0026gt; next; .... } Node数组实际上是一个哈希桶,每个node根据其key计算出hash后会放在相应的位置上.\n如果该位置已经有了元素,也就是发生了hash冲突,那么这个位置就会拉出来一条链表,这种解决hash冲突的方法叫链地址法.\n其它变量:\n initialCapacity : Node数组初始长度,必须为2的次幂,默认是16. loadFactor : 加载因子,默认为0.75,当hashmap容量达到一定比例时,比如16*0.75=12,会触发扩容. threshold : 扩容阈值,也就是hashmap能容纳的最大键值对数,扩容阀值 = initialCapacity * loadFactor ,当键值对数量到达该值后触发扩容,扩容后为当前的两倍. size : map里面键值对的数量. TREEIFY_THRESHOLD = 8 : 树化阈值,当链表超过这个长度时,将会转化为红黑树.树化相关的几个变量都是1.8后新增的. UNTREEIFY_THRESHOLD = 6 : 链化阈值,当红黑树的size小于该值将退化成链表. MIN_TREEIFY_CAPACITY = 64 : 另一个决定是否树化的阈值,当node数组的长度小于该值时,即便链表过长也不会进行树化.  Hash() 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步.\n计算索引 对于任意给定的对象,只要它们的hashCode()返回的值相同,那么它们落到哈希桶的位置(index)也应该相同.\n那么很自然首先就想到了把hash值对数组长度取摸计算, index = hashCode() % length.这样元素的分布是比较均匀的.\n可是问题在于,模运算的消耗还是比较大的.\n来看看jdk是怎么解决的:\n方法一： static final int hash(Object key) { //jdk1.8 \u0026amp; jdk1.7  int h; // h = key.hashCode() 为第一步 取hashCode值  // h ^ (h \u0026gt;\u0026gt;\u0026gt; 16) 为第二步 高位参与运算  return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 方法二： static int indexFor(int h, int length) { //jdk1.7的源码，jdk1.8没有这个方法，而是直接写入到各个方法里  return h \u0026amp; (length-1); //第三步 取模运算 } 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。\n方法一把hash值算了出来,但在方法二中计算索引并不使用%运算,而是**\u0026amp;**.\n这就是hashmap在速度上的优化,当length总是2的次方时, h \u0026amp; (length-1) 等价于 h % length.而\u0026amp;比%效率更高.\n证明方法也很简单,先举个例子:\n假设数组长度n=1 0000, n-1=1111, 哈希值h=1111 1111 1111 1111 1111 0000 1110 1010\n那么(n-1) \u0026amp; hash = 0101 = hash % n = 5 (十进制),两者是相等的.\n如果还不明白为什么两者等价,看看下面的图感受一下吧.\n 假设hash值为m,数组长度为length,那么我们需要证明的就是m % length =m \u0026amp; (length - 1) 可以把m分为高位H和低位L,低位L的位数与length相同. 比如m=1100 1011 ,length=1000 ,那么H=1100 0000 ,L=1011 那么m % length = (H + L) % length = H % length + L % length(二进制的模运算和十进制是一样的,可以用分配律) 由于length是2的次方,那么H % length一定等于0(因为H是m的高位,其低位全0) 那么m % length = L % length. 由于L与length位数相同,所以 L % length有两种情况:  L \u0026gt;= length ,那么L的最高位为1, L % length =L - length,也就是结果为L除了最高位之外的低位.如果L的位数是n,那么L的n-1位就是L % length的结果,所以L % length = L \u0026amp; ( length - 1) =m \u0026amp; (length - 1) L \u0026lt; length, 那么L的最高位为0, L % length =L,由于最高位必为0,所以这个也只用看L的n-1位即可   综上,m % length = L % length = L \u0026amp; ( length - 1) = m \u0026amp; (length - 1),证明完毕  扰动函数 上面分析了方法二,接下来继续看方法一的作用.\n如果Node数组的长度比较小,那么运算 h \u0026amp; (length-1)就等于把h的高位给抛弃掉了,这样任意高位不同,但低位相同的hash值都会得到相同的索引,哈希冲撞的概率比较大.\n比如,1111 0000和0001 0000在对0000 1111进行按位与运算后的值是相等的。\n为了解决这个问题,需要对hash进行扰动计算.\nJDK的解决方案如同方法一所示 : 将hash的高位和低位进行异或运算,这样低位也能保留高位的特征,从而减少了冲撞的可能.\nPut HashMap的put方法执行过程可以通过下图来理解:\n 判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容(初始化)； 根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，\u0008转向③； 判断\u0008table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； 遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；\u0008遍历过程中若发现key已经存在直接覆盖value即可； 插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。  JDK1.8HashMap的put方法源码如下:\npublic V put(K key, V value) { // 对key的hashCode()做hash  return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // 步骤1：tab为空则创建  if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤2：计算index，并对null做处理  if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node\u0026lt;K,V\u0026gt; e; K k; // 步骤3：节点key存在，直接覆盖value  if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) e = p; // 步骤4：判断该链为红黑树  else if (p instanceof TreeNode) e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤5：该链为链表  else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); //链表长度大于8转换为红黑树进行处理  if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st  treeifyBin(tab, hash); break; } // key已经存在直接覆盖value  if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key  V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 步骤6：超过最大容量 就扩容  if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; } Get get方法的流程与put相近,不过免除了扩容和是否树化等判断,相对简单.\n 先用hash计算出index,如果对应位置为null,则返回null. 如果对应位置是链表,则遍历链表,用key或key.equals()逐一对比,如果没有找到相同的key则返回null. 如果对应位置是树,那么就调用红黑树专门的get方法(红黑树算法比较复杂,这里就不展开了)  总结下来就两步:根据key的hashcode()找到对应索引,然后根据key.equals()在该位置上找到对应node.\nResize 当我们不断往hashmap里添加元素,最终size\u0026gt;=threshold,那么就会触发扩容,node数组的长度需要增加,以便承载更多元素.\n在Java里数组是无法自动扩容的,因此扩容的方法就是使用一个新的数组来代替原来小容量的数组.顺便说下,ArrayList的扩容也是同样的原理.\n由于1.8引入了红黑树,resize方法比较复杂,这里先分析1.7的resize源码,本质上区别不大.\nvoid resize(int newCapacity) { //传入新的容量  Entry[] oldTable = table; //引用扩容前的Entry数组  int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { //扩容前的数组大小如果已经达到最大(2^30)了  threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了  return; } Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组  transfer(newTable); //！！将数据转移到新的Entry数组里  table = newTable; //HashMap的table属性引用新的Entry数组  threshold = (int)(newCapacity * loadFactor);//修改阈值  } transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。\nvoid transfer(Entry[] newTable) { Entry[] src = table; //src引用了旧的Entry数组  int newCapacity = newTable.length; for (int j = 0; j \u0026lt; src.length; j++) { //遍历旧的Entry数组  Entry\u0026lt;K,V\u0026gt; e = src[j]; //取得旧Entry数组的每个元素  if (e != null) { src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象）  do { Entry\u0026lt;K,V\u0026gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置  e.next = newTable[i]; //标记[1]  newTable[i] = e; //将元素放在数组上  e = next; //访问下一个Entry链上的元素  } while (e != null); } } } 头插 \u0026amp; 尾插 1.7的扩容使用了头插入方式,同一位置上新元素总会被放在链表的头部位置.\n假设扩容后某链表全部元素还是在同一位置,那么扩容相当于反转这条链表了.\n看看下图感受下1.7的扩容流程:\n在多线程环境下,头插入可能会形成环链表,从而陷入死循环.\n假设一条链表为A -\u0026gt; B .那么扩容前它们的关系就是A.next = B.\n扩容之后链表会变成B -\u0026gt; A. 关系变成B.next = A . 这两条关系同时成立的话就会死循环.\ne.next = newTable[i]; //1  newTable[i] = e; //2  e = next; //3 如果线程A执行完第一步就退出轮到线程B继续扩容,那么死循环就有可能发生.\n因此在1.8中改成了尾插入.至于原先为什么使用头插入,可能是出于二八定理,作者认为最后插入的元素也就是最近插入的元素,更容易被用得到,因此扩容后应该放在前面.\n对链表的优化 除此之外,1.8对于新旧链表的迁移还有一个优化.\n不知道你有没有思考过这样一个问题,扩容后需要重新计算index吗?\n有人可能下意识地认为:当然要!根据公式h \u0026amp; (length-1),index与数组长度有关,扩容后数组长度变了,自然要重新算一遍.\n其实的确是需要的.\n不过并不像大多数人以为那样继续套用h \u0026amp; (newLength-1)这个公式.\n上面这张图,数组扩容后,长度从n变为2n, key1的索引没变,而key2的索引加了n.\n索引的变化也只有这两种情况了,要么不变,要么+n.下面证明一下:\n 扩容前索引为 h \u0026amp; (n-1) ,扩容后索引为 h \u0026amp; (2n-1) (2n-1)- (n-1) = n ,所以用二进制表示的话,2n-1与n-1的区别仅在于高位多了个1,低位不变. 那么按位与的结果仅仅看hash的那一位是否为1,若为1,则结果那一位就为1,也就是新索引=旧索引+n. 若那个高位不为1,则结果不变,也就是新索引=旧索引. 因此,用按位与,也就是 h \u0026amp; n可以判断出h的高位是否为1.  因此,1.8中用if ((e.hash \u0026amp; oldCap) == 0)来判断节点的索引在扩容后是不变还是+n.\n有人觉得这个是性能上的优化,我觉得其实是配合尾插入算法改进的逻辑.\n因为计算h \u0026amp; (n-1)和计算(h \u0026amp; n)==0的开销是一样的,但是后者可以知道索引是否改变的情况,从而新建两条链表,一条链表是索引不变的元素,另一条链表是索引变化的元素,然后再分别插入到两个位置去.\nNode\u0026lt;K,V\u0026gt; loHead = null, loTail = null;//lo链表复制索引不变的元素  Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null;//hi链表复制索引+n的元素  Node\u0026lt;K,V\u0026gt; next; do { next = e.next; if ((e.hash \u0026amp; oldCap) == 0) {//用按位与判断节点的索引是否变化  if (loTail == null)//这里就是尾插入了  loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead;//lo链表直接复制到新数组的原位置  } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead;//hi链表则复制到新数组的+n位置  } 扩容后索引是否变化是随机的,可以认为两者概率相等,那么扩容后,链表的元素就均匀地分布到新数组中了.\n总结 hashmap的内容这里就介绍完了,下面简单总结一下:\n hashmap由链表+数组组成. 解决哈希冲突的办法是链地址法. Hash算法本质上就是三步：取key的hashCode值、高位运算、取模(按位与)运算。 1.8之后引入了红黑树,get/set/remove都得考虑链表和红黑树两种情况 当哈希冲突严重时,红黑树可以将时间复杂度从O(n)降低到O(log n) 扩容的方法就是使用一个新的数组来代替原来小容量的数组. 扩容条件是size \u0026gt;= (threshold = loadFactor * initialCapacity ). 1.7的头插法不安全,1.8改成了尾插法.  使用注意事项  扩容十分消耗性能,因此初始化时需要预估一个大致的初始容量,且该值为2的次幂. 负载因子是可以更改的,往小了调hash冲突概率低,查询快,但map的空间利用不充分,属于空间换时间.往大了调就是时间换空间. hashmap并非线程安全,并发环境下请使用J.U.C包下的ConcurrentHashmap替代  ","permalink":"http://euthpic.github.io/tech/hashmap%E7%AE%80%E5%8D%95%E5%8E%9F%E7%90%86/","summary":"思维导图 集合框架 Java集合是Java提供的工具包，包含了常用的数据结构：集合、链表、队列、栈、数组、映射等。 Java集合工具包的位置是Ja","title":"Hashmap简单原理"},{"content":"上一篇文章我们介绍了Hashmap,今天来看一看List接口下最主要的两个类 : ArrayList和LinkedList\n思维导图 ArrayList 链表与数组 从名字就可以看出来,ArrayList使用的是数组,LinkedList使用的是链表.\n 数组在内存中是地址连续的,而链表是离散的,每个节点都需要记录其后继节点(双向链表同时需要记录前节点) 数组只要知道了其中一个节点的地址,就能通过计算相对地址来访问其他节点,因此数组可以实现任意访问(ramdaomly access),适合读操作比较多的场景. 访问链表中的任意节点都需要从头节点开始遍历,逐一获取其后续节点,因此链表的写操作比较慢.但是也由于地址不连续,在增删节点时不需要考虑位置,直接修改相关的next节点就行了,因此链表适合频繁增删的场景.  链表与数组各有优点和缺点,这也是我们系统设计的一个特点 : 没有完美的方案,有的只是根据业务场景做出取舍,要么空间换时间,要么牺牲效率来提高安全性.\n数据结构 就是一个名叫elementData的数组,默认的初始长度是10;\ntransient Object[] elementData; // non-private to simplify nested class access  /** * Default initial capacity. */ private static final int DEFAULT_CAPACITY = 10; 初始化 ArrayList有三个构造方法:\n无参时设置为空数组,直到第一次往list里面add数据才会初始化为长度为10的数组.这里的相关逻辑有几层调用链,后面扩容的时候再详细展开\npublic ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;//设置成空数组  } private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; 参数为集合时将其转化为数组:\npublic ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // defend against c.toArray (incorrectly) not returning Object[]  // (see e.g. https://bugs.openjdk.java.net/browse/JDK-6260652)  if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array.  this.elementData = EMPTY_ELEMENTDATA; } } 当参数为一个整型,也就是指定数组长度时,新建一个指定长度的数组\npublic ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA;//当传入的是0,处理方法和无参构造的一致  } else { throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } Add 由于list的get,set都太简单,这里就直接跳过来分析add了.和map一样,add也可能触发扩容.\nprivate void add(E e, Object[] elementData, int s) { if (s == elementData.length)//扩容条件,当数组满了的时候进行扩容  elementData = grow(); elementData[s] = e; size = s + 1; } public boolean add(E e) { modCount++; add(e, elementData, size); return true; } public void add(int index, E element) { rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length)//也是满了就扩容  elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index);//如果在数组中间插入元素,那么就得复制一个新的数组了  elementData[index] = element; size = s + 1; }  add之前会检查数组当前的容量,看看需不需要扩容. 由于list不像map那样复杂,一个位置只有一个元素,所以扩容条件只有一个: 数组已满 由于数组是连续的,如果是在指定位置插入,那么就需要复制一个新的数组来操作了.  也就是说,如果数组已满,且你又要再指定位置插入一个数据,那么数组将被复制两次.\n一个小坑 来看一段代码\nArrayList\u0026lt;Integer\u0026gt; list=new ArrayList\u0026lt;\u0026gt;(10); list.set(5,4); 这段代码可以正常运行吗?\n可能有人会说list的大小是10,那我10以内比如往5的位置set个值应该没问题吧.\n实际上会报一个数组越界的异常.\n不过这并不算真正的越界,我们的数组长度还是10.\npublic E set(int index, E element) { Objects.checkIndex(index, size);//边界选用的是size而不是length  E oldValue = elementData(index); elementData[index] = element; return oldValue; } @HotSpotIntrinsicCandidate public static \u0026lt;X extends RuntimeException\u0026gt; int checkIndex(int index, int length, BiFunction\u0026lt;String, List\u0026lt;Integer\u0026gt;, X\u0026gt; oobef) { if (index \u0026lt; 0 || index \u0026gt;= length) throw outOfBoundsCheckIndex(oobef, index, length); return index; } 由于list里面判断边界用的是size(实际节点个数),而不是length(数组长度),所以这种情况会报一个\u0026quot;越界\u0026quot;.\n这种写法实际上是把list当成数组来用了.\n官方认为虽然elementData是list的实现细节,但是它对外界是隐藏的,只能操作逻辑上的list而不是实际存在的elementData.\n封装的一大意义,就是向用户屏蔽用户不需要的操作.否则,用户可能会有意无意地调用这些操作,这将成为软件工程中重要的bug来源.\n如果list里面还没有元素,那么你只能add,不能get,set.\n如果list的size是5,那么你就不能访问第五个元素之后的位置.\nlist的add还没走到后面的位置,你就不能提前去put或者set.\n扩容 在list里面,扩容的函数是grow().\nprivate Object[] grow(int minCapacity) { return elementData = Arrays.copyOf(elementData, newCapacity(minCapacity)); } private Object[] grow() { return grow(size + 1);//扩容后数组长度最少+1  } 这里调用了Arrays.copyOf(),这个方法的作用就是将旧数组的内容复制到新的数组,新数组多出来的部分置为null.\n里面是个native方法,源码就不贴了.\n这里还调用了newCapacity(),这个方法用于判断新数组的长度.\nprivate int newCapacity(int minCapacity) { // overflow-conscious code  int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1);//新数组长度为原来的1.5倍,向下取整  if (newCapacity - minCapacity \u0026lt;= 0) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA)//初始化,数组为空,则长度设为10  return Math.max(DEFAULT_CAPACITY, minCapacity); if (minCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); return minCapacity; } return (newCapacity - MAX_ARRAY_SIZE \u0026lt;= 0) ? newCapacity : hugeCapacity(minCapacity); } 这里关键的两条表达式:\n newCapacity = oldCapacity + (oldCapacity \u0026raquo; 1) if (newCapacity - minCapacity \u0026lt;= 0)  什么情况下第二条表达式为true呢?\n 当oldCapacity=0,minCapacity=0+1=1,newCapacity=0+0=0,那么newCapacity - minCapacity=-1 \u0026lt; 0.这对应的是数组为空的情况,也就是上面我们说的无参构造方法会走到这条逻辑,将数组初始化长度为10. 当0\u0026lt;oldCapacity\u0026lt;4时,newCapacity - minCapacity依然小于0,这种情况扩容后数组长度+1. 其余情况newCapacity - minCapacity\u0026gt;0,此时新数组长度=newCapacity,也就是原来的1.5倍(向下取整)  看到这里,也许你会默默总结 :\n哦,原来扩容后,除了初始化外,数组的长度有两种情况,一种是+1,一种是+50%\n其实当原长度=2或3时,扩容后它们虽然也是长度+1,但是1也是2和3的50%(向下取整).\n所以看到没有,如果让我们来写这段逻辑,可能我们的思路是这样:\n  if 数组长度=0, then\u0026hellip;.\n  if 数组长度=1, then\u0026hellip;\n  而JDK的大神们用一两条表达式就把各种情况清清楚楚地分隔开了.不禁膜拜\u0026hellip;\u0026hellip;\n缩容 ArrayList没有自动缩容的机制,不过它提供了一个方法把list的容量最小化(刚好装得下所有元素)\npublic void trimToSize() { modCount++; if (size \u0026lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size);//同样是复制  } } Remove 和add一样,remove一样需要通过复制数组来进行.\n但有一点不一样,add不指定位置时,默认插入到末尾,此时不需要复制.\n而remove不指定位置时,默认从头开始找,然后删除找到的一个元素.那么删除的元素位置不确定,所以都当成删除中间的节点来处理,因此不论哪种情况都需要复制数组.\n也许有人会建议作者找到对应元素后判断一下是否末尾元素,如果是直接置null不就节省了一次复制的操作了?\n我以为,在实际情况中,要删除的节点恰好位于末尾是小概率事件,为了这小概率事件而每次都额外判断一次恐怕划不来.\n删除指定位置:\npublic E remove(int index) { Objects.checkIndex(index, size); final Object[] es = elementData; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) E oldValue = (E) es[index]; fastRemove(es, index); return oldValue; } 根据**equals()**删除指定元素\npublic boolean remove(Object o) { final Object[] es = elementData; final int size = this.size; int i = 0; found: { if (o == null) { for (; i \u0026lt; size; i++) if (es[i] == null)//null也可以删,都是删除找到的第一个  break found; } else { for (; i \u0026lt; size; i++) if (o.equals(es[i])) break found; } return false; } fastRemove(es, i); return true; } private void fastRemove(Object[] es, int i) { modCount++; final int newSize; if ((newSize = size - 1) \u0026gt; i) System.arraycopy(es, i + 1, es, i, newSize - i);//复制到新数组  es[size = newSize] = null;//最后一个位置设为null  } 简单概括下流程:\n 先遍历去查找元素,元素是null则找null,元素非null则调用equals()来比较. 删除找到的第一个元素,然后剩余元素复制到新的数组. 新数组的最后一个位置设为null  深克隆/浅克隆 这个在Hashmap那篇文章忘了讲,在这里补回来.\n先看下定义:\n浅克隆\n被复制对象的所有变量都含有与原来的对象相同的值，而所有的对其他对象的引用仍然指向原来的对象。换言之，浅复制仅仅复制所拷贝的对象，而不复制它所引用的对象。 深克隆\n被复制对象的所有变量都含有与原来的对象相同的值，除去那些引用其他对象的变量。那些引用其他对象的变量将指向被复制过的新对象，而不再是原有的那些被引用的对象。换言之，深复制把要复制的对象所引用的对象都复制了一遍。 其实它们的区别也就是值传递和引用传递的区别.\n浅克隆=引用传递 深克隆=值传递\n在我印象中,JDK好像没有几个类是实现深克隆的,所以如果不想复制后的新对象与旧对象相互干扰,在复制之前要重写clone()方法.\nArrayList的克隆方法\n/** * Returns a shallow copy of this {@code ArrayList} instance. (The * elements themselves are not copied.) * * @return a clone of this {@code ArrayList} instance */ public Object clone() { try { ArrayList\u0026lt;?\u0026gt; v = (ArrayList\u0026lt;?\u0026gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // this shouldn\u0026#39;t happen, since we are Cloneable  throw new InternalError(e); } } 注释写的很明白了,是浅复制(shallow copy).除了modCount置0之外,数组也是直接复制过去的.\n如果改成深克隆的话,应该要遍历数组,依次调用每个元素的clone方法而不是直接复制.(要注意套娃的情况)\nLinkedList 数据结构 LinkedList有一个内部类Node,这就是存取数据的结构了.\nprivate static class Node\u0026lt;E\u0026gt; { E item; Node\u0026lt;E\u0026gt; next; Node\u0026lt;E\u0026gt; prev; Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; this.next = next; this.prev = prev; } } 很典型的双向链表了.\n此外LinkedList内部也持有头尾双指针.\n由于链表不存在扩容的概念,因此size是链表的长度也是实际元素的个数.\ntransient int size = 0; /** * Pointer to first node. */ transient Node\u0026lt;E\u0026gt; first; /** * Pointer to last node. */ transient Node\u0026lt;E\u0026gt; last; 初始化 来看一下构造函数\npublic LinkedList() { } public LinkedList(Collection\u0026lt;? extends E\u0026gt; c) { this(); addAll(c); } 空的.\n没错是空的,它不需要像ArrayList先构造出一个数组那样现去构造出first节点.\nfirst是null的就让它暂时null吧,等到add第一个元素再去构造.\nAdd public boolean add(E e) {//插入一个节点,默认插入链尾  linkLast(e); return true; } public void add(int index, E element) {//往特定位置插入  checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); } public void addFirst(E e) {//往头部插入  linkFirst(e); } public void addLast(E e) {往尾部插入 linkLast(e); } 可以看到LinkedList支持往任意位置插入元素.\n根据linkFirst,linkBefore,linkLast,插入可分为3种情况:\n 插入头部 插入到某元素前面 插入尾部  private void linkFirst(E e) { final Node\u0026lt;E\u0026gt; f = first; final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(null, e, f);//小细节,这里是新建一个node而不是直接赋值  first = newNode;//new节点变为first节点  if (f == null)//链表是空的,那么first节点和last节点都同时为new节点  last = newNode; else f.prev = newNode;//原first节点的前驱变为new节点  size++; modCount++; } void linkBefore(E e, Node\u0026lt;E\u0026gt; succ) { // assert succ != null;  final Node\u0026lt;E\u0026gt; pred = succ.prev;//记录该节点的前驱  final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(pred, e, succ); succ.prev = newNode;//该节点的前驱为new节点  if (pred == null)//该节点的前驱为null,也就是该节点为first节点  first = newNode; else pred.next = newNode;//前驱的next指向新节点  size++; modCount++; } void linkLast(E e) { final Node\u0026lt;E\u0026gt; l = last; final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++; } 常规的链表操作了,唯一需要注意的是 : 插入的不是节点本身,而是它的复制.\n应该说,这里用到的只是这个节点的值,而不需要知道它前驱节点以及后继节点的信息.\n所以对链表任意操作并不会影响插入时参考过的那些节点.\nRemove public E remove() {//默认删除头节点  return removeFirst(); } public E removeFirst() { final Node\u0026lt;E\u0026gt; f = first; if (f == null)//链表为空时抛出异常  throw new NoSuchElementException(); return unlinkFirst(f); } public E removeLast() {//删除尾节点  final Node\u0026lt;E\u0026gt; l = last; if (l == null)//链表为空时抛出异常  throw new NoSuchElementException(); return unlinkLast(l); } public E remove(int index) {//删除指定位置的节点  checkElementIndex(index); return unlink(node(index));//node(index)作用是返回index位置的节点  } public boolean remove(Object o) {//删除找到的第一个指定的元素  if (o == null) { for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { if (x.item == null) { unlink(x); return true; } } } else { for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { if (o.equals(x.item)) { unlink(x); return true; } } } return false; } 删除也是支持指定任意位置,任意元素的.\n删除同样分为三种情况:\n 删除头节点 删除尾节点 删除中间节点  private E unlinkFirst(Node\u0026lt;E\u0026gt; f) { // assert f == first \u0026amp;\u0026amp; f != null;  final E element = f.item;//记录返回值  final Node\u0026lt;E\u0026gt; next = f.next;//记录后驱  f.item = null;//释放节点的资源, help GC就很骚了....  f.next = null; // help GC  first = next;//first节点后移  if (next == null) last = null;//若链表只有一个first节点,那么删除后last也为null  else next.prev = null;//否则next的前驱置为null,这里也可以写成first.prev=null  size--; modCount++; return element; } private E unlinkLast(Node\u0026lt;E\u0026gt; l) { // assert l == last \u0026amp;\u0026amp; l != null;  final E element = l.item; final Node\u0026lt;E\u0026gt; prev = l.prev; l.item = null; l.prev = null; // help GC  last = prev; if (prev == null) first = null; else prev.next = null; size--; modCount++; return element; } E unlink(Node\u0026lt;E\u0026gt; x) { // assert x != null;  final E element = x.item; final Node\u0026lt;E\u0026gt; next = x.next; final Node\u0026lt;E\u0026gt; prev = x.prev; if (prev == null) { first = next; } else { prev.next = next; x.prev = null; } if (next == null) { last = prev; } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } 链表的操作也很简单,就不写那么多注释了(为偷懒找个接口).\n\u0026ldquo;二分查找\u0026rdquo;? 链表的存取很简单,本来想直接跳过的,不过这次看源码又新发现了一个小的优化点,就顺便提一下.\n在上面remove操作时有个node(idnex),这个函数在get和set中也用到.\nNode\u0026lt;E\u0026gt; node(int index) { // assert isElementIndex(index);  if (index \u0026lt; (size \u0026gt;\u0026gt; 1)) {//判断index位于链表的前半部分还是后半部分  Node\u0026lt;E\u0026gt; x = first; for (int i = 0; i \u0026lt; index; i++) x = x.next; return x; } else { Node\u0026lt;E\u0026gt; x = last; for (int i = size - 1; i \u0026gt; index; i--) x = x.prev; return x; } } 前面提到过,LinkedList是支持操作任意节点的.\n而我们又知道,链表的节点地址是离散的,那么访问中间的节点一定要从first节点开始遍历吗?\n由于是双向链表,JDK在访问Linkedlist前,会先用 if (index \u0026lt; (size \u0026gt;\u0026gt; 1))判断一下要访问的节点位于前半部分还是后半部分.\n如果是前半部分,就从first节点向后遍历;\n如果是后半部分,就从last节点向前遍历.\n最好的情况下,这样的优化可以把时间从O(n)降低到O(1).\n这个问题并不难,只要意识到这个问题都能回答上来.\n问题只在于意识.\n写这一部分是为了提醒自己在一些小细节上不要忘了优化的意识.\n队列 \u0026amp; 双向队列 \u0026amp; 栈 LinkedList作为双向链表,可以同时作为队列/双向队列/栈来使用.\n作为队列时:\n peek : 返回头节点(并非删除) poll : 返回并删除头节点(出队) offer : 队尾新增节点(入队,等同于add)  作为双向队列时:\n push : 队首新增节点(入栈) pop : 返回并删除头节点(出栈)  还有很多api,像peekFirst,pollLast等等.不列举了.\napi不用背的,用到的时候忘了,点进源码看下注释,一下就记起来了.\nClone /** * Returns a shallow copy of this {@code LinkedList}. (The elements * themselves are not cloned.) * * @return a shallow copy of this {@code LinkedList} instance */ public Object clone() { LinkedList\u0026lt;E\u0026gt; clone = superClone(); // Put clone into \u0026#34;virgin\u0026#34; state  clone.first = clone.last = null; clone.size = 0; clone.modCount = 0; // Initialize clone with our elements  for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) clone.add(x.item); return clone; } 和ArrayList一样,也是浅复制\n","permalink":"http://euthpic.github.io/tech/list%E7%AE%80%E5%8D%95%E5%8E%9F%E7%90%86/","summary":"上一篇文章我们介绍了Hashmap,今天来看一看List接口下最主要的两个类 : ArrayList和LinkedList 思维导图 ArrayList 链表与数组 从","title":"List简单原理"},{"content":"一.\t前言 设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。\n二.\t设计原则   开闭原则\n一个软件实体如类,模块,和函数应该对拓展开放,对修改关闭\n  依赖倒置原则\n高层模块不应该依赖低层模块,二者都应该依赖其抽象.\n抽象不应该依赖细节,细节应该依赖抽象.\n针对接口编程,不要针对实现编程.\n尽量抽象出接口,拓展其实现子类而非拓展原有类的方法,传参用接口,这样完成什么功能由客户端自己传入的子类类型决定,服务方不用修改.\n  单一职责原则\n不要存在多于一个导致类变更的原因.\n一个类/接口/方法只负责一项职责,不要承担过多的任务.比如更新用户信息可以拆分成更新用户名,用户密码,地址等等.\n该原则目的在于降低当需求变更时修改代码导致的BUG风险.修改代码时容易因为新情况的加入而忽略旧情况或者新旧边界值等.\n  接口隔离原则\n用多个专门的接口,而不使用单一的总接口,客户端不应该依赖它不需要的接口\n一个类对一个类的依赖应该建立在最小的接口上\n建立单一接口,不要建立庞大臃肿的接口\n尽量细化接口,接口中的方法尽量少.细粒度可以组装,但是粗粒度不能拆分.\n例如,animal接口中有eat,fly,swim三个方法,dog类实现该接口,就不得不实现一个空的fly方法,所以应该将接口拆分,让dog类去实现其所需要的接口方法.\n  迪米特法则(最少知道原则)\n一个对象应该对其他对象保持最少的了解,尽量降低类与类之间的耦合\nA找B买东西,B需要C进货,但是A不需要了解C,他只需要和B交互即可\n  里氏替换原则\n子类对象替换父类对象时,程序的功能不受影响.也就是子类可以拓展父类,但不得改变或者削弱父类原有的功能\n 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。  举这么一个例子,父类中的A方法被B方法引用,子类重写了A方法,并且只调用了A方法,完全无视B方法,如果此时有另外一个类会扫描该父类的所有子类并调用其B方法,那么B方法的行为可能就会改变而导致BUG\n  合成/复用原则(组合/复用原则)\n新对象需要使用已有对象,使之成为新对象的一部分,此时组合关系应优先于继承关系,因为继承受到里氏替换原则和其他的约束\n  三.\t分类   创建型\n 单例 简单工厂 工厂方法 抽象工厂 生成器 原型    行为型\n 责任链 命令 解释器 迭代器 中介者 备忘录 观察者 状态 策略 模版方法 访问者 空对象    结构型\n 适配器 桥接 组合 装饰 外观 享元 代理    单例（Singleton） Intent 确保一个类只有一个实例，并提供该实例的全局访问点。\nClass Diagram 使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。\n私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。\n\nImplementation Ⅰ 懒汉式-线程不安全 以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。\n这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。\npublic class Singleton { private static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } Ⅱ 饿汉式-线程安全 线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。\n但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。\nprivate static Singleton uniqueInstance = new Singleton(); Ⅲ 懒汉式-线程安全 只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。\n但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。\npublic static synchronized Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } Ⅳ 双重校验锁-线程安全(double check) uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。\n双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } double check的改进在于,第一个if判断的作用为分流,只有首次调用时对象未实例化才会进入if里面,当对象已经实例化后访问不会遭到阻塞,第二个if判断才进行加锁保证安全.\n不过,由于指令重排序的原因,单纯的double check无法百分百保证线程安全.例如， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址  由于 JVM 具有指令重排的特性，执行顺序有可能变成 1\u0026gt;3\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例导致空指针异常。例如，线程 T1 执行了 1 和 3(此时uniqueInstance != null)，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\nⅤ 静态内部类实现 当 Singleton 类被加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\npublic class Singleton { private Singleton() { } private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } Ⅵ 枚举实现 public enum Singleton { INSTANCE; private String objName; public String getObjName() { return objName; } public void setObjName(String objName) { this.objName = objName; } public static void main(String[] args) { // 单例测试  Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName(\u0026#34;firstName\u0026#34;); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName(\u0026#34;secondName\u0026#34;); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试  try { Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) { System.out.println(enumConstant.getObjName()); } } catch (Exception e) { e.printStackTrace(); } } } firstName secondName secondName secondName 关于枚举的拓展:\n枚举类的每一个枚举量在编译的时候都会编译成一个单独的final类,与静态内部类一样,该类的加载安全由JVM的类加载机制来保证.\n该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码(如果尝试再次实例化对象则抛出异常)。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。\n该实现在多次序列化和序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。\n反序列化攻击:\n​\t如果单例类实现了Serializable接口,那么调用readObject()方法再到newInstance()方法会调用无参构造方法实例化新的对象,要防止该攻击,要么单例类不实现该接口,要么在Singleton类中定义readResolve()方法就可以解决该问题\nprivate Object readResolve() { return singleton;\t//直接返回对象而非创建对象  } Examples  Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons   JDK  java.lang.Runtime#getRuntime() \u0026ndash;饿汉式 java.awt.Desktop#getDesktop() [java.lang.System#getSecurityManager()](  Spring  AbstractFactoryBean.getObect()  AbstractBeanFactory的作用  api里是这样说的,是抽象BeanFactory的基类,同时实现了ConfigurableBeanFactory的SPI,提供了所有的功能  也可以从我们定义的资源中resource中来获取bean的定义.\n 也提供了单例bean的缓存通过他的爷爷如图中的DefaultSingletonBeanRegistry,同时提供了单例和多例和别名的定义等操作.  上述只是其中的一部分,总之来说它的作用还是挺大的.\n生成器（Builder） Intent 将一个复杂对象的构建与它的表示分离,使得同样的构建过程可以创建不同的表示.\n封装一个对象的构造过程，并允许按步骤构造.\n生成器模式生产的是一个可定制化的产品,可通过调用Builder类的多个方法来设置产品的属性.与工厂方法相比,工厂方法的产品是模型已经固定好了的.\nClass Diagram \nJDK  java.lang.StringBuilder.append() java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders  Guava  CacheBuilder.newBuilder()  生成器模式\n生成器模式可以改进成链式调用(在guava中大力提倡),Builder类中赋值的方法返回类型不是void而是它本身,这样就可以链式调用起来,最后再通过builder()方法返回定制化的Product\n可以这样简单理解:\n 简单工厂只有一个工厂,这个工厂生产多种产品,新增产品时需要增加方法.(一对多) 工厂方法拓展了工厂类,有多个工厂类,每个工厂生产一种产品,新增产品时新增工厂类.(一对一) 抽象工厂结合两者,生产产品族(二维),有多个工厂类,每个工厂类又是一个简单工厂.(一对多)  简单工厂（Simple Factory） Intent 在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。\nClass Diagram 简单工厂把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。\n这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。客户类往往有多个，如果不使用简单工厂，那么所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。\n\nImplementation public interface Product { } public class ConcreteProduct implements Product { } public class ConcreteProduct1 implements Product { } public class ConcreteProduct2 implements Product { } 以下的 Client 类包含了实例化的代码，这是一种错误的实现。如果在客户类中存在这种实例化代码，就需要考虑将代码放到简单工厂中。\npublic class Client { public static void main(String[] args) { int type = 1; Product product; if (type == 1) { product = new ConcreteProduct1(); } else if (type == 2) { product = new ConcreteProduct2(); } else { product = new ConcreteProduct(); } // do something with the product  } } 以下的 SimpleFactory 是简单工厂实现，它被所有需要进行实例化的客户类调用。\npublic class SimpleFactory { public Product createProduct(int type) { if (type == 1) { return new ConcreteProduct1(); } else if (type == 2) { return new ConcreteProduct2(); } return new ConcreteProduct(); } } public class Client { public static void main(String[] args) { SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); // do something with the product  } } 工厂方法（Factory Method） Intent 定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。\nClass Diagram 在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。\n下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。\n\nImplementation public abstract class Factory { abstract public Product factoryMethod(); public void doSomething() { Product product = factoryMethod(); // do something with the product  } } public class ConcreteFactory extends Factory { public Product factoryMethod() { return new ConcreteProduct(); } } public class ConcreteFactory1 extends Factory { public Product factoryMethod() { return new ConcreteProduct1(); } } public class ConcreteFactory2 extends Factory { public Product factoryMethod() { return new ConcreteProduct2(); } } JDK  Collection.iterator() 产品为内部类Itr  抽象工厂（Abstract Factory） Intent 提供一个接口，用于创建 相关的对象家族 。\nClass Diagram 抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。\n抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。\n至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。\n从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。\n\nImplementation public class AbstractProductA { } public class AbstractProductB { } public class ProductA1 extends AbstractProductA { } public class ProductA2 extends AbstractProductA { } public class ProductB1 extends AbstractProductB { } public class ProductB2 extends AbstractProductB { } public abstract class AbstractFactory { abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB(); } public class ConcreteFactory1 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA1(); } AbstractProductB createProductB() { return new ProductB1(); } } public class ConcreteFactory2 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA2(); } AbstractProductB createProductB() { return new ProductB2(); } } public class Client { public static void main(String[] args) { AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB  } } JDK  数据库连接Connection.prepareStatement() 产品为Statement类  外观（Facade）(门面模式) Intent 提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。\n某个功能,需要调用多个接口,逻辑固定,因此可以封装成一个接口,通过该接口完成该功能.\nClass Diagram \nImplementation 观看电影需要操作很多电器，使用外观模式实现一键看电影功能。\npublic class SubSystem { public void turnOnTV() { System.out.println(\u0026#34;turnOnTV()\u0026#34;); } public void setCD(String cd) { System.out.println(\u0026#34;setCD( \u0026#34; + cd + \u0026#34; )\u0026#34;); } public void startWatching(){ System.out.println(\u0026#34;startWatching()\u0026#34;); } } public class Facade { private SubSystem subSystem = new SubSystem(); public void watchMovie() { subSystem.turnOnTV(); subSystem.setCD(\u0026#34;a movie\u0026#34;); subSystem.startWatching(); } } public class Client { public static void main(String[] args) { Facade facade = new Facade(); facade.watchMovie(); } } 设计原则 最少知识原则：只和你的密友谈话。也就是说客户对象所需要交互的对象应当尽可能少。\n原型模式（Prototype） Intent 使用原型实例指定要创建对象的类型，通过赋值这个原型来创建新对象。\n不需要知道任何创建的细节,不调用构造函数.核心是clone()方法,需要实现Cloneable接口,因此需要注意浅克隆(复制基本值)和深克隆(复制复杂对象)的区别.\n核心作用是节省资源,复制的开销小于创建一个新的对象.\n如果复制的对象包含复杂的成员对象,则需要重写clone方法,将这些对象也一起克隆.\nClass Diagram \nImplementation public abstract class Prototype { abstract Prototype myClone(); } public class ConcretePrototype extends Prototype { private String filed; public ConcretePrototype(String filed) { this.filed = filed; } @Override Prototype myClone() { return new ConcretePrototype(filed); } @Override public String toString() { return filed; } } public class Client { public static void main(String[] args) { Prototype prototype = new ConcretePrototype(\u0026#34;abc\u0026#34;); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); } } abc 克隆破坏单例 如果克隆的是一个单例对象,那么clone()方法会返回一个新的对象,从而破坏了单例.因此需要重写clone方法,让它也通过单一入口来获取单例对象.\n@Override protected Object clone() throws CloneNotSupportedException { return getInstance(); } JDK  java.lang.Object#clone()  ","permalink":"http://euthpic.github.io/tech/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","summary":"一. 前言 设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细","title":"设计模式"},{"content":"","permalink":"http://euthpic.github.io/tech/%E6%8E%92%E6%9F%A5%E7%BA%BF%E4%B8%8A%E6%95%85%E9%9A%9C%E6%80%9D%E8%B7%AF/","summary":"","title":"排查线上故障思路"},{"content":"遇到问题  yml更新后重新install app,然后pod偶尔一直处于scheduling状态,需要重新install才解决(pvc没有挂载) helm配置文件更新后(install/upgrade),pod重启会获取到错误的tag,此时要么去jenkins里重新构建一次来更新tag,要么手动修改values.yaml里面的deplyApp里面的tag(可以在rancher里面查看最新的tag是什么) helm配置里的yml解析失败,整数和布尔类型的配置都要用双引号 loki可以收集stdout的日志,如果要看日志文件需要像以前那样进入pod里看. yml更新之后,通过helm upgrade来应用.但是如果修改了metadata的东西,就需要helm install了. configmap的文件更新后,需要使用k apply -f应用更新,然后重新构建pod jenkins任务名不能和项目名一致(keservices),如果因此导致构建失败,需要把任务和对应的项目目录删掉,改名后再重试. 使用k describe来查看pod的情况,最下面的Events含有pod启动过程中的日志,可以排查pod启动失败的原因 环境变量(helm里面定义的配置)优先于本地配置 pod中文乱码问题 使用-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2来替代xmx 一个pod在同一个pvc里最多只能挂载一个目录 含有helm模板的yaml,只能通过helm命令来使用,原生的kubectl无法识别(kube apply -f) 项目内访问服务的方式与yaml中定义的一样,url中的域名使用svc名称+namespace即可. Ingress的networking.k8s.io/v1版本虽然在1.18中已经推荐使用,但查看相关issue发现其实只能在1.19+使用 环境变量区分大小写 集群内调用svc的话,对应的端口是port.集群外调用svc的话,对应的端口时targetPort. configmap里面的配置值不支持整数,如果创建configmap时报错,优先检查下有没有这个问题.为每个配置值都用双引号包起来是比较推荐的做法 在values.yaml里面的configmap不能含有数字配置项,如果必须包含数字,则只能另外新建configmap再引入(可能跟helm的语法规则有关) 通过crd拓展k8s的api helm中的chart = docker中的image, repository = docker中的镜像库, release = 容器 rocketmq修改配置的话需要修改为自己的镜像 外部访问容器遇到问题的排查思路 : 检查http协议有无问题,k8s集群内通过SVC访问的话用http,外部访问的话现在默认是https -\u0026gt; 检查容器暴露的targetPort有无问题 -\u0026gt; 检查SVC的port是否转发到正确的targetPort -\u0026gt; 检查SVC的port是否默认的80,SVC访问的url为svc:port -\u0026gt; 检查ingress是否正确转发到了svc -\u0026gt; 检查ingress是否有访问日志   ## 失败示例: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 26s (x701 over 13h) default-scheduler persistentvolumeclaim \u0026quot;x-pvc\u0026quot; not found ## 成功示例 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 79s default-scheduler Successfully assigned test-oss/x-deploy-9bdcd8d7b-gxqk8 to pt-k8s-c Normal Pulling 78s kubelet Pulling image \u0026quot;harbor.x.com.cn/product/x:6798daa\u0026quot; Normal Pulled 75s kubelet Successfully pulled image \u0026quot;harbor.x.com.cn/product/x:6798daa\u0026quot; Normal Created 75s kubelet Created container x Normal Started 74s kubelet Started container x 常用命令    k exec -n test-oss x-deploy-769f5f5d98-w7m7g \u0026ndash; printenv\n 2. 获取svc信息 k get svc -n test-oss proxy-host-svc -o wide\n 3. 获取异常pod信息 kubectl get pods -n test-oss | grep -v Running\n","permalink":"http://euthpic.github.io/tech/k8s%E6%94%B9%E9%80%A0/","summary":"遇到问题 yml更新后重新install app,然后pod偶尔一直处于scheduling状态,需要重新install才解决(pvc没有挂载)","title":"K8s改造"},{"content":" Docker镜像分层 SpringBoot 2.3.x 新增对分层的支持  spring-boot-maven-plugin开启分层编译支持 Dockerfile修改 分层后的Jar结构  classpath.idx： 文件列出了依赖的 jar 包列表，到时候会按照这个顺序载入。 layers.idx： 文件清单，记录了所有要被复制到 Dokcer 镜像中的文件信息。   列出可以从分层 Jar 中提取出的文件夹信息(spring-boot-jarmode-layertools)    java -Djarmode=layertools -jar isuwang-mobile-oss-1.0.jar list dependencies -- 依赖项一般变化不大 spring-boot-loader -- spring-boot启动模块 snapshot-dependencies -- 快照依赖，为快照版本的依赖，更新迭代的会快一些 application -- 业务层，也就是我们最频繁变动的  构建速度的测试     普通构建 分层构建     全构建 3m41s 3m58s   修改代码后构建 2m24s 2m54s   修改api后构建 1m58s 3m31s      主要耗时步骤(154m -\u0026gt; 耗时 1m7s / 1m37s ) Step 4/16 : COPY demo-1.0.jar application.jar (耗时 25s) Step 5/16 : RUN java -Djarmode=layertools -jar application.jar extract 总结  镜像构建 : 在构建上，使用分层 Jar 构建镜像可能比普通方式构建镜像更繁琐(构建两次,还要解压)，所以也更耗时，故而在构建上分层 Jar 构建镜像没有优势,甚至处于劣势 镜像推送 : 在推送上，如果每次构建镜像都只是修改构建镜像项目的源码，使用分层 Jar 构建镜像，可以大大加快镜像推送速度。如果是修改构建镜像项目中的依赖包，则和普通构建一样速度很慢(因为dependencies层是中间层镜像最大的一层) 镜像拉取 : 与推送一样 综上,在本地构建方面,普通构建有优势;在网络传输方面(docker pull/docker push),分层构建有优势  最后附上实验用的dockerfile\n#FROM openjdk:8u212-jdk-alpine3.9 #MAINTAINER fa \u0026#34;fa@x.com.cn\u0026#34; ## 1.普通镜像构建脚本文件 ## 系统编码 #ENV LANG=C.UTF-8 LC_ALL=C.UTF-8 #RUN apk add --no-cache bash ## 设置时区为上海 #RUN apk add tzdata \u0026amp;\u0026amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026amp;\u0026amp; apk del tzdata #VOLUME /tmp #ADD demo-1.0.jar app.jar #EXPOSE 8099 #ENTRYPOINT exec java ${JAVA_OPTS} -Djava.security.egd=file:/dev/./urandom -jar app.jar    # 2. 分层镜像构建脚本文件 # 指定基础镜像，这是分阶段构建的前期阶段 FROMopenjdk:8u212-jdk-alpine3.9 as builder MAINTAINERfa \u0026#34;fa@x.com.cn\u0026#34; # 执行工作目录 WORKDIRapplication # 配置参数 #ARG JAR_FILE=target/*.jar # 将编译构建得到的jar文件复制到镜像空间中(主要的耗时步骤) COPY application.jar application.jar # 通过工具spring-boot-jarmode-layertools从application.jar中提取拆分后的构建结果 RUN java -Djarmode=layertools -jar application.jar extract  # 正式构建镜像 FROMopenjdk:8u212-jdk-alpine3.9 #系统编码 ENV LANG=C.UTF-8 LC_ALL=C.UTF-8 RUN apk add --no-cache bash # 设置时区为上海 RUN apk add tzdata \u0026amp;\u0026amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026amp;\u0026amp; apk del tzdata EXPOSE8080 EXPOSE9595 WORKDIRapplication # 前一阶段从jar中提取除了多个文件，这里分别执行COPY命令复制到镜像空间中，每次COPY都是一个layer COPY --from=builder application/dependencies/ ./ COPY --from=builder application/spring-boot-loader/ ./ COPY --from=builder application/snapshot-dependencies/ ./ COPY --from=builder application/application/ ./ #ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;org.springframework.boot.loader.JarLauncher\u0026#34;] ENTRYPOINT exec java ${JAVA_OPTS} org.springframework.boot.loader.JarLauncher","permalink":"http://euthpic.github.io/tech/springboot%E5%88%86%E5%B1%82%E6%9E%84%E5%BB%BAdocker%E9%95%9C%E5%83%8F/","summary":"Docker镜像分层 SpringBoot 2.3.x 新增对分层的支持 spring-boot-maven-plugin开启分层编译支持 Dockerfile修改 分层后的Ja","title":"SpringBoot分层构建Docker镜像"}]